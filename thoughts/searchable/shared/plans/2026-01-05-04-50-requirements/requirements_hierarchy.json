{
  "requirements": [
    {
      "id": "REQ_000",
      "description": "The system must support TypeScript project initialization with pnpm package manager, tsup/esbuild build system, Vitest testing framework, Zod type validation, Biome linting/formatting, fast-check property testing, and native Node.js 18+ async runtime",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_000.1",
          "description": "Initialize package.json with pnpm package manager, defining project metadata, scripts for development/build/test workflows, and establishing pnpm as the exclusive package manager with strict dependency resolution",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "package.json exists in project root with valid JSON structure",
            "package.json contains name, version, description, main, types, and exports fields",
            "packageManager field is set to 'pnpm@9.0.0' or higher to enforce pnpm usage",
            "scripts section includes dev, build, test, test:coverage, type-check, lint, and format commands",
            "engines field specifies node >=18.0.0 requirement",
            "pnpm-lock.yaml is generated after initial install",
            ".npmrc file is created with strict-peer-dependencies=false and auto-install-peers=true",
            "pnpm install command executes successfully without errors",
            "All scripts in package.json execute without throwing missing command errors"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "package.json configuration file with project metadata",
              "pnpm-lock.yaml lockfile for deterministic installs",
              ".npmrc configuration for pnpm behavior",
              "Script definitions for build, test, lint workflows",
              "Engine constraints for Node.js version compatibility"
            ]
          },
          "testable_properties": [],
          "function_id": "PackageManager.initializePnpm",
          "related_concepts": [
            "pnpm workspace configuration",
            "package manager enforcement",
            "dependency management",
            "npm script automation",
            "semantic versioning"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_000.2",
          "description": "Configure tsconfig.json with ES2022 target, strict type-checking mode, ESNext module system, bundler resolution strategy, and appropriate compiler options for modern TypeScript development with maximum type safety",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "tsconfig.json exists in project root with valid JSON structure",
            "compilerOptions.target is set to 'ES2022'",
            "compilerOptions.module is set to 'ESNext'",
            "compilerOptions.moduleResolution is set to 'bundler'",
            "compilerOptions.strict is set to true enabling all strict type checks",
            "compilerOptions.esModuleInterop is set to true for CommonJS compatibility",
            "compilerOptions.skipLibCheck is set to true for faster compilation",
            "compilerOptions.resolveJsonModule is set to true for JSON imports",
            "compilerOptions.outDir is set to './dist' for compiled output",
            "compilerOptions.rootDir is set to './src' for source files",
            "compilerOptions.declaration is set to true for .d.ts generation",
            "compilerOptions.declarationMap is set to true for declaration source maps",
            "compilerOptions.sourceMap is set to true for debugging",
            "include array contains ['src/**/*'] pattern",
            "exclude array contains ['node_modules', 'dist', '**/*.test.ts'] patterns",
            "types array includes ['vitest/globals'] for test globals",
            "tsc --noEmit command executes successfully with zero type errors on sample code"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "tsconfig.json compiler configuration file",
              "Compiler options for strict type safety",
              "Module resolution configuration",
              "Source and output directory mappings",
              "Type definition includes for testing framework",
              "Declaration file generation settings"
            ]
          },
          "testable_properties": [],
          "function_id": "TypeScriptConfig.configureCompiler",
          "related_concepts": [
            "strict mode type checking",
            "module resolution strategies",
            "ES2022 language features",
            "declaration file generation",
            "source map configuration",
            "path alias resolution"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_000.3",
          "description": "Set up Vitest testing framework with coverage reporting, global test utilities, ESM support, and integration with TypeScript for fast unit and integration testing with comprehensive code coverage metrics",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "vitest package is installed as devDependency",
            "@vitest/coverage-v8 package is installed for coverage reporting",
            "vitest.config.ts exists in project root with valid TypeScript configuration",
            "test.globals is set to true in config for global test functions",
            "test.environment is set to 'node' for Node.js testing context",
            "coverage.provider is set to 'v8' for native coverage",
            "coverage.reporter includes ['text', 'json', 'html'] formats",
            "coverage.exclude includes ['**/*.test.ts', '**/*.spec.ts', 'dist/**', 'node_modules/**']",
            "coverage.thresholds are defined with branches: 80, functions: 80, lines: 80, statements: 80",
            "vitest run command executes sample test successfully",
            "vitest --coverage command generates coverage reports in coverage/ directory",
            "Coverage HTML report opens and displays metrics correctly",
            "vitest --watch mode starts and detects file changes"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "vitest.config.ts configuration file",
              "Coverage reporter configuration",
              "Test environment settings for Node.js",
              "Global test utilities enablement",
              "Coverage threshold definitions",
              "Test file pattern matching configuration",
              "Sample test file to validate setup (src/**/*.test.ts pattern)"
            ]
          },
          "testable_properties": [],
          "function_id": "TestFramework.setupVitest",
          "related_concepts": [
            "test runner configuration",
            "coverage threshold enforcement",
            "test environment setup",
            "assertion library integration",
            "test file patterns",
            "watch mode configuration",
            "parallel test execution"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_000.4",
          "description": "Integrate Zod library for runtime type validation and schema definition, enabling compile-time type inference from runtime schemas and providing robust validation for API inputs, configuration, and data models",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "zod package is installed as production dependency",
            "zod version is 3.22.0 or higher",
            "Sample schema file exists demonstrating basic Zod usage (e.g., src/schemas/example.ts)",
            "Sample schema includes z.object(), z.string(), z.number(), and z.array() usage",
            "Type inference using z.infer<typeof schema> is demonstrated in sample code",
            "Sample validation code shows both parse() and safeParse() methods",
            "Error handling for ZodError is demonstrated with formatted error messages",
            "Schema composition example exists showing z.intersection() or z.union()",
            "Test file exists validating Zod schema behavior with valid and invalid inputs",
            "Documentation comment exists explaining Zod usage patterns for the project"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Input validation for API request payloads",
              "Configuration validation at startup",
              "Environment variable parsing and validation"
            ],
            "middleware": [
              "Request body validation middleware",
              "Query parameter validation middleware"
            ],
            "shared": [
              "Zod schema definitions for data models",
              "Type inference utilities for schema-to-type conversion",
              "Validation error formatter utilities",
              "Reusable schema composition helpers",
              "Base schemas for common patterns (ID formats, timestamps, etc.)"
            ]
          },
          "testable_properties": [],
          "function_id": "TypeValidation.integrateZod",
          "related_concepts": [
            "runtime type checking",
            "schema-driven validation",
            "type inference from schemas",
            "validation error handling",
            "data transformation",
            "parse vs safeParse patterns",
            "schema composition"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_000.5",
          "description": "Configure Biome for unified linting and formatting, replacing separate ESLint and Prettier setups with a single fast tool that enforces consistent code style and catches potential errors across TypeScript codebase",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "@biomejs/biome package is installed as devDependency",
            "biome.json configuration file exists in project root",
            "biome.json specifies linter.enabled: true and formatter.enabled: true",
            "linter.rules includes recommended TypeScript rules",
            "formatter.indentStyle is set to 'tab' or 'space' with consistent indentWidth",
            "formatter.lineWidth is set to 100 or project-preferred value",
            "files.include specifies ['src/**/*.ts', 'src/**/*.tsx'] patterns",
            "files.ignore includes ['dist/**', 'node_modules/**', 'coverage/**'] patterns",
            "biome check src/ command executes without errors on formatted code",
            "biome format --write src/ command successfully formats all source files",
            "biome lint src/ command detects intentionally introduced linting errors",
            "package.json lint script is set to 'biome check src/'",
            "package.json format script is set to 'biome format --write src/'",
            "All existing source files pass biome check without violations"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "biome.json configuration file with linter and formatter rules",
              "Linting rule definitions for TypeScript best practices",
              "Formatting rules for consistent code style",
              "File pattern configurations for include/exclude",
              "Integration scripts in package.json for lint and format commands",
              "CI/CD configuration snippet for automated checks"
            ]
          },
          "testable_properties": [],
          "function_id": "CodeQuality.configureBiome",
          "related_concepts": [
            "unified toolchain",
            "code formatting rules",
            "linting rule configuration",
            "pre-commit hooks",
            "CI/CD integration",
            "import sorting",
            "code style consistency"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_000.6",
          "description": "Set up fast-check library for property-based testing, enabling generative testing strategies to validate code behavior across wide input ranges and discover edge cases that example-based tests might miss",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "fast-check package is installed as devDependency",
            "@fast-check/vitest package is installed for Vitest integration",
            "fast-check version is 3.15.0 or higher",
            "Sample property test file exists (e.g., src/**/*.property.test.ts)",
            "Sample property test uses fc.assert() with fc.property()",
            "Sample test demonstrates arbitrary generation using fc.string(), fc.integer(), fc.array()",
            "Sample test demonstrates custom arbitrary creation with fc.record()",
            "Property test validates an invariant (e.g., round-trip, idempotence, commutativity)",
            "Failed property test demonstrates shrinking by showing minimal counterexample",
            "Integration with Vitest allows property tests to run via vitest command",
            "Documentation exists explaining when to use property tests vs example-based tests",
            "At least one property test validates core data model invariant (e.g., ID format, compression rules)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "fast-check library integration",
              "@fast-check/vitest plugin configuration",
              "Sample property test demonstrating arbitrary generators",
              "Custom arbitrary definitions for domain models",
              "Property test utilities for common invariants",
              "Documentation on property-based testing patterns",
              "Test helpers for generating valid domain objects"
            ]
          },
          "testable_properties": [],
          "function_id": "PropertyTesting.setupFastCheck",
          "related_concepts": [
            "property-based testing methodology",
            "generative testing strategies",
            "arbitrary value generation",
            "shrinking failed examples",
            "test invariants",
            "hypothesis testing equivalence",
            "counterexample discovery"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_000.7",
          "description": "Validate native Node.js 18+ async runtime support by verifying availability of modern async/await features, top-level await, fetch API, and other ES2022+ capabilities required for the TypeScript project without external polyfills",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "package.json engines.node field specifies '>=18.0.0'",
            ".nvmrc file exists specifying Node.js version 18 or higher",
            "Sample async/await code executes successfully in Node.js runtime",
            "Top-level await usage is demonstrated in sample TypeScript file and executes without error",
            "Native fetch() API is accessible and functional without node-fetch polyfill",
            "AbortController is available for request cancellation",
            "Async iterator usage (for await...of) works with async generators",
            "Promise.allSettled() is available and tested",
            "Process execution validation script confirms Node.js version >= 18.0.0",
            "CI/CD configuration specifies Node.js 18+ in workflow matrix",
            "README documents Node.js version requirement clearly"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Async/await patterns in business logic",
              "Native fetch for HTTP requests",
              "AbortController for request cancellation",
              "Async generators for streaming data"
            ],
            "middleware": [
              "Async middleware request handlers",
              "Promise-based request/response processing"
            ],
            "shared": [
              ".nvmrc file for Node.js version pinning",
              "Runtime validation script checking Node.js version",
              "Async utility functions leveraging native APIs",
              "Documentation of async patterns and best practices",
              "CI/CD configuration for Node.js version matrix",
              "Example code demonstrating modern async features"
            ]
          },
          "testable_properties": [],
          "function_id": "RuntimeEnvironment.validateNodeAsync",
          "related_concepts": [
            "Node.js version compatibility",
            "async/await patterns",
            "top-level await",
            "native fetch API",
            "promise-based APIs",
            "async iterators",
            "AbortController support"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_000.8",
          "description": "Configure tsup or esbuild as the build system for fast TypeScript compilation and bundling, producing optimized ESM and CJS outputs with proper sourcemaps and type declarations for library distribution",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "tsup package is installed as devDependency OR esbuild with custom build script exists",
            "tsup.config.ts exists (if using tsup) with entry, format, dts, sourcemap, and clean options",
            "Build configuration specifies both 'esm' and 'cjs' output formats",
            "dts option is set to true for generating .d.ts declaration files",
            "sourcemap option is set to true for debugging support",
            "clean option is set to true to remove dist/ before build",
            "entry field points to 'src/index.ts' or appropriate entry points",
            "package.json main field points to './dist/index.cjs' for CJS consumers",
            "package.json module field points to './dist/index.mjs' for ESM consumers",
            "package.json types field points to './dist/index.d.ts' for TypeScript consumers",
            "package.json exports field defines conditional exports for 'import' and 'require'",
            "npm run build command successfully generates output in dist/ directory",
            "dist/ contains .mjs, .cjs, .d.ts, and .map files",
            "tsup --watch mode starts and rebuilds on file changes",
            "Built output is importable in both ESM and CJS test files"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "tsup.config.ts or esbuild configuration",
              "Build script definitions in package.json",
              "Output format configurations (ESM + CJS)",
              "Declaration file generation settings",
              "Sourcemap configuration for debugging",
              "Entry point definitions for bundling",
              "Package.json exports field for dual module support",
              "Watch mode configuration for development"
            ]
          },
          "testable_properties": [],
          "function_id": "BuildSystem.configureTsupEsbuild",
          "related_concepts": [
            "build tool selection",
            "ESM vs CJS output",
            "code bundling",
            "tree shaking",
            "minification",
            "sourcemap generation",
            "watch mode",
            "dual package hazard mitigation"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_001",
      "description": "The system must port all Python dataclasses and enums to TypeScript types with Zod schemas for runtime validation, including ContextEntry with 8 entry types (FILE, COMMAND, COMMAND_RESULT, TASK, TASK_RESULT, SEARCH_RESULT, SUMMARY, CONTEXT_REQUEST) and RequirementNode with hierarchical structure",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_001.1",
          "description": "Convert all Python dataclasses from context_window_array/models.py to TypeScript types, including EntryType enum with 8 variants, ContextEntry interface with all 12 fields (id, entry_type, source, content, summary, created_at, references, searchable, compressed, ttl, parent_id, derived_from), and associated helper types",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "EntryType enum is defined with all 8 variants: FILE, COMMAND, COMMAND_RESULT, TASK, TASK_RESULT, SEARCH_RESULT, SUMMARY, CONTEXT_REQUEST",
            "ContextEntry interface includes all 12 fields with correct TypeScript types",
            "Optional fields use T | undefined syntax (content, ttl, parent_id)",
            "Default values are documented in JSDoc comments",
            "id field is typed as string with regex pattern documentation",
            "created_at uses Date type",
            "references and derived_from use string[] type",
            "Boolean fields (searchable, compressed) have explicit boolean type",
            "File is created at src/context/models.ts",
            "All types are exported with named exports",
            "File includes comprehensive JSDoc documentation for each type",
            "SearchResult and StoreSearchResult interfaces are included if present in Python",
            "Type definitions compile without errors with TypeScript strict mode"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "src/context/models.ts - EntryType enum definition",
              "src/context/models.ts - ContextEntry interface with all 12 fields",
              "src/context/models.ts - SearchResult interface (if applicable)",
              "src/context/models.ts - StoreSearchResult interface (if applicable)",
              "src/context/models.ts - JSDoc documentation for all types",
              "src/context/models.ts - Type export statements"
            ]
          },
          "testable_properties": [],
          "function_id": "ContextModels.convertPythonDataclasses",
          "related_concepts": [
            "Python dataclass to TypeScript interface mapping",
            "Enum conversion (Python Enum to TypeScript enum)",
            "Optional field handling (Optional[T] to T | undefined)",
            "Type imports and exports",
            "Date handling (datetime to Date)",
            "Array type conversion (List[T] to T[])"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_001.2",
          "description": "Create comprehensive Zod schemas for runtime validation of ContextEntry and all related types, including regex validation for id field (ctx_[a-zA-Z0-9]{8}), enum validation for entry_type, cross-field validation rules (compressed entries must not have content, must have summary), and type inference exports",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "EntryTypeSchema validates against all 8 enum values using z.nativeEnum(EntryType)",
            "ContextEntrySchema includes validation for all 12 fields",
            "id field uses z.string().regex(/^ctx_[a-zA-Z0-9]{8}$/) with custom error message",
            "entry_type uses z.nativeEnum(EntryType)",
            "source is validated as non-empty string with z.string().min(1)",
            "content is optional with z.string().optional()",
            "summary is required with z.string().min(1)",
            "created_at uses z.date() or z.string().datetime() for ISO strings",
            "references uses z.array(z.string()).default([])",
            "searchable uses z.boolean().default(true)",
            "compressed uses z.boolean().default(false)",
            "ttl is optional with z.number().positive().optional()",
            "parent_id is optional with z.string().optional()",
            "derived_from uses z.array(z.string()).default([])",
            "Cross-field validation: compressed=true requires content=undefined",
            "Cross-field validation: compressed=true requires summary to be non-empty",
            "Type inference: export type ContextEntry = z.infer<typeof ContextEntrySchema>",
            "All schemas are exported with named exports",
            "Validation errors include clear, actionable error messages",
            "Schema passes validation with valid sample data",
            "Schema rejects invalid data with appropriate error messages"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "src/context/validation.ts - EntryTypeSchema using z.nativeEnum()",
              "src/context/validation.ts - ContextEntrySchema with all field validations",
              "src/context/validation.ts - id field regex validation",
              "src/context/validation.ts - Cross-field validation using .refine()",
              "src/context/validation.ts - Custom error messages for all validations",
              "src/context/validation.ts - Type inference exports",
              "src/context/validation.ts - SearchResultSchema (if applicable)",
              "src/context/validation.ts - Schema export statements",
              "src/context/__tests__/validation.test.ts - Unit tests for valid data",
              "src/context/__tests__/validation.test.ts - Unit tests for invalid data rejection",
              "src/context/__tests__/validation.test.ts - Tests for cross-field validation rules"
            ]
          },
          "testable_properties": [],
          "function_id": "ContextValidation.createZodSchemas",
          "related_concepts": [
            "Zod schema definition",
            "Runtime type validation",
            "Type inference with z.infer<>",
            "Regex pattern validation",
            "Enum validation with z.nativeEnum()",
            "Optional field handling with z.optional()",
            "Default values with .default()",
            "Cross-field validation with .refine()",
            "Schema composition",
            "Error message customization"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_001.3",
          "description": "Port all planning_pipeline/models.py dataclasses to TypeScript interfaces including RequirementNode with hierarchical id structure, RequirementHierarchy, ImplementationComponents (frontend/backend/middleware/shared arrays), TestableProperty with 4 property types, and all constant enums (VALID_REQUIREMENT_TYPES, VALID_PROPERTY_TYPES, VALID_CATEGORIES)",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "RequirementCategory type union includes: 'functional', 'non_functional', 'quality_attribute', 'constraint'",
            "RequirementType type union includes all valid requirement types from Python constant",
            "PropertyType type union includes: 'invariant', 'round_trip', 'idempotence', 'oracle'",
            "ImplementationComponents interface has optional arrays: frontend?, backend?, middleware?, shared?",
            "TestableProperty interface includes: property_type (PropertyType), strategy (string), description (string)",
            "RequirementNode interface includes all fields: id, function_id, category, description, acceptance_criteria (string[]), related_concepts (optional string[]), parent_id (optional string), children (optional RequirementNode[]), implementation_components (optional ImplementationComponents), testable_properties (optional TestableProperty[])",
            "RequirementHierarchy interface includes: root_nodes (RequirementNode[]), metadata (optional record)",
            "Hierarchical id format is documented (e.g., 'parent_1.2.3')",
            "All constant arrays are exported as const assertions",
            "Type guards are provided for runtime category validation",
            "File is created at src/planning/models.ts",
            "All types compile without errors in TypeScript strict mode",
            "Recursive RequirementNode type correctly handles children array"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "src/planning/models.ts - RequirementCategory union type",
              "src/planning/models.ts - RequirementType union type",
              "src/planning/models.ts - PropertyType union type",
              "src/planning/models.ts - ImplementationComponents interface",
              "src/planning/models.ts - TestableProperty interface",
              "src/planning/models.ts - RequirementNode interface with recursive children",
              "src/planning/models.ts - RequirementHierarchy interface",
              "src/planning/models.ts - VALID_REQUIREMENT_TYPES constant array",
              "src/planning/models.ts - VALID_PROPERTY_TYPES constant array",
              "src/planning/models.ts - VALID_CATEGORIES constant array",
              "src/planning/models.ts - Type guard functions (isValidCategory, isValidPropertyType)",
              "src/planning/models.ts - JSDoc documentation with examples",
              "src/planning/models.ts - Export statements for all types and constants"
            ]
          },
          "testable_properties": [],
          "function_id": "PlanningModels.convertRequirementTypes",
          "related_concepts": [
            "Hierarchical data structures",
            "Tree-based requirement representation",
            "Component categorization",
            "Property-based testing types",
            "Self-referential types (parent-child relationships)",
            "Union types for categories",
            "Constant type definitions",
            "Type guards for category validation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_001.4",
          "description": "Implement Zod schemas for all planning models including RequirementNode with hierarchical validation, cross-field validation (children nodes must have parent_id matching parent), component array validation, property type validation, and acceptance criteria non-empty validation",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "RequirementCategorySchema validates against all valid categories using z.enum()",
            "RequirementTypeSchema validates against all valid requirement types",
            "PropertyTypeSchema validates against 4 property types: invariant, round_trip, idempotence, oracle",
            "ImplementationComponentsSchema validates optional arrays with z.array(z.string()).optional()",
            "TestablePropertySchema validates all 3 required fields with correct types",
            "RequirementNodeSchema uses z.lazy() for recursive children validation",
            "RequirementNodeSchema validates hierarchical id format (parent_X.Y.Z pattern)",
            "RequirementNodeSchema ensures acceptance_criteria is non-empty array",
            "Cross-field validation: if children exist, each child's parent_id must match parent's id",
            "Cross-field validation: related_concepts array contains unique values",
            "RequirementHierarchySchema validates root_nodes as array of RequirementNode",
            "All schemas provide type inference exports",
            "Schema validates valid requirement hierarchies without errors",
            "Schema rejects invalid hierarchies with clear error messages",
            "Performance: recursive validation completes in <100ms for trees with 100 nodes"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "src/planning/validation.ts - RequirementCategorySchema using z.enum()",
              "src/planning/validation.ts - RequirementTypeSchema using z.enum()",
              "src/planning/validation.ts - PropertyTypeSchema using z.enum()",
              "src/planning/validation.ts - ImplementationComponentsSchema with optional arrays",
              "src/planning/validation.ts - TestablePropertySchema with required fields",
              "src/planning/validation.ts - RequirementNodeSchema with z.lazy() for recursion",
              "src/planning/validation.ts - Hierarchical id validation regex",
              "src/planning/validation.ts - Cross-field validation for parent-child relationships",
              "src/planning/validation.ts - RequirementHierarchySchema",
              "src/planning/validation.ts - Type inference exports",
              "src/planning/__tests__/validation.test.ts - Valid hierarchy tests",
              "src/planning/__tests__/validation.test.ts - Invalid hierarchy rejection tests",
              "src/planning/__tests__/validation.test.ts - Parent-child relationship tests",
              "src/planning/__tests__/validation.test.ts - Performance tests for large trees"
            ]
          },
          "testable_properties": [],
          "function_id": "PlanningValidation.createRequirementSchemas",
          "related_concepts": [
            "Recursive schema validation",
            "Hierarchical data validation",
            "Cross-reference validation (parent-child ids)",
            "Array element validation",
            "Lazy schema evaluation for recursive types",
            "Schema composition",
            "Refinement rules for business logic",
            "Type inference from complex schemas"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_001.5",
          "description": "Create helper functions for type conversions including string-to-enum converters (parseEntryType, parsePropertyType), id generators (generateContextId with ctx_XXXXXXXX format), hierarchy traversal utilities (findNodeById, flattenHierarchy, validateHierarchyIntegrity), and serialization helpers (toJSON, fromJSON with date parsing)",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "parseEntryType(value: string): EntryType throws error for invalid values",
            "parsePropertyType(value: string): PropertyType throws error for invalid values",
            "generateContextId(): string returns format 'ctx_' + 8 alphanumeric characters",
            "generateContextId() uses crypto.randomBytes or crypto.getRandomValues for randomness",
            "generateContextId() generates unique IDs (collision probability < 1 in 1 million)",
            "findNodeById(hierarchy: RequirementHierarchy, id: string): RequirementNode | undefined traverses tree correctly",
            "flattenHierarchy(hierarchy: RequirementHierarchy): RequirementNode[] returns all nodes in depth-first order",
            "validateHierarchyIntegrity(hierarchy: RequirementHierarchy): ValidationResult checks parent-child id consistency",
            "validateHierarchyIntegrity detects orphaned nodes (parent_id references non-existent parent)",
            "validateHierarchyIntegrity detects circular references",
            "serializeContextEntry(entry: ContextEntry): string converts to JSON with Date to ISO string",
            "deserializeContextEntry(json: string): ContextEntry parses JSON and converts ISO string to Date",
            "All helper functions include error handling with descriptive error messages",
            "All functions are pure (no side effects)",
            "All functions include JSDoc with usage examples",
            "Type conversion helpers return Result types for safe error handling"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "src/context/helpers.ts - parseEntryType with validation",
              "src/context/helpers.ts - generateContextId using crypto",
              "src/context/helpers.ts - serializeContextEntry function",
              "src/context/helpers.ts - deserializeContextEntry function",
              "src/planning/helpers.ts - parsePropertyType with validation",
              "src/planning/helpers.ts - findNodeById with recursive search",
              "src/planning/helpers.ts - flattenHierarchy with depth-first traversal",
              "src/planning/helpers.ts - validateHierarchyIntegrity with all checks",
              "src/planning/helpers.ts - Result type for safe error handling",
              "src/shared/types.ts - Common utility types (Result, Option, etc.)",
              "src/context/__tests__/helpers.test.ts - Tests for context helpers",
              "src/planning/__tests__/helpers.test.ts - Tests for planning helpers",
              "src/context/__tests__/helpers.test.ts - ID generation uniqueness tests",
              "src/planning/__tests__/helpers.test.ts - Hierarchy integrity tests",
              "src/planning/__tests__/helpers.test.ts - Circular reference detection tests"
            ]
          },
          "testable_properties": [],
          "function_id": "TypeHelpers.implementConversionUtilities",
          "related_concepts": [
            "Type conversion utilities",
            "String parsing with validation",
            "ID generation with crypto randomness",
            "Tree traversal algorithms",
            "Depth-first search",
            "Breadth-first search",
            "Serialization/deserialization",
            "Date parsing and formatting",
            "Error handling for invalid conversions"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_002",
      "description": "The system must implement a CentralContextStore with Map-based storage supporting CRUD operations (add, get, getByType, remove), vector search with TF-IDF, context compression, statistics reporting, and JSON export capabilities",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_002.1",
          "description": "Initialize CentralContextStore class with Map-based storage for ContextEntry objects and VectorSearchIndex instance",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "CentralContextStore class is exported and instantiable",
            "Private entries property is initialized as Map<string, ContextEntry>",
            "Private searchIndex property is initialized as VectorSearchIndex instance",
            "Constructor accepts optional configuration for initial capacity or custom search index",
            "TypeScript strict mode passes with proper type annotations",
            "Instance methods are properly bound to class context"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "CentralContextStore class definition in src/context/store.ts",
              "Constructor with optional StoreConfig parameter",
              "Private readonly entries: Map<string, ContextEntry> field",
              "Private readonly searchIndex: VectorSearchIndex field"
            ],
            "middleware": [],
            "shared": [
              "StoreConfig interface for initialization options",
              "ContextEntry type imported from models.ts",
              "VectorSearchIndex type imported from search.ts"
            ]
          },
          "testable_properties": [],
          "function_id": "CentralContextStore.constructor",
          "related_concepts": [
            "Map<string, ContextEntry> data structure",
            "VectorSearchIndex initialization",
            "Dependency injection for search index",
            "Memory management for large entry collections"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.2",
          "description": "Add a new ContextEntry to the store with runtime Zod validation, automatic Map insertion, and optional search index registration",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "Method signature: add(entry: ContextEntry): void",
            "ContextEntrySchema.parse(entry) is called before insertion",
            "Throws ZodError if entry validation fails",
            "Entry is added to this.entries Map using entry.id as key",
            "If entry.searchable is true, entry is added to searchIndex",
            "If entry.searchable is false, entry is NOT added to searchIndex",
            "Duplicate entry IDs overwrite existing entries in Map",
            "Method completes synchronously without async operations"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "add() method in CentralContextStore class",
              "Runtime validation using ContextEntrySchema.parse()",
              "Map.set() operation for entries storage",
              "Conditional searchIndex.add() based on searchable flag",
              "Error handling for validation failures"
            ],
            "middleware": [
              "Zod validation catches malformed entry data before storage",
              "Entry ID format validation (ctx_[a-zA-Z0-9]{8})"
            ],
            "shared": [
              "ContextEntry type definition",
              "ContextEntrySchema Zod schema",
              "VectorSearchIndex.add() method signature"
            ]
          },
          "testable_properties": [],
          "function_id": "CentralContextStore.add",
          "related_concepts": [
            "Zod schema validation",
            "ContextEntrySchema runtime checks",
            "Searchable flag conditional indexing",
            "Duplicate entry handling",
            "Entry ID uniqueness enforcement"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.3",
          "description": "Retrieve a single ContextEntry by ID from Map storage, returning undefined if not found",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "Method signature: get(id: string): ContextEntry | undefined",
            "Returns ContextEntry if entry with matching id exists in Map",
            "Returns undefined if id does not exist in Map",
            "Does not throw errors for missing entries",
            "Lookup is O(1) time complexity using Map.get()",
            "Returns the exact entry object reference (no cloning)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "get() method in CentralContextStore class",
              "Direct Map.get(id) call",
              "Return type: ContextEntry | undefined"
            ],
            "middleware": [],
            "shared": [
              "ContextEntry type definition",
              "Entry ID string type"
            ]
          },
          "testable_properties": [],
          "function_id": "CentralContextStore.get",
          "related_concepts": [
            "Map.get() method",
            "Optional return types",
            "Entry existence checking",
            "O(1) lookup performance"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.4",
          "description": "Filter and return all ContextEntry objects matching a specific EntryType enum value",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "Method signature: getByType(entryType: EntryType): ContextEntry[]",
            "Returns array of ContextEntry objects where entry.entry_type === entryType",
            "Returns empty array if no entries match the type",
            "All EntryType enum values are supported (FILE, COMMAND, TASK, etc.)",
            "Returned array contains all matching entries (no limit)",
            "Order of entries in returned array is not guaranteed",
            "Does not modify stored entries"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "getByType() method in CentralContextStore class",
              "Array.from(this.entries.values()) to get all entries",
              "filter() with entry_type comparison",
              "Return filtered array of ContextEntry objects"
            ],
            "middleware": [],
            "shared": [
              "EntryType enum definition",
              "ContextEntry type definition with entry_type property"
            ]
          },
          "testable_properties": [],
          "function_id": "CentralContextStore.getByType",
          "related_concepts": [
            "Array.from() for Map.values()",
            "Array.filter() method",
            "EntryType enum comparison",
            "Type-based entry retrieval",
            "Entry categorization"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.5",
          "description": "Remove a ContextEntry from Map storage and search index by ID, returning boolean success indicator",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "Method signature: remove(id: string): boolean",
            "Returns true if entry existed and was removed",
            "Returns false if entry did not exist",
            "Entry is removed from this.entries Map using Map.delete()",
            "Entry is removed from searchIndex if it was searchable",
            "Method is idempotent (calling twice with same ID returns false second time)",
            "Does not throw errors for missing entries"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "remove() method in CentralContextStore class",
              "Retrieve entry before deletion to check searchable flag",
              "Map.delete(id) for entries removal",
              "searchIndex.remove(id) if entry was searchable",
              "Return boolean based on Map.delete() result"
            ],
            "middleware": [],
            "shared": [
              "VectorSearchIndex.remove() method signature",
              "Entry ID string type"
            ]
          },
          "testable_properties": [],
          "function_id": "CentralContextStore.remove",
          "related_concepts": [
            "Map.delete() method",
            "Search index synchronization",
            "Entry removal cascading",
            "Return value conventions",
            "Idempotent deletion"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.6",
          "description": "Check if an entry with the given ID exists in the store",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "Method signature: contains(id: string): boolean",
            "Returns true if entry with matching id exists in Map",
            "Returns false if id does not exist in Map",
            "Uses Map.has() for O(1) performance",
            "Does not throw errors for any input"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "contains() method in CentralContextStore class",
              "Direct Map.has(id) call",
              "Return boolean result"
            ],
            "middleware": [],
            "shared": [
              "Entry ID string type"
            ]
          },
          "testable_properties": [],
          "function_id": "CentralContextStore.contains",
          "related_concepts": [
            "Map.has() method",
            "Entry existence verification",
            "Boolean return values",
            "O(1) lookup performance"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.7",
          "description": "Perform TF-IDF vector search on searchable entries using VectorSearchIndex, returning ranked results with configurable limit",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "Method signature: search(query: string, limit: number = 10): SearchResult[]",
            "Delegates to this.searchIndex.search(query, limit)",
            "Returns array of SearchResult objects with id, score, and entry reference",
            "Results are sorted by descending similarity score",
            "Limit parameter defaults to 10 if not provided",
            "Only searchable entries are included in results",
            "Empty query returns empty array or top results based on implementation",
            "Returns up to limit results (fewer if insufficient matches)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "search() method in CentralContextStore class",
              "Direct delegation to searchIndex.search(query, limit)",
              "Return SearchResult[] array"
            ],
            "middleware": [],
            "shared": [
              "SearchResult type definition with id, score, entry properties",
              "VectorSearchIndex.search() method signature",
              "TF-IDF search algorithm in VectorSearchIndex"
            ]
          },
          "testable_properties": [],
          "function_id": "CentralContextStore.search",
          "related_concepts": [
            "VectorSearchIndex.search() delegation",
            "TF-IDF similarity ranking",
            "Search result limiting",
            "Searchable flag filtering",
            "Cosine similarity scoring"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.8",
          "description": "Compress a ContextEntry by clearing content field, setting compressed flag, and updating Map storage while preserving summary",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "Method signature: compress(id: string): void",
            "Throws error if entry with id does not exist",
            "Sets entry.compressed = true",
            "Sets entry.content = undefined (clears content)",
            "Preserves entry.summary field (does not modify)",
            "Updates Map storage with modified entry",
            "Removes entry from searchIndex if it was searchable (compressed entries are not searchable)",
            "Method completes synchronously"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "compress() method in CentralContextStore class",
              "Retrieve entry using Map.get(id)",
              "Throw error if entry is undefined",
              "Mutate entry: compressed = true, content = undefined",
              "Update Map with modified entry",
              "Remove from searchIndex if entry was searchable"
            ],
            "middleware": [],
            "shared": [
              "ContextEntry type with compressed and content properties",
              "Error class for missing entries"
            ]
          },
          "testable_properties": [],
          "function_id": "CentralContextStore.compress",
          "related_concepts": [
            "Context compression strategy",
            "Memory optimization",
            "Summary preservation",
            "Entry mutation",
            "Compressed flag management"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.9",
          "description": "Calculate and return store statistics including total entry count and breakdown by EntryType",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "Method signature: getStats(): { total: number; byType: Record<EntryType, number> }",
            "Returns object with total property equal to Map.size",
            "Returns byType object with counts for each EntryType present in store",
            "EntryType values not present in store may be omitted or have count 0",
            "All entries are counted exactly once in both total and byType",
            "Method completes synchronously with O(n) complexity",
            "Returns accurate counts even after add/remove operations"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "getStats() method in CentralContextStore class",
              "Initialize byType as empty Record<EntryType, number>",
              "Iterate over this.entries.values()",
              "Increment count for each entry's entry_type in byType",
              "Return { total: this.entries.size, byType }"
            ],
            "middleware": [],
            "shared": [
              "StoreStats type: { total: number; byType: Record<EntryType, number> }",
              "EntryType enum definition",
              "ContextEntry type with entry_type property"
            ]
          },
          "testable_properties": [],
          "function_id": "CentralContextStore.getStats",
          "related_concepts": [
            "Statistics aggregation",
            "Entry type counting",
            "Map.size property",
            "Record<EntryType, number> type",
            "Iteration over Map values"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.10",
          "description": "Export all stored ContextEntry objects as a plain JavaScript Record object for JSON serialization",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "Method signature: exportToDict(): Record<string, ContextEntry>",
            "Returns Record object with entry IDs as keys and ContextEntry objects as values",
            "All entries in Map are included in returned Record",
            "Returned object is serializable to JSON",
            "Method completes synchronously",
            "Returned object can be used with JSON.stringify() without errors",
            "Date objects in entries are preserved in exportable format"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "exportToDict() method in CentralContextStore class",
              "Use Object.fromEntries(this.entries) to convert Map to Record",
              "Return Record<string, ContextEntry> object"
            ],
            "middleware": [],
            "shared": [
              "ContextEntry type definition",
              "Record<string, ContextEntry> return type",
              "JSON serialization compatibility for ContextEntry fields"
            ]
          },
          "testable_properties": [],
          "function_id": "CentralContextStore.exportToDict",
          "related_concepts": [
            "Object.fromEntries() conversion",
            "Map to Record serialization",
            "JSON export preparation",
            "Entry cloning considerations",
            "Checkpoint persistence"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.11",
          "description": "Retrieve all ContextEntry objects from the store as an array",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "Method signature: getAll(): ContextEntry[]",
            "Returns array containing all ContextEntry objects in store",
            "Returns empty array if store is empty",
            "Order of entries in array is not guaranteed",
            "Method completes synchronously",
            "Returned array is independent copy (modifying it does not affect store)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "getAll() method in CentralContextStore class",
              "Use Array.from(this.entries.values())",
              "Return ContextEntry[] array"
            ],
            "middleware": [],
            "shared": [
              "ContextEntry type definition"
            ]
          },
          "testable_properties": [],
          "function_id": "CentralContextStore.getAll",
          "related_concepts": [
            "Map.values() iterator",
            "Array.from() conversion",
            "Full store retrieval",
            "Entry iteration"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_003",
      "description": "The system must implement a 7-step planning pipeline with sequential execution (Research, Memory Sync, Decomposition, Context Generation, Planning, Phase Decomposition, Beads Integration), checkpoint recovery at any step, interactive user prompts, and BAML integration for structured LLM outputs",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_003.1",
          "description": "Define and implement the PipelineStep interface that standardizes execution flow for all 7 pipeline steps (Research, Memory Sync, Decomposition, Context Generation, Planning, Phase Decomposition, Beads Integration)",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "PipelineStep interface defines required methods: execute(context: PipelineContext): Promise<StepResult>",
            "StepResult type includes success boolean, error details, and output data",
            "PipelineContext class maintains shared state across all steps",
            "Each of the 7 steps (ResearchStep, MemorySyncStep, DecompositionStep, ContextGenerationStep, PlanningStep, PhaseDecompositionStep, BeadsIntegrationStep) implements the PipelineStep interface",
            "Step name property is defined for each implementation for identification in checkpoints",
            "Each step validates its input from PipelineContext before execution",
            "Each step updates PipelineContext with its output upon successful completion",
            "Error handling wraps step execution failures in StepResult with descriptive error messages",
            "Type safety enforced through TypeScript interfaces and Zod schemas",
            "Unit tests verify each step implements the interface correctly with 100% coverage"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Define PipelineStep interface in src/planning/interfaces/pipeline-step.ts",
              "Create StepResult type with success, error, and data fields",
              "Create PipelineContext class to hold pipeline state and configuration",
              "Implement ResearchStep class in src/planning/steps/research-step.ts",
              "Implement MemorySyncStep class in src/planning/steps/memory-sync-step.ts",
              "Implement DecompositionStep class in src/planning/steps/decomposition-step.ts",
              "Implement ContextGenerationStep class in src/planning/steps/context-generation-step.ts",
              "Implement PlanningStep class in src/planning/steps/planning-step.ts",
              "Implement PhaseDecompositionStep class in src/planning/steps/phase-decomposition-step.ts",
              "Implement BeadsIntegrationStep class in src/planning/steps/beads-integration-step.ts",
              "Add async execute method to each step implementation",
              "Add input validation in each step's execute method"
            ],
            "middleware": [
              "Add step execution middleware to log step start/completion",
              "Add error catching middleware to wrap step failures"
            ],
            "shared": [
              "Define PipelineStep interface in shared types",
              "Define StepResult type in shared types",
              "Define PipelineContext class in shared models",
              "Define PipelineConfig interface for pipeline initialization",
              "Create Zod schemas for validating step inputs and outputs",
              "Create error types: StepExecutionError, StepValidationError"
            ]
          },
          "testable_properties": [],
          "function_id": "PipelineStep.interface",
          "related_concepts": [
            "Interface design",
            "Step abstraction",
            "Pipeline pattern",
            "Context passing",
            "Error handling",
            "Async execution"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_003.2",
          "description": "Create the PlanningPipeline orchestrator class that executes all 7 steps sequentially, manages step order, handles step failures, and coordinates data flow between steps",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "PlanningPipeline class instantiates with PipelineConfig containing all necessary configuration",
            "Pipeline maintains ordered array of all 7 steps in correct sequence",
            "run() method executes steps sequentially using async/await",
            "Each step receives the current PipelineContext state",
            "Pipeline propagates context updates from each step to the next",
            "Pipeline halts execution if any step returns success: false",
            "Pipeline returns PipelineResult with final state and all step outputs",
            "Pipeline supports force restart option to bypass checkpoint loading",
            "Pipeline logs step start, completion, and timing information",
            "Pipeline handles step failures gracefully with error context",
            "Integration test verifies all 7 steps execute in order with valid data flow",
            "Unit test verifies pipeline stops on first failure and returns error details"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Create PlanningPipeline class in src/planning/pipeline.ts",
              "Add constructor that accepts PipelineConfig and initializes all 7 step instances",
              "Add steps property as ordered array of PipelineStep instances",
              "Implement run() async method that iterates through steps sequentially",
              "Add step execution loop with await for each step.execute(context)",
              "Add context propagation logic after each successful step",
              "Add failure handling that breaks loop and returns error result",
              "Add timing instrumentation to track step execution duration",
              "Add logging calls for step lifecycle events (start, success, failure)",
              "Implement getResult() method to compile final PipelineResult",
              "Add forceRestart option handling to skip checkpoint loading"
            ],
            "middleware": [
              "Add pipeline-level middleware to track overall execution time",
              "Add middleware to emit events for pipeline start/complete/failure"
            ],
            "shared": [
              "Define PipelineConfig interface with all required configuration fields",
              "Define PipelineResult type with success, final context, and step results",
              "Define PipelineState enum (INITIALIZED, RUNNING, COMPLETED, FAILED)",
              "Create utility functions for step timing and logging",
              "Create error type: PipelineExecutionError with step context"
            ]
          },
          "testable_properties": [],
          "function_id": "PlanningPipeline.orchestrator",
          "related_concepts": [
            "Sequential execution",
            "Pipeline orchestration",
            "Step coordination",
            "Error propagation",
            "State management",
            "Async control flow"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_003.3",
          "description": "Implement checkpoint save/restore functionality that persists pipeline state after each step, enables recovery from any step, supports multiple checkpoint formats (JSON, binary), and manages checkpoint cleanup",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "CheckpointManager class provides save(stepName: string, context: PipelineContext): Promise<void> method",
            "CheckpointManager class provides load(stepName: string): Promise<PipelineContext | null> method",
            "Checkpoints are saved to disk after each successful step execution",
            "Checkpoint filenames include step name and timestamp for uniqueness",
            "Checkpoint data includes full PipelineContext serialized state",
            "Checkpoint data includes metadata: timestamp, step name, pipeline version",
            "load() method returns null if no checkpoint exists for given step",
            "load() method validates checkpoint schema before restoration",
            "PipelineContext is correctly restored with all nested objects and data",
            "CheckpointManager supports cleanup of old checkpoints (retention policy)",
            "Integration test verifies pipeline can resume from any step checkpoint",
            "Unit test verifies checkpoint serialization preserves all context data",
            "Unit test verifies corrupted checkpoint handling returns null gracefully"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Create CheckpointManager class in src/planning/checkpoint-manager.ts",
              "Add save() method that serializes PipelineContext to JSON",
              "Add load() method that reads and deserializes checkpoint file",
              "Implement checkpoint file naming convention with step name and timestamp",
              "Add file I/O operations using Node.js fs/promises API",
              "Add checkpoint directory creation if not exists",
              "Implement checkpoint metadata storage (timestamp, version, step)",
              "Add checkpoint validation using Zod schema",
              "Implement cleanup() method to remove old checkpoints based on retention policy",
              "Add error handling for file read/write failures",
              "Add logging for checkpoint save/load operations"
            ],
            "middleware": [],
            "shared": [
              "Define CheckpointMetadata interface with timestamp, step, version fields",
              "Define CheckpointData type combining metadata and context",
              "Create Zod schema for validating checkpoint structure on load",
              "Define checkpoint file path utilities (getCheckpointPath, getCheckpointDir)",
              "Create error types: CheckpointSaveError, CheckpointLoadError, CheckpointCorruptedError",
              "Define CHECKPOINT_RETENTION_DAYS constant for cleanup policy"
            ]
          },
          "testable_properties": [],
          "function_id": "CheckpointManager.saveRestore",
          "related_concepts": [
            "State persistence",
            "Checkpoint recovery",
            "File serialization",
            "State restoration",
            "Crash recovery",
            "Incremental progress"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_003.4",
          "description": "Integrate BAML TypeScript client (@baml/client) for structured LLM outputs, configure BAML function definitions, implement type-safe LLM calls for requirement decomposition and planning steps, and add BAML error handling",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "BAML TypeScript client (@baml/client) is installed as dependency",
            "BAML client is initialized with API key and configuration",
            "BAML function definitions are ported from Python baml_src/ directory",
            "DecompositionStep calls BAML client for requirement decomposition with typed output",
            "PlanningStep calls BAML client for implementation plan generation",
            "ContextGenerationStep calls BAML client for tech stack analysis",
            "BAML responses are validated against Zod schemas for type safety",
            "BAML client errors are caught and wrapped in domain-specific error types",
            "BAML retry logic is implemented for transient failures (rate limits, timeouts)",
            "Integration tests use mock BAML client to avoid real API calls",
            "E2E tests marked with @integration use real BAML client",
            "Unit test verifies BAML response parsing handles all expected schemas",
            "Unit test verifies BAML error handling returns appropriate error types"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Install @baml/client npm package",
              "Create BAMLClientWrapper class in src/integrations/baml-client.ts",
              "Initialize BAML client with API key from environment variable",
              "Port BAML function definitions from baml_src/ to TypeScript BAML format",
              "Create decomposeRequirements() method that calls BAML decomposition function",
              "Create generatePlan() method that calls BAML planning function",
              "Create analyzeTechStack() method that calls BAML tech stack function",
              "Add response parsing logic to convert BAML outputs to domain types",
              "Add Zod schema validation for all BAML response types",
              "Implement retry logic with exponential backoff for transient errors",
              "Add error handling that wraps BAML errors in BAMLIntegrationError",
              "Add logging for BAML request/response cycles"
            ],
            "middleware": [
              "Add middleware to track BAML API call metrics (latency, success rate)",
              "Add middleware to enforce rate limiting on BAML calls"
            ],
            "shared": [
              "Define BAMLConfig interface with API key, retry settings, timeout",
              "Define BAML response types matching Python models (RequirementHierarchy, TechStack, etc.)",
              "Create Zod schemas for validating BAML responses",
              "Define error types: BAMLIntegrationError, BAMLValidationError, BAMLRateLimitError",
              "Define constants: BAML_MAX_RETRIES, BAML_RETRY_DELAY_MS, BAML_TIMEOUT_MS",
              "Create utility functions for BAML response transformation"
            ]
          },
          "testable_properties": [],
          "function_id": "BAMLClient.integration",
          "related_concepts": [
            "BAML SDK integration",
            "LLM orchestration",
            "Structured outputs",
            "Type safety",
            "Prompt engineering",
            "BAML function definitions"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_003.5",
          "description": "Implement interactive prompt system for user control at pipeline checkpoints, allowing users to revise steps, restart pipeline, continue execution, or abort, with clear status display and input validation",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "InteractivePromptManager class provides prompt(options: PromptOptions): Promise<UserChoice> method",
            "Prompts are displayed at each checkpoint with step status summary",
            "User is presented with options: Continue, Restart, Revise Step, Abort",
            "Continue option resumes pipeline from next step",
            "Restart option clears checkpoints and re-runs pipeline from step 1",
            "Revise Step option allows user to modify step configuration and re-run",
            "Abort option gracefully exits pipeline with cleanup",
            "Input validation ensures only valid choices are accepted",
            "Prompt displays current step name, completion status, and execution time",
            "Prompt displays summary of step output (truncated if large)",
            "Non-interactive mode (--non-interactive flag) skips prompts and auto-continues",
            "Integration test verifies all prompt choices trigger correct pipeline behavior",
            "Unit test verifies input validation rejects invalid choices",
            "Unit test verifies non-interactive mode bypasses prompts"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Create InteractivePromptManager class in src/planning/interactive-prompt.ts",
              "Add prompt() method that displays options and waits for user input",
              "Integrate prompts library (inquirer or prompts) for terminal UI",
              "Add step status display with name, duration, and success/failure indication",
              "Add step output summary display (truncate long outputs)",
              "Implement Continue choice handler that returns CONTINUE signal",
              "Implement Restart choice handler that clears checkpoints and returns RESTART signal",
              "Implement Revise Step choice handler that prompts for new config and returns REVISE signal",
              "Implement Abort choice handler that returns ABORT signal",
              "Add input validation that re-prompts on invalid input",
              "Add --non-interactive CLI flag handling to bypass prompts",
              "Integrate prompt calls into PlanningPipeline after each step checkpoint"
            ],
            "middleware": [],
            "shared": [
              "Define PromptOptions interface with step info, status, output summary",
              "Define UserChoice enum (CONTINUE, RESTART, REVISE, ABORT)",
              "Define PromptConfig interface with non-interactive mode flag",
              "Create utility function formatStepSummary(step: StepResult): string",
              "Create utility function truncateOutput(text: string, maxLength: number): string"
            ]
          },
          "testable_properties": [],
          "function_id": "InteractivePrompt.system",
          "related_concepts": [
            "User interaction",
            "Interactive CLI",
            "Checkpoint control",
            "User decision flow",
            "Input validation",
            "Terminal UI"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_004",
      "description": "The system must provide a dual-LLM architecture where the Working LLM (Orchestrator) sees summaries only with no entry bounds and the Implementation LLM (Workers) sees full content bounded to less than 200 entries, with addressable context entries using unique ctx_XXXXX IDs",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_004.1",
          "description": "Implement WorkingLLMContext that provides summary-only access to context entries with no entry bounds, enabling the orchestrator LLM to make decisions based on compressed information without full content details",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "WorkingLLMContext class must only include summary fields from ContextEntry objects, never the full content field",
            "Context building must work with unlimited number of entries (no bounds checking)",
            "All entry types (FILE, COMMAND, TASK, SEARCH_RESULT, etc.) must be supported with summary-only access",
            "Context must include entry metadata (id, type, source, created_at, references) along with summaries",
            "Compressed entries must be handled identically to non-compressed entries (both return summary only)",
            "Context output must be formatted for LLM consumption (structured text or JSON)",
            "Token estimation must be available to track approximate context size",
            "Context building must preserve entry ordering and relationships via parent_id/derived_from references",
            "Unit tests must verify no content field is ever included in output",
            "Integration tests must verify context building with 500+ entries succeeds without bounds errors"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "WorkingLLMContext class with buildContext() method",
              "Summary extraction logic from CentralContextStore",
              "Token estimation service for summary-only context",
              "Context formatting service for LLM-readable output"
            ],
            "middleware": [],
            "shared": [
              "ContextEntry interface/type with summary field",
              "EntryType enum definition",
              "Context formatting utilities",
              "Token counting utilities"
            ]
          },
          "testable_properties": [],
          "function_id": "WorkingLLMContext.buildContext",
          "related_concepts": [
            "context compression",
            "summary extraction",
            "orchestrator decision making",
            "memory optimization",
            "dual-LLM architecture"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_004.2",
          "description": "Implement ImplementationLLMContext that provides full content access bounded to less than 200 entries, with strict validation to prevent context overflow for implementation worker LLMs",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "ImplementationLLMContext must include full content field (not just summary) from ContextEntry objects",
            "Context building must throw EntryBoundsError if entry count equals or exceeds 200",
            "Bounds validation must occur before context construction begins",
            "Context must support filtering by entry type, source, or other criteria while maintaining bounds",
            "Full content must be included for all non-compressed entries",
            "Compressed entries must include summary only with clear indication of compression status",
            "Context output must include all metadata (id, type, source, references, created_at, parent_id, derived_from)",
            "Token estimation must account for full content size, not just summaries",
            "Unit tests must verify bounds validation rejects 200+ entries",
            "Unit tests must verify full content is included for entries within bounds",
            "Integration tests must verify realistic implementation tasks stay within 200-entry limit"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "ImplementationLLMContext class with buildContextWithBounds() method",
              "Bounds validation service that counts entries before construction",
              "Full content extraction from CentralContextStore",
              "EntryBoundsError exception class with descriptive error messages",
              "Token estimation service for full-content context"
            ],
            "middleware": [],
            "shared": [
              "ContextEntry interface with content and compressed fields",
              "EntryBoundsError exception type",
              "Constants for MAX_IMPLEMENTATION_ENTRIES (199)",
              "Context formatting utilities for full content"
            ]
          },
          "testable_properties": [],
          "function_id": "ImplementationLLMContext.buildContextWithBounds",
          "related_concepts": [
            "context bounds validation",
            "full content access",
            "entry limit enforcement",
            "worker LLM constraints",
            "error handling for bounds violation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_004.3",
          "description": "Create context entry ID generation system using the ctx_XXXXX pattern where XXXXX is a unique alphanumeric identifier, ensuring global uniqueness and consistent format across all entry types",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "ID generation must produce identifiers matching regex pattern ^ctx_[a-zA-Z0-9]{8}$",
            "Generated IDs must be globally unique within a context store instance",
            "ID generation must be deterministic or use sufficient entropy to prevent collisions",
            "IDs must be assigned at entry creation time and remain immutable",
            "All entry types must use the same ID generation mechanism",
            "ID validation function must reject malformed IDs",
            "Unit tests must verify ID format matches pattern for 10,000 generated IDs",
            "Unit tests must verify no collisions occur in 10,000 generated IDs",
            "Property-based tests must verify ID pattern invariant holds for all generated IDs",
            "IDs must be serializable to JSON and reconstructible from JSON without loss"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "generateContextId() function using crypto.randomBytes or nanoid",
              "validateContextId() function for format checking",
              "ID collision detection in CentralContextStore.add()",
              "ID format constants and regex patterns"
            ],
            "middleware": [],
            "shared": [
              "CONTEXT_ID_PATTERN constant (regex)",
              "CONTEXT_ID_LENGTH constant (8 characters)",
              "ContextEntry interface with id: string field",
              "Type guard: isValidContextId(id: string): boolean"
            ]
          },
          "testable_properties": [],
          "function_id": "ContextEntry.generateId",
          "related_concepts": [
            "unique identifier generation",
            "ID format validation",
            "addressable context",
            "entry reference system",
            "collision prevention"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_004.4",
          "description": "Add task batching system that groups implementation tasks while respecting the 200-entry limit, ensuring each batch contains all necessary context entries for successful task execution without exceeding bounds",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "TaskBatcher must accept a list of TaskSpec objects and output TaskBatch objects",
            "Each TaskBatch must contain task specifications plus required context entry IDs",
            "Total context entries per batch (task entries + referenced entries + dependencies) must be < 200",
            "Batching algorithm must resolve entry dependencies via parent_id and derived_from fields",
            "Tasks with shared context entries must be grouped into the same batch when possible",
            "Tasks that cannot fit into a batch due to excessive dependencies must be flagged with error",
            "Batch ordering must respect task dependencies (prerequisites before dependents)",
            "Unit tests must verify batches stay under 200-entry limit with varying task sizes",
            "Unit tests must verify dependency resolution includes all required entries",
            "Integration tests must verify realistic task lists produce valid batches",
            "Property-based tests must verify batch size invariant holds for all input combinations"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "TaskBatcher class with batchTasks() method",
              "Dependency resolution algorithm that traverses parent_id and derived_from",
              "Batch size calculation including tasks + dependencies",
              "Batch validation service that enforces < 200 entry limit",
              "Task scheduling service that orders batches by dependencies"
            ],
            "middleware": [],
            "shared": [
              "TaskSpec interface with id, description, required_context_ids fields",
              "TaskBatch interface with tasks, context_entry_ids, total_entries fields",
              "BatchingError exception for over-limit tasks",
              "Constants for MAX_BATCH_ENTRIES (199)"
            ]
          },
          "testable_properties": [],
          "function_id": "TaskBatcher.batchTasks",
          "related_concepts": [
            "task grouping",
            "context optimization",
            "entry limit management",
            "dependency resolution",
            "batch scheduling"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_004.5",
          "description": "Implement context manager with request/release lifecycle for ImplementationLLMContext, enabling explicit resource management and preventing context leaks through controlled allocation and deallocation",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "ContextManager must provide requestContext(entryIds: string[]) method that returns ImplementationLLMContext",
            "requestContext must validate entry count < 200 before allocation",
            "ContextManager must provide releaseContext(contextId: string) method for cleanup",
            "releaseContext must mark context entries as available for compression/cleanup",
            "Unreleased contexts must be tracked and reported as warnings/errors",
            "TypeScript async generator pattern (async function*) must be supported for try/finally guarantees",
            "Context request must fail-fast if entries are not found in CentralContextStore",
            "Multiple concurrent contexts must be supported with independent lifecycles",
            "Unit tests must verify contexts are released after use",
            "Unit tests must verify unreleased contexts are tracked",
            "Integration tests must verify context lifecycle in realistic implementation workflows",
            "Error handling must ensure context release occurs even on exceptions"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "ContextManager class with requestContext() and releaseContext() methods",
              "Active context tracking with Map<contextId, ContextMetadata>",
              "Context ID generation for manager-level tracking (separate from entry IDs)",
              "Cleanup service that identifies unreleased contexts",
              "Async generator wrapper: requestAndRelease() for automatic cleanup",
              "Context statistics tracking (request count, release count, active contexts)"
            ],
            "middleware": [],
            "shared": [
              "ContextMetadata interface with contextId, entryIds, requestedAt, releasedAt fields",
              "ContextManagerError exception for lifecycle violations",
              "Async generator type signature for requestAndRelease pattern",
              "Constants for CONTEXT_TIMEOUT_MS (warn on long-lived contexts)"
            ]
          },
          "testable_properties": [],
          "function_id": "ContextManager.requestAndRelease",
          "related_concepts": [
            "resource management",
            "context lifecycle",
            "memory management",
            "async context managers",
            "cleanup guarantees"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_005",
      "description": "The system must provide CLI tools built with commander framework supporting three commands: 'orchestrate' for context-engineered orchestration with new/existing project options, 'loop' for autonomous feature implementation with configurable model selection, and 'plan' for planning-focused orchestration",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_005.1",
          "description": "Initialize commander CLI framework with TypeScript configuration, establish project structure for CLI commands, configure tsup/esbuild for CLI binary compilation, and set up package.json bin entry point",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [
            "package.json includes commander dependency and bin entry point mapping to compiled CLI file",
            "tsup or esbuild configuration compiles TypeScript CLI to executable JavaScript with shebang",
            "CLI responds to --version flag with correct version from package.json",
            "CLI responds to --help flag with usage information for all commands",
            "TypeScript strict mode enabled with proper type definitions for Command instances",
            "CLI executable can be run directly via 'silmari' command after npm link or global install"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Create src/cli.ts entry point with commander Program initialization",
              "Configure Command instance with name, description, and version",
              "Set up CLI build process to generate executable with #!/usr/bin/env node shebang",
              "Implement global error handler for uncaught CLI exceptions"
            ],
            "middleware": [],
            "shared": [
              "Define CLIOptions interface for shared command options",
              "Create package.json bin configuration mapping 'silmari' to dist/cli.js",
              "Define tsup/esbuild configuration for CLI bundling with target: node"
            ]
          },
          "testable_properties": [],
          "function_id": "CLI.setupCommanderFramework",
          "related_concepts": [
            "commander.js",
            "TypeScript CLI",
            "npm bin entry",
            "CLI argument parsing",
            "help text generation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_005.2",
          "description": "Create 'orchestrate' command with mutually exclusive --new and --project options for project path handling, integrate Orchestrator class, validate path arguments, and handle context-engineered orchestration workflow",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [
            "Command 'silmari orchestrate --new <path>' creates new project and starts orchestration",
            "Command 'silmari orchestrate --project <path>' continues existing project orchestration",
            "Error thrown if both --new and --project are provided simultaneously",
            "Error thrown if neither --new nor --project is provided",
            "Path argument validated for existence (--project) or parent directory existence (--new)",
            "--model option accepts 'sonnet' or 'opus' with default value 'sonnet'",
            "Orchestrator instance initialized with validated options and executed",
            "Help text displays clear usage examples for both new and existing projects"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Register 'orchestrate' command with commander using program.command('orchestrate')",
              "Add --new <path> option with description 'Create new project at specified path'",
              "Add --project <path> option with description 'Continue existing project at specified path'",
              "Add --model <model> option with choices ['sonnet', 'opus'] and default 'sonnet'",
              "Implement action handler validating mutual exclusivity of --new and --project",
              "Instantiate Orchestrator class with resolved options object",
              "Call orchestrator.run() method and handle async execution",
              "Implement graceful shutdown on SIGINT/SIGTERM"
            ],
            "middleware": [
              "Path validation middleware checking file system existence and permissions",
              "Model option validation ensuring value is 'sonnet' or 'opus'"
            ],
            "shared": [
              "Define OrchestrateOptions interface with newPath, projectPath, and model fields",
              "Create path resolution utility normalizing relative to absolute paths",
              "Define ModelType type as 'sonnet' | 'opus'"
            ]
          },
          "testable_properties": [],
          "function_id": "CLI.implementOrchestrateCommand",
          "related_concepts": [
            "commander command",
            "option validation",
            "mutually exclusive options",
            "path resolution",
            "Orchestrator class integration"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_005.3",
          "description": "Create 'loop' command accepting project path as positional argument, implement autonomous session management via LoopRunner class, support model selection, and enable continuous feature implementation loop",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [
            "Command 'silmari loop <project-path>' starts autonomous loop for specified project",
            "Positional argument <project-path> is required and validated for existence",
            "--model option accepts 'sonnet' or 'opus' with default value 'sonnet'",
            "LoopRunner instance initialized with project path and model selection",
            "Loop executes continuously until all features complete or max sessions reached",
            "Graceful shutdown on Ctrl+C preserves current session state",
            "Error handling reports session failures and provides recovery options",
            "Help text explains autonomous loop behavior and session limits"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Register 'loop' command with commander using program.command('loop')",
              "Add required positional argument <project-path> with validation",
              "Add --model <model> option with choices ['sonnet', 'opus'] and default 'sonnet'",
              "Implement action handler instantiating LoopRunner class",
              "Call loopRunner.start() method with async/await",
              "Implement SIGINT handler for graceful loop termination",
              "Add --max-sessions option with default 100 for iteration limits",
              "Implement session count tracking and reporting"
            ],
            "middleware": [
              "Project path validation checking directory exists and contains required files",
              "Feature list validation ensuring features.json or equivalent exists"
            ],
            "shared": [
              "Define LoopOptions interface with projectPath, model, and maxSessions fields",
              "Create LoopRunner class with start(), getNextFeature(), runClaudeSession() methods",
              "Define Session interface tracking session number, feature ID, and status"
            ]
          },
          "testable_properties": [],
          "function_id": "CLI.implementLoopCommand",
          "related_concepts": [
            "commander argument",
            "autonomous loop",
            "session management",
            "LoopRunner class",
            "continuous execution"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_005.4",
          "description": "Implement model selection functionality supporting 'sonnet' and 'opus' models across all commands, validate model choices, propagate model configuration to Orchestrator and LoopRunner classes, and ensure consistent model usage",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [
            "All commands accept --model option with choices ['sonnet', 'opus']",
            "Default model is 'sonnet' when --model option not provided",
            "Invalid model values rejected with clear error message listing valid choices",
            "Model selection propagated to Orchestrator and LoopRunner constructor options",
            "Model configuration accessible to Claude SDK invocation logic",
            "Model choice displayed in CLI output for user confirmation",
            "Environment variable SILMARI_DEFAULT_MODEL can override default if set"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Create --model option using commander.option() with choices: ['sonnet', 'opus']",
              "Implement model option validator rejecting values outside allowed set",
              "Add default value resolution checking SILMARI_DEFAULT_MODEL env var then 'sonnet'",
              "Pass model value to Orchestrator/LoopRunner via options object",
              "Log selected model at command execution start"
            ],
            "middleware": [
              "Model validation middleware ensuring value in ['sonnet', 'opus']"
            ],
            "shared": [
              "Define ModelType as string literal union type: 'sonnet' | 'opus'",
              "Create VALID_MODELS constant array ['sonnet', 'opus']",
              "Define getDefaultModel() utility checking environment then returning 'sonnet'",
              "Add model field to all command option interfaces (OrchestrateOptions, LoopOptions)"
            ]
          },
          "testable_properties": [],
          "function_id": "CLI.addModelSelectionSupport",
          "related_concepts": [
            "model configuration",
            "option validation",
            "enum types",
            "configuration propagation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_005.5",
          "description": "Create main CLI entry point with version display from package.json, comprehensive help text, command registration, error handling, and executable configuration for npm distribution",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [
            "CLI entry point file (src/cli.ts) contains shebang #!/usr/bin/env node",
            "Version flag --version displays version from package.json",
            "Help flag --help displays program description and lists all available commands",
            "All commands (orchestrate, loop, plan) registered and accessible",
            "Global error handler catches and formats errors with clear messages",
            "Exit codes properly set: 0 for success, 1 for user errors, 2 for system errors",
            "Compiled CLI executable has correct permissions (chmod +x)",
            "CLI runs successfully via 'npx silmari' or global 'silmari' command",
            "Help text includes examples for each command showing common usage patterns"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Create src/cli.ts with #!/usr/bin/env node shebang at top",
              "Import commander and initialize Program instance",
              "Set program name to 'silmari' via program.name()",
              "Set program description via program.description() explaining context engine purpose",
              "Load version from package.json and set via program.version()",
              "Register all command modules (orchestrate, loop, plan)",
              "Implement try-catch wrapper calling program.parseAsync(process.argv)",
              "Implement error handler categorizing errors and setting appropriate exit codes",
              "Add process.on('unhandledRejection') handler for async errors",
              "Add process.on('uncaughtException') handler for sync errors"
            ],
            "middleware": [],
            "shared": [
              "Define CLIError class extending Error with exitCode property",
              "Create formatError() utility function for consistent error display",
              "Define exit code constants: SUCCESS=0, USER_ERROR=1, SYSTEM_ERROR=2",
              "Add version extraction utility reading package.json version field"
            ]
          },
          "testable_properties": [],
          "function_id": "CLI.createCLIEntryPoint",
          "related_concepts": [
            "CLI entry point",
            "version management",
            "help text",
            "error handling",
            "executable packaging"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_005.6",
          "description": "Create 'plan' command for planning-focused orchestration, integrate PlanningOrchestrator class, support project path argument, enable model selection, and provide planning-specific options",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [
            "Command 'silmari plan <project-path>' starts planning orchestration for project",
            "Positional argument <project-path> required and validated",
            "--model option accepts 'sonnet' or 'opus' with default 'sonnet'",
            "--step option allows starting from specific pipeline step (research, decomposition, planning, etc.)",
            "--interactive flag enables checkpoint prompts for user control",
            "PlanningOrchestrator instance initialized with validated options",
            "Planning pipeline executes through all 7 steps unless --step provided",
            "Help text explains planning process and available pipeline steps"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Register 'plan' command with commander using program.command('plan')",
              "Add required positional argument <project-path>",
              "Add --model <model> option with choices ['sonnet', 'opus'] and default 'sonnet'",
              "Add --step <step> option with choices matching pipeline step names",
              "Add --interactive boolean flag for checkpoint prompts",
              "Implement action handler validating project path and options",
              "Instantiate PlanningOrchestrator class with options",
              "Call orchestrator.run() and handle async execution",
              "Display planning progress with step indicators"
            ],
            "middleware": [
              "Project path validation checking directory structure",
              "Step name validation ensuring value matches valid pipeline steps"
            ],
            "shared": [
              "Define PlanningOptions interface with projectPath, model, startStep, and interactive fields",
              "Create PipelineStep type as union of step name strings",
              "Define PIPELINE_STEPS constant array with all 7 step names"
            ]
          },
          "testable_properties": [],
          "function_id": "CLI.implementPlanCommand",
          "related_concepts": [
            "planning orchestration",
            "requirement decomposition",
            "planning pipeline",
            "PlanningOrchestrator class"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_006",
      "description": "The system must achieve 90%+ test coverage using Vitest for unit and integration tests, fast-check for property-based testing, with test markers for slow/integration/e2e tests, mocking of BAML clients and Claude SDK responses, and comprehensive fixtures for all modules",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_006.1",
          "description": "Configure Vitest testing framework with coverage reporting to track code coverage across all modules and ensure 90%+ threshold is maintained",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "vitest.config.ts file exists with coverage configuration set to 90% minimum for lines, branches, functions, and statements",
            "Coverage provider (c8 or istanbul) is configured and functional",
            "Test file patterns include **/*.test.ts and **/*.spec.ts",
            "Coverage excludes node_modules, dist, and test files themselves",
            "npm scripts include 'test', 'test:watch', 'test:coverage', and 'test:ui' commands",
            "Coverage reports generate in multiple formats: text, html, json, and lcov",
            "CI/CD pipeline integrates coverage reporting and fails builds below 90% threshold",
            "Coverage output directory is added to .gitignore",
            "Global test setup and teardown hooks are configured if needed",
            "TypeScript paths are resolved correctly in test environment"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "vitest.config.ts configuration file with coverage thresholds",
              "package.json scripts for test execution",
              "tsconfig.test.json for test-specific TypeScript configuration",
              ".gitignore entries for coverage output",
              "Global test setup file (vitest.setup.ts) for shared test configuration",
              "Coverage reporter configuration (text-summary, html, lcov)"
            ]
          },
          "testable_properties": [],
          "function_id": "TestSetup.configureVitestWithCoverage",
          "related_concepts": [
            "vitest.config.ts configuration",
            "coverage thresholds",
            "test execution modes",
            "Istanbul/c8 coverage providers",
            "test file patterns",
            "source file exclusions"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_006.2",
          "description": "Implement property-based tests using fast-check to validate invariants across core data models including ContextEntry, RequirementNode, and pipeline state transitions",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "fast-check library is installed with Vitest integration (@fast-check/vitest)",
            "Property tests exist for ContextEntry ID format validation (ctx_[a-f0-9]{8})",
            "Property tests verify compressed entries never have content field populated",
            "Property tests validate parent-child relationship consistency in RequirementNode hierarchy",
            "Property tests ensure pipeline checkpoint state can be serialized and deserialized without data loss",
            "Property tests verify EntryType discriminated union exhaustiveness",
            "Property tests validate task batching respects <200 entry limit across all combinations",
            "Custom arbitraries are created for complex domain types (ContextEntry, RequirementNode)",
            "Property tests run at least 1000 iterations per test by default",
            "Shrinking behavior is verified for failing test cases",
            "Property test failures include counterexamples in output",
            "At least 20 property-based tests are implemented across all core modules"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "Custom fast-check arbitraries for ContextEntry generation (arbitrary.contextEntry.ts)",
              "Custom fast-check arbitraries for RequirementNode generation (arbitrary.requirementNode.ts)",
              "Custom fast-check arbitraries for EntryType enum (arbitrary.entryType.ts)",
              "Property test suite for context/models.ts invariants",
              "Property test suite for planning/models.ts invariants",
              "Property test suite for context/store.ts operations",
              "Property test suite for context/batching.ts entry limits",
              "Helper functions for property test assertions",
              "Shrinking strategies for complex domain types"
            ]
          },
          "testable_properties": [],
          "function_id": "PropertyTesting.implementFastCheckInvariants",
          "related_concepts": [
            "property-based testing",
            "fast-check arbitraries",
            "invariant validation",
            "state machine properties",
            "data model constraints",
            "hypothesis testing equivalents"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_006.3",
          "description": "Create comprehensive mock system for BAML client responses and Claude SDK interactions to enable isolated unit testing without external API dependencies",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "Mock BAML client interface matches @baml/client TypeScript API",
            "Mock factory functions exist for all BAML function responses (decompose, extractTechStack, etc.)",
            "Mock Claude SDK responses cover success, error, and edge cases",
            "Vitest mock utilities (vi.mock, vi.fn) are used consistently",
            "Mock responses include realistic data matching production Zod schemas",
            "Mock system supports both sync and async BAML operations",
            "Mock factories accept configuration for customizing response data",
            "Spy functions track BAML client invocation count and arguments",
            "Mock system includes error simulation for network failures, timeouts, and API errors",
            "Default mock implementations exist in shared test utilities",
            "Mock reset/cleanup occurs between test cases automatically",
            "Documentation exists for creating new mocks following established patterns"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "Mock BAML client implementation (mocks/baml-client.mock.ts)",
              "Mock Claude SDK implementation (mocks/claude-sdk.mock.ts)",
              "Mock factory for DecompositionResult (fixtures/decomposition.fixture.ts)",
              "Mock factory for RequirementHierarchy (fixtures/requirement-hierarchy.fixture.ts)",
              "Mock factory for TechStack extraction (fixtures/tech-stack.fixture.ts)",
              "Mock factory for Pipeline context (fixtures/pipeline-context.fixture.ts)",
              "Mock factory for Claude API responses (fixtures/claude-responses.fixture.ts)",
              "Test utilities for mock configuration (test-utils/mock-config.ts)",
              "Type definitions for mock interfaces (types/mocks.d.ts)",
              "Global mock setup in vitest.setup.ts"
            ]
          },
          "testable_properties": [],
          "function_id": "MockSystem.createBAMLAndClaudeSDKMocks",
          "related_concepts": [
            "test doubles",
            "mock factories",
            "fixture data",
            "API response mocking",
            "dependency injection",
            "test isolation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_006.4",
          "description": "Implement test markers and configuration to categorize tests as slow, integration, or e2e, with separate execution modes and CI/CD pipeline integration",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "Custom Vitest test markers exist for @slow, @integration, and @e2e tests using describe.extend or test.extend",
            "vitest.config.ts includes workspace configuration for separating unit and integration tests",
            "Environment variable TEST_MODE controls which test suites run (unit|integration|e2e|all)",
            "npm scripts exist for 'test:unit', 'test:integration', 'test:e2e', and 'test:all'",
            "Integration tests marked with @integration skip by default in local development",
            "E2E tests marked with @e2e require explicit opt-in via environment variable",
            "Slow tests marked with @slow have increased timeout values (30s+)",
            "CI/CD pipeline runs unit tests on all PRs, integration tests on main branch only",
            "Test output distinguishes between test categories in reports",
            "Integration tests can access real BAML and Claude SDK with API keys",
            "E2E tests use isolated test workspaces that are cleaned up after execution",
            "Documentation explains when to use each test category and marker"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "Vitest workspace configuration (vitest.workspace.ts) for test categorization",
              "Custom test marker utilities (test-utils/markers.ts)",
              "Environment configuration for TEST_MODE (config/test-env.ts)",
              "package.json scripts for categorized test execution",
              "CI/CD workflow files with conditional test execution (.github/workflows/test.yml)",
              "Integration test configuration (vitest.config.integration.ts)",
              "E2E test configuration (vitest.config.e2e.ts)",
              "Test timeout configuration utilities (test-utils/timeouts.ts)",
              "Test workspace setup and teardown helpers (test-utils/workspace.ts)",
              "Documentation on test categorization strategy (docs/testing-strategy.md)"
            ]
          },
          "testable_properties": [],
          "function_id": "TestMarkers.addIntegrationAndE2EConfiguration",
          "related_concepts": [
            "test categorization",
            "conditional test execution",
            "CI/CD optimization",
            "test suite organization",
            "custom test runners",
            "environment-specific testing"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_006.5",
          "description": "Build comprehensive test fixtures and helper utilities for all modules including context management, planning pipeline, and orchestration to reduce test boilerplate and ensure consistency",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "Fixture factories exist for all core data models (ContextEntry, RequirementNode, PipelineContext, TaskBatch)",
            "Builder pattern implementations allow fluent fixture customization (e.g., contextEntryBuilder().withType().compressed().build())",
            "Fixtures include valid, invalid, and edge case data variations",
            "Helper functions exist for common test assertions (assertContextEntryValid, assertRequirementHierarchyComplete)",
            "Test utilities provide setup/teardown for CentralContextStore state",
            "Fixture data matches production Zod schema validation rules",
            "Fixtures support both minimal and complete data generation",
            "Helper functions for file system operations in tests (createTempDir, cleanupTempFiles)",
            "Helper functions for subprocess mocking (mockSpawn, mockExec)",
            "Fixtures for BAML prompt templates and responses",
            "Helper functions for checkpoint state creation and validation",
            "All fixtures are co-located with test files or in shared test-utils directory",
            "TypeScript type inference works correctly with fixture builders",
            "Documentation exists for all fixture factories and helper functions"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "ContextEntry fixture builder (fixtures/builders/context-entry.builder.ts)",
              "RequirementNode fixture builder (fixtures/builders/requirement-node.builder.ts)",
              "PipelineContext fixture builder (fixtures/builders/pipeline-context.builder.ts)",
              "TaskBatch fixture builder (fixtures/builders/task-batch.builder.ts)",
              "SearchResult fixture builder (fixtures/builders/search-result.builder.ts)",
              "Test assertion helpers (test-utils/assertions.ts)",
              "File system test utilities (test-utils/fs-helpers.ts)",
              "Subprocess mock utilities (test-utils/process-helpers.ts)",
              "Store state management utilities (test-utils/store-helpers.ts)",
              "Checkpoint test utilities (test-utils/checkpoint-helpers.ts)",
              "BAML prompt fixture templates (fixtures/baml-prompts/)",
              "Sample requirement hierarchies (fixtures/data/requirement-hierarchies.json)",
              "Sample tech stack configurations (fixtures/data/tech-stacks.json)",
              "Global test setup with fixture registration (vitest.setup.ts)",
              "Fixture documentation (docs/testing-fixtures.md)"
            ]
          },
          "testable_properties": [],
          "function_id": "TestFixtures.buildComprehensiveFixturesAndHelpers",
          "related_concepts": [
            "test data builders",
            "fixture factories",
            "test helpers",
            "data seeding",
            "test utilities",
            "shared test infrastructure"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_007",
      "description": "The system must support production deployment with Docker multi-stage builds (builder and production images), docker-compose configuration for workspace volumes, Claude Code CLI installation, environment variable management for ANTHROPIC_API_KEY, and Node.js 20-alpine base image",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_007.1",
          "description": "Create multi-stage Dockerfile with builder stage that installs dependencies, copies source code, and builds the TypeScript application using pnpm and tsup",
          "type": "sub_process",
          "parent_id": "REQ_007",
          "children": [],
          "acceptance_criteria": [
            "Builder stage uses node:20-alpine as base image",
            "Builder stage sets WORKDIR to /app",
            "Builder stage copies package.json and pnpm-lock.yaml before other files for layer caching",
            "Builder stage runs 'corepack enable' to enable pnpm",
            "Builder stage runs 'pnpm install --frozen-lockfile' to install exact dependency versions",
            "Builder stage copies all source files with 'COPY . .'",
            "Builder stage runs 'pnpm build' to compile TypeScript to JavaScript",
            "Builder stage produces compiled output in /app/dist directory",
            "Dockerfile includes comments explaining each stage purpose",
            "Build stage completes successfully without errors"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "Dockerfile with FROM node:20-alpine AS builder",
              "Package manager configuration (corepack enable)",
              "Build script configuration in package.json",
              "TypeScript compiler configuration (tsconfig.json)",
              "Build output directory structure"
            ]
          },
          "testable_properties": [],
          "function_id": "Docker.MultiStageBuilder",
          "related_concepts": [
            "multi-stage builds",
            "build optimization",
            "dependency caching",
            "layer optimization"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_007.2",
          "description": "Configure production image stage with minimal dependencies, copying only built artifacts and production node_modules from builder stage, excluding dev dependencies and source files",
          "type": "sub_process",
          "parent_id": "REQ_007",
          "children": [],
          "acceptance_criteria": [
            "Production stage uses node:20-alpine as base image",
            "Production stage sets WORKDIR to /app",
            "Production stage copies only /app/dist from builder stage using COPY --from=builder",
            "Production stage copies only production node_modules from builder stage",
            "Production stage copies package.json for metadata",
            "Production stage sets NODE_ENV=production environment variable",
            "Production stage defines CMD to run the compiled CLI: ['node', 'dist/cli.js']",
            "Production image size is significantly smaller than builder image",
            "Production image does not contain TypeScript source files",
            "Production image does not contain dev dependencies",
            "Image successfully starts and runs CLI commands"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "Production stage definition in Dockerfile",
              "COPY --from=builder directives for selective artifact copying",
              "ENV NODE_ENV=production configuration",
              "CMD entrypoint configuration",
              "Runtime dependencies validation"
            ]
          },
          "testable_properties": [],
          "function_id": "Docker.ProductionImage",
          "related_concepts": [
            "image size optimization",
            "production security",
            "runtime dependencies",
            "alpine linux"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_007.3",
          "description": "Set up docker-compose.yml with service definition for silmari-ts container, volume mounts for workspace and Claude config persistence, environment variable passthrough, and restart policies",
          "type": "sub_process",
          "parent_id": "REQ_007",
          "children": [],
          "acceptance_criteria": [
            "docker-compose.yml uses version '3.8' or higher",
            "Service named 'silmari-ts' is defined",
            "Service references build context '.' for local Dockerfile",
            "Volume mount './workspace:/workspace' maps host workspace to container",
            "Named volume 'claude-config:/root/.claude' persists Claude configuration",
            "Environment section includes ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY} with variable substitution",
            "Environment section includes NODE_ENV=production",
            "Command override specified: ['node', 'dist/cli.js', 'loop', '/workspace']",
            "Volume 'claude-config' is declared in top-level volumes section",
            "Service can be started with 'docker-compose up'",
            "Workspace files are accessible inside container at /workspace",
            "Claude configuration persists across container restarts",
            "ANTHROPIC_API_KEY is successfully passed from host .env file"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "docker-compose.yml file with service definitions",
              "Volume declarations for workspace and config persistence",
              "Environment variable configuration with ${} substitution",
              "Command override for CLI execution",
              "Documentation of required .env file format"
            ]
          },
          "testable_properties": [],
          "function_id": "Docker.ComposeConfiguration",
          "related_concepts": [
            "docker compose",
            "volume management",
            "service orchestration",
            "configuration management"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_007.4",
          "description": "Add installation step in production Dockerfile stage to globally install @anthropic-ai/claude-code CLI using npm, ensuring it's available in PATH for subprocess execution",
          "type": "sub_process",
          "parent_id": "REQ_007",
          "children": [],
          "acceptance_criteria": [
            "Production stage includes 'RUN npm install -g @anthropic-ai/claude-code' command",
            "Installation step occurs after base image setup but before CMD",
            "Claude Code CLI is accessible via 'claude' command in container",
            "Installation completes without errors during build",
            "Command 'which claude' returns valid path when container runs",
            "Claude Code CLI version can be verified with 'claude --version'",
            "Installation uses npm (not pnpm) for global package compatibility",
            "Claude binary is in /usr/local/bin or equivalent PATH location"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Child process spawning for 'claude' command in loop runner",
              "CLI argument passing for model selection and prompts",
              "Output stream handling from Claude subprocess"
            ],
            "middleware": [],
            "shared": [
              "RUN command in Dockerfile for global npm install",
              "PATH verification in container startup",
              "Claude Code CLI wrapper functions in TypeScript",
              "Error handling for missing CLI tool"
            ]
          },
          "testable_properties": [],
          "function_id": "Docker.ClaudeCodeInstallation",
          "related_concepts": [
            "npm global packages",
            "CLI tool installation",
            "runtime dependencies",
            "PATH configuration"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_007.5",
          "description": "Configure environment variable management with NODE_ENV set to production, ANTHROPIC_API_KEY passed from host environment, support for .env file loading, and validation of required variables at runtime",
          "type": "sub_process",
          "parent_id": "REQ_007",
          "children": [],
          "acceptance_criteria": [
            "Dockerfile sets ENV NODE_ENV=production in production stage",
            "docker-compose.yml maps ANTHROPIC_API_KEY from host to container",
            "Application validates ANTHROPIC_API_KEY is present at startup",
            "Application throws descriptive error if required environment variables are missing",
            "Example .env.example file documents all required variables",
            "Example .env.example includes ANTHROPIC_API_KEY=your_key_here placeholder",
            ".env file is listed in .gitignore to prevent secret commits",
            ".dockerignore includes .env to prevent secrets in build context",
            "Documentation explains how to set environment variables for deployment",
            "Container logs startup confirmation including environment configuration status",
            "Runtime validation checks NODE_ENV value and applies appropriate logging/error handling"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Environment variable validation at application startup",
              "Error messages for missing required variables",
              "Configuration loading from process.env",
              "Runtime checks for ANTHROPIC_API_KEY validity"
            ],
            "middleware": [
              "Environment validation middleware for API key presence",
              "NODE_ENV-based behavior switching (dev vs production logging)"
            ],
            "shared": [
              "ENV declarations in Dockerfile",
              "Environment variable type definitions (TypeScript interface)",
              ".env.example template file",
              ".gitignore and .dockerignore entries",
              "Configuration validation utility functions",
              "Deployment documentation for environment setup"
            ]
          },
          "testable_properties": [],
          "function_id": "Docker.EnvironmentConfiguration",
          "related_concepts": [
            "environment variables",
            "secrets management",
            "configuration validation",
            "dotenv"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_008",
      "description": "The system must implement vector search using TF-IDF with bag-of-words tokenization and cosine similarity calculation, matching Python implementation accuracy, supporting search result ranking with configurable limit, and integration with CentralContextStore for searchable entries",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_008.1",
          "description": "Implement TF-IDF (Term Frequency-Inverse Document Frequency) algorithm with bag-of-words tokenization for text vectorization, supporting document indexing and term frequency calculation",
          "type": "sub_process",
          "parent_id": "REQ_008",
          "children": [],
          "acceptance_criteria": [
            "Tokenizer splits text into individual words using whitespace and punctuation delimiters",
            "Text is normalized to lowercase before tokenization to ensure case-insensitive matching",
            "Special characters and punctuation are removed or handled consistently with Python implementation",
            "Term frequency (TF) is calculated as the number of times a term appears in a document",
            "Document frequency (DF) tracks the number of documents containing each term",
            "Inverse document frequency (IDF) is calculated as log(total_documents / document_frequency)",
            "TF-IDF score is computed as TF * IDF for each term in each document",
            "Empty strings and whitespace-only tokens are filtered out",
            "Tokenization results match Python implementation for identical input text",
            "Token vectors are stored in a sparse format to optimize memory usage",
            "Support for updating IDF values when new documents are added to the index"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "Tokenizer utility function with normalization and cleaning",
              "TermFrequency interface/type with term-to-count mapping",
              "DocumentFrequency interface/type tracking terms across documents",
              "TfIdfVector interface/type representing document vectors",
              "IDF calculation function: calculateIdf(totalDocs, docFrequency)",
              "TF calculation function: calculateTf(termCount, totalTerms)",
              "Text normalization utility (lowercase, punctuation removal)",
              "Sparse vector data structure (Map<string, number> for term weights)",
              "Constants for tokenization (delimiters, stop words if applicable)"
            ]
          },
          "testable_properties": [],
          "function_id": "VectorSearchIndex.implementTfIdfTokenization",
          "related_concepts": [
            "natural language processing",
            "text tokenization",
            "term frequency calculation",
            "inverse document frequency",
            "bag-of-words model",
            "text normalization",
            "stop word handling"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_008.2",
          "description": "Implement cosine similarity calculation for ranking search results based on vector similarity between query and indexed documents",
          "type": "sub_process",
          "parent_id": "REQ_008",
          "children": [],
          "acceptance_criteria": [
            "Cosine similarity is calculated as dot_product(A, B) / (magnitude(A) * magnitude(B))",
            "Dot product correctly handles sparse vectors with non-overlapping terms",
            "Vector magnitude is calculated as sqrt(sum of squared term weights)",
            "Similarity scores are normalized to range [0, 1] where 1 indicates identical vectors",
            "Zero-magnitude vectors are handled gracefully (return similarity of 0)",
            "Calculation matches Python implementation output for identical vector inputs",
            "Numerical precision matches Python (within acceptable floating-point tolerance)",
            "Performance is optimized for sparse vectors (only iterate over non-zero terms)",
            "Function supports both document-to-document and query-to-document similarity",
            "Edge cases handled: empty vectors, single-term vectors, identical vectors"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "CosineSimilarity calculation function accepting two TfIdfVectors",
              "DotProduct utility function for sparse vector multiplication",
              "VectorMagnitude utility function calculating Euclidean norm",
              "SimilarityScore type (number in range 0-1)",
              "Vector normalization utility (optional, for pre-normalized vectors)",
              "Sparse vector iteration utility to avoid zero-value operations",
              "Numerical comparison utility with epsilon tolerance for floating-point",
              "Unit tests comparing outputs with Python reference implementation"
            ]
          },
          "testable_properties": [],
          "function_id": "VectorSearchIndex.calculateCosineSimilarity",
          "related_concepts": [
            "vector similarity",
            "dot product calculation",
            "vector magnitude",
            "euclidean norm",
            "similarity scoring",
            "sparse vector operations",
            "numerical precision"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_008.3",
          "description": "Create VectorSearchIndex class with add() method for indexing entries and search() method for querying, maintaining document vectors and term statistics",
          "type": "sub_process",
          "parent_id": "REQ_008",
          "children": [],
          "acceptance_criteria": [
            "VectorSearchIndex class maintains a collection of indexed documents with IDs",
            "add(entry: ContextEntry) method tokenizes content and updates TF-IDF index",
            "add() method updates document frequency counts for all terms in the entry",
            "add() method recalculates IDF values for affected terms after insertion",
            "search(query: string, limit: number) method returns ranked SearchResult array",
            "search() tokenizes the query and converts it to a TF-IDF vector",
            "search() calculates cosine similarity between query vector and all document vectors",
            "search() sorts results by similarity score in descending order",
            "search() respects the limit parameter to return top N results",
            "Class maintains internal state: document vectors, term frequencies, document frequencies",
            "Index statistics are accessible: total documents, total terms, average document length",
            "Support for checking if a document ID is already indexed",
            "Thread-safe or idempotent operations for concurrent access considerations"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "VectorSearchIndex class with private state (documents, term stats)",
              "add(entry: ContextEntry): void method for indexing documents",
              "search(query: string, limit?: number): SearchResult[] method",
              "Private method: indexDocument(id: string, content: string)",
              "Private method: updateDocumentFrequencies(terms: string[])",
              "Private method: recalculateIdf()",
              "Private method: vectorizeQuery(query: string): TfIdfVector",
              "SearchResult interface: { id: string, score: number, entry?: ContextEntry }",
              "Internal storage: Map<string, TfIdfVector> for document vectors",
              "Internal storage: Map<string, number> for document frequencies",
              "Internal storage: Map<string, ContextEntry> for entry metadata (optional)",
              "getStats() method returning index statistics",
              "contains(id: string): boolean method to check if document is indexed"
            ]
          },
          "testable_properties": [],
          "function_id": "VectorSearchIndex.createSearchIndexClass",
          "related_concepts": [
            "inverted index",
            "document storage",
            "incremental indexing",
            "index statistics",
            "query processing",
            "result ranking",
            "index state management"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_008.4",
          "description": "Integrate VectorSearchIndex with CentralContextStore to automatically index searchable entries and provide search functionality through the store interface",
          "type": "sub_process",
          "parent_id": "REQ_008",
          "children": [],
          "acceptance_criteria": [
            "CentralContextStore maintains a private VectorSearchIndex instance",
            "When add(entry) is called, if entry.searchable is true, entry is added to search index",
            "Search index only indexes the entry.content or entry.summary field (consistent with Python)",
            "Compressed entries are indexed using their summary instead of content",
            "search(query: string, limit?: number) method delegates to VectorSearchIndex.search()",
            "Search results return SearchResult objects with entry references",
            "Removing an entry from the store removes it from the search index (if implemented)",
            "compress(id) operation updates the search index to use summary instead of content",
            "Index is initialized lazily or during store construction",
            "Search functionality gracefully handles empty index (returns empty results)",
            "Integration tests verify entries are searchable immediately after addition",
            "Search results exclude non-searchable entries even if manually indexed"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "Update CentralContextStore class to include private searchIndex: VectorSearchIndex",
              "Modify add(entry: ContextEntry) to call searchIndex.add(entry) if entry.searchable",
              "Add search(query: string, limit?: number): SearchResult[] public method",
              "Update compress(id: string) to re-index entry with summary content",
              "Add optional remove(id: string) method that also removes from search index",
              "Helper method: getIndexableContent(entry: ContextEntry): string",
              "Helper method to determine if entry should be indexed based on searchable flag",
              "Integration between EntryType enum and searchable flag (certain types default searchable)",
              "StoreSearchResult type extending SearchResult with full ContextEntry data",
              "Unit tests mocking VectorSearchIndex to verify integration logic",
              "Integration tests with real VectorSearchIndex verifying end-to-end search"
            ]
          },
          "testable_properties": [],
          "function_id": "CentralContextStore.integrateVectorSearch",
          "related_concepts": [
            "store-index coupling",
            "automatic indexing",
            "searchable flag filtering",
            "lazy initialization",
            "index synchronization",
            "entry lifecycle management",
            "search facade pattern"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_008.5",
          "description": "Add configurable result limit and ranking options to search functionality, supporting pagination, score thresholds, and result ordering customization",
          "type": "sub_process",
          "parent_id": "REQ_008",
          "children": [],
          "acceptance_criteria": [
            "search() method accepts optional limit parameter (default 10, matching Python)",
            "Results are always sorted by cosine similarity score in descending order (highest first)",
            "limit parameter enforces maximum number of results returned (top-k selection)",
            "search() supports optional minScore threshold to filter low-relevance results",
            "Zero or negative limit values are handled (return all results or error)",
            "Search with limit larger than total documents returns all matching documents",
            "Results array is ordered consistently (deterministic ordering for equal scores)",
            "Performance optimization: limit applied during scoring to avoid scoring all documents (if possible)",
            "SearchOptions interface allows future extension (sorting, filters, pagination)",
            "Ranking configuration validates input parameters (limit > 0, minScore in [0,1])",
            "Default configuration matches Python implementation behavior exactly"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "SearchOptions interface: { limit?: number, minScore?: number, orderBy?: 'score' }",
              "Update search() signature: search(query: string, options?: SearchOptions): SearchResult[]",
              "Default SearchOptions constant: DEFAULT_SEARCH_OPTIONS = { limit: 10, minScore: 0 }",
              "Result filtering logic: filter results where score >= minScore",
              "Result sorting logic: sort by score descending, then by ID for determinism",
              "Top-k selection: Array.slice(0, limit) after sorting",
              "Input validation for SearchOptions (limit must be positive integer, minScore in range)",
              "Performance optimization: early termination if limit reached during scoring (heap-based)",
              "Unit tests for edge cases: limit 0, limit > total docs, minScore = 1.0",
              "Benchmark tests comparing performance with Python implementation",
              "Documentation for SearchOptions parameters and defaults"
            ]
          },
          "testable_properties": [],
          "function_id": "VectorSearchIndex.configureResultRanking",
          "related_concepts": [
            "result pagination",
            "score thresholding",
            "ranking algorithms",
            "result filtering",
            "search configuration",
            "top-k selection",
            "performance optimization"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_009",
      "description": "The system must implement BeadsController as a typed wrapper for beads CLI supporting createIssue with title/type/description/priority, listIssues with status/type filters and JSON output parsing, closeIssue operation, and addDependency for issue relationships using child_process async execution",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_009.1",
          "description": "Initialize BeadsController class with typed configuration, error handling setup, and CLI path validation",
          "type": "sub_process",
          "parent_id": "REQ_009",
          "children": [],
          "acceptance_criteria": [
            "BeadsController class accepts optional config object with beadsCliPath property",
            "Constructor validates beads CLI is available on system PATH or at specified path",
            "Constructor throws descriptive error if beads CLI not found",
            "Constructor initializes internal state for tracking subprocess operations",
            "Constructor sets default timeout values for CLI operations (e.g., 30 seconds)",
            "Constructor can be instantiated with TypeScript strict mode enabled",
            "Constructor validates that Node.js version supports child_process.spawn promises"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "BeadsController class definition with constructor",
              "CLI path validation logic using which/where commands",
              "Configuration interface for optional parameters",
              "Error classes for CLI not found scenarios"
            ],
            "middleware": [],
            "shared": [
              "BeadsControllerConfig interface with optional beadsCliPath property",
              "BeadsControllerError custom error class",
              "Constants for default timeout values",
              "Type definitions for constructor parameters"
            ]
          },
          "testable_properties": [],
          "function_id": "BeadsController.constructor",
          "related_concepts": [
            "dependency injection",
            "configuration management",
            "CLI path resolution",
            "error handling initialization"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_009.2",
          "description": "Implement createIssue method with comprehensive parameter validation, type safety, and async CLI execution returning issue ID",
          "type": "sub_process",
          "parent_id": "REQ_009",
          "children": [],
          "acceptance_criteria": [
            "Method accepts CreateIssueOptions object with title (required), type (required), description (optional), priority (optional)",
            "Type parameter restricted to 'task' | 'epic' | 'bug' using TypeScript literal types",
            "Priority parameter validated as number between 1-5 if provided",
            "Title and description parameters sanitized to prevent command injection",
            "Method uses child_process.spawn to execute 'bd create' command asynchronously",
            "CLI arguments properly escaped for shell execution",
            "Method captures stdout and parses issue ID from output",
            "Method returns Promise<string> containing created issue ID",
            "Method throws typed error if CLI execution fails with non-zero exit code",
            "Method includes timeout handling (default 30 seconds) for CLI execution",
            "Method validates that title is non-empty string before execution",
            "Zod schema validates all parameters before CLI invocation"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "createIssue async method implementation",
              "CLI argument builder function for create command",
              "stdout parser to extract issue ID from beads output",
              "Error handling for non-zero exit codes",
              "Timeout implementation using AbortController"
            ],
            "middleware": [
              "Parameter sanitization to prevent command injection",
              "Input validation using Zod schemas before CLI execution"
            ],
            "shared": [
              "CreateIssueOptions interface with title, type, description, priority properties",
              "IssueType type alias: 'task' | 'epic' | 'bug'",
              "CreateIssueOptionsSchema Zod schema for runtime validation",
              "CLI execution utility function for spawning processes",
              "String sanitization utility for shell arguments"
            ]
          },
          "testable_properties": [],
          "function_id": "BeadsController.createIssue",
          "related_concepts": [
            "parameter validation",
            "type safety with Zod schemas",
            "async subprocess execution",
            "CLI argument escaping",
            "stdout parsing"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_009.3",
          "description": "Implement listIssues method with optional filter support, JSON output parsing, and typed return values",
          "type": "sub_process",
          "parent_id": "REQ_009",
          "children": [],
          "acceptance_criteria": [
            "Method accepts optional ListIssuesFilters object with status and type properties",
            "Status filter restricted to 'open' | 'closed' using TypeScript literal types",
            "Type filter accepts any string value matching beads issue types",
            "Method executes 'bd list --format=json' with optional filter arguments",
            "Method parses JSON output from stdout into array of BeadsIssue objects",
            "Method returns Promise<BeadsIssue[]> with strongly typed issue objects",
            "Method handles empty result set returning empty array",
            "Method throws typed error if JSON parsing fails",
            "Method validates JSON structure matches BeadsIssue schema using Zod",
            "Method handles CLI errors gracefully with descriptive error messages",
            "Method supports no filters returning all issues",
            "BeadsIssue type includes id, title, type, status, priority, description properties"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "listIssues async method implementation",
              "CLI argument builder for list command with filters",
              "JSON parsing logic for beads output",
              "Error handling for malformed JSON responses",
              "Result transformation from CLI output to typed objects"
            ],
            "middleware": [
              "Filter parameter validation using Zod schemas",
              "JSON schema validation for CLI output"
            ],
            "shared": [
              "ListIssuesFilters interface with optional status and type properties",
              "IssueStatus type alias: 'open' | 'closed'",
              "BeadsIssue interface with id, title, type, status, priority, description, created_at, updated_at",
              "BeadsIssueSchema Zod schema for validating parsed JSON",
              "ListIssuesFiltersSchema Zod schema for filter validation",
              "JSON parsing utility with error handling"
            ]
          },
          "testable_properties": [],
          "function_id": "BeadsController.listIssues",
          "related_concepts": [
            "optional filtering",
            "JSON parsing",
            "type transformation",
            "error handling for malformed JSON",
            "filter parameter validation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_009.4",
          "description": "Implement closeIssue operation with issue ID validation and confirmation of closure",
          "type": "sub_process",
          "parent_id": "REQ_009",
          "children": [],
          "acceptance_criteria": [
            "Method accepts issue ID as string parameter",
            "Issue ID validated as non-empty string before execution",
            "Method executes 'bd close <issue-id>' command asynchronously",
            "Method returns Promise<void> on successful closure",
            "Method throws typed error if issue ID not found",
            "Method throws typed error if CLI execution fails",
            "Method handles case where issue already closed (idempotent behavior)",
            "Method validates issue ID format matches expected pattern (if applicable)",
            "Method includes timeout handling for CLI execution",
            "Method provides descriptive error messages for different failure scenarios"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "closeIssue async method implementation",
              "CLI execution for close command",
              "Exit code validation and error mapping",
              "Idempotency logic for already-closed issues"
            ],
            "middleware": [
              "Issue ID format validation",
              "Parameter sanitization for shell execution"
            ],
            "shared": [
              "Issue ID validation utility function",
              "CloseIssueError custom error class with error codes",
              "Constants for expected issue ID format patterns"
            ]
          },
          "testable_properties": [],
          "function_id": "BeadsController.closeIssue",
          "related_concepts": [
            "issue ID validation",
            "state mutation operations",
            "operation confirmation",
            "idempotency handling"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_009.5",
          "description": "Implement addDependency operation for creating issue relationships with bidirectional validation",
          "type": "sub_process",
          "parent_id": "REQ_009",
          "children": [],
          "acceptance_criteria": [
            "Method accepts source issue ID and target issue ID as parameters",
            "Both issue IDs validated as non-empty strings",
            "Method executes 'bd add-dependency <source-id> <target-id>' command",
            "Method returns Promise<void> on successful dependency creation",
            "Method throws typed error if either issue ID not found",
            "Method throws typed error if dependency already exists (idempotent behavior acceptable)",
            "Method validates that source and target IDs are different (prevent self-dependency)",
            "Method includes timeout handling for CLI execution",
            "Method provides descriptive error messages distinguishing between source/target errors",
            "Method handles potential circular dependency detection if beads CLI supports it"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "addDependency async method implementation",
              "CLI execution for add-dependency command",
              "Validation logic to prevent self-dependencies",
              "Error handling for existing dependencies",
              "Exit code interpretation for various failure scenarios"
            ],
            "middleware": [
              "Issue ID validation for both source and target",
              "Self-dependency prevention validation",
              "Parameter sanitization for shell execution"
            ],
            "shared": [
              "AddDependencyOptions interface (if additional options needed)",
              "DependencyError custom error class with specific error codes",
              "Validation utility for issue ID pairs",
              "Constants for dependency-related error messages"
            ]
          },
          "testable_properties": [],
          "function_id": "BeadsController.addDependency",
          "related_concepts": [
            "graph relationships",
            "dependency validation",
            "circular dependency prevention",
            "relationship types"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_009.6",
          "description": "Implement shared async CLI execution utility using child_process.spawn with comprehensive error handling, timeout support, and output capture",
          "type": "sub_process",
          "parent_id": "REQ_009",
          "children": [],
          "acceptance_criteria": [
            "Utility function uses child_process.spawn for all CLI executions",
            "Function accepts command arguments as array of strings",
            "Function captures both stdout and stderr streams",
            "Function uses AbortController for timeout implementation",
            "Function returns Promise with stdout, stderr, and exit code",
            "Function throws timeout error if execution exceeds configured timeout",
            "Function properly handles process cleanup on timeout or error",
            "Function buffers output streams into strings for parsing",
            "Function handles UTF-8 encoding for output streams",
            "Function propagates process exit code for error handling",
            "Function logs command execution for debugging (optional based on config)",
            "Function handles process spawn errors (command not found, permission denied)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "executeCliCommand private async method",
              "Stream buffering logic for stdout/stderr",
              "AbortController integration for timeout",
              "Process cleanup on completion/error/timeout",
              "Spawn error handling",
              "Exit code propagation"
            ],
            "middleware": [
              "Command argument validation",
              "Logging integration for CLI execution tracking"
            ],
            "shared": [
              "CliExecutionResult interface with stdout, stderr, exitCode properties",
              "CliExecutionOptions interface with timeout, cwd, env properties",
              "TimeoutError custom error class",
              "SpawnError custom error class",
              "Utility function for converting streams to strings",
              "Constants for default timeout values and buffer sizes"
            ]
          },
          "testable_properties": [],
          "function_id": "BeadsController.executeCliCommand",
          "related_concepts": [
            "subprocess management",
            "async process execution",
            "stream handling",
            "timeout mechanisms",
            "signal handling",
            "output buffering"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_010",
      "description": "The system must implement requirement decomposition with BAML-based structured extraction, hierarchical ID generation (parent_1.2.3 format), three-tier hierarchy (parent, sub_process, implementation), component breakdown (frontend/backend/middleware/shared), and testable properties with property types (invariant, round_trip, idempotence, oracle)",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_010.1",
          "description": "Implement BAML-based requirement extraction service that parses natural language requirements into structured format with type validation",
          "type": "sub_process",
          "parent_id": "REQ_010",
          "children": [],
          "acceptance_criteria": [
            "BAML function definition created with proper schema for requirement extraction",
            "BAML client successfully invokes extraction with timeout handling",
            "Extracted requirements contain all required fields: description, category, acceptance_criteria",
            "BAML responses are validated against Zod schema before processing",
            "Extraction errors return structured DecompositionError with appropriate error codes",
            "BAML prompt template includes examples for few-shot learning",
            "Extraction handles both single requirements and batch processing",
            "Successfully extracts related_concepts and cross-cutting concerns",
            "Integration tests verify extraction with real BAML client",
            "Mock BAML responses used in unit tests for fast feedback",
            "Extraction preserves original requirement text for traceability"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "BAML function definition in baml_src/ directory with RequirementExtraction schema",
              "DecompositionService class with extractRequirements() async method",
              "BAML client wrapper with retry logic and timeout configuration",
              "Error mapping from BAML errors to DecompositionError types",
              "Batch extraction method for processing multiple requirements",
              "Result validation service using Zod schemas"
            ],
            "middleware": [
              "Request validation for extraction input format",
              "Response transformation from BAML format to internal models",
              "Error handling middleware for BAML client failures"
            ],
            "shared": [
              "RequirementExtractionSchema Zod schema for BAML responses",
              "DecompositionError class with error codes (BAML_FAILURE, VALIDATION_FAILED, TIMEOUT)",
              "BAMLConfig interface for client configuration",
              "ExtractionResult type definition",
              "RequirementCategory enum (functional, non_functional, technical, business)"
            ]
          },
          "testable_properties": [],
          "function_id": "BAMLExtractor.extractRequirements",
          "related_concepts": [
            "BAML client integration",
            "structured LLM outputs",
            "requirement parsing",
            "schema validation",
            "error handling for extraction failures"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_010.2",
          "description": "Create deterministic hierarchical ID generation algorithm that produces unique identifiers in parent_1.2.3 format with validation and collision detection",
          "type": "sub_process",
          "parent_id": "REQ_010",
          "children": [],
          "acceptance_criteria": [
            "ID format strictly follows pattern: parent_X.Y.Z where X is parent level, Y is sub-process level, Z is implementation level",
            "generateID() method accepts parent ID and sibling count parameters",
            "Top-level requirements generate IDs like parent_1, parent_2, parent_3",
            "Sub-process requirements generate IDs like parent_1.1, parent_1.2, parent_2.1",
            "Implementation requirements generate IDs like parent_1.1.1, parent_1.1.2",
            "ID validation regex correctly identifies valid and invalid formats",
            "parseID() method extracts depth level and parent reference from ID",
            "getNextSiblingID() method increments last number in sequence",
            "Collision detection prevents duplicate ID generation",
            "Property-based tests verify ID uniqueness across 1000+ generations",
            "IDs maintain sortable lexicographic order for hierarchy traversal"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "HierarchicalIDGenerator class with generateID() static method",
              "ID validation service with regex pattern matching",
              "parseID() method to extract ID components (prefix, depth, parent)",
              "getNextSiblingID() method for sibling ID generation",
              "Collision detection using Set<string> or Map<string, RequirementNode>",
              "getDepth() utility method to determine hierarchy level from ID"
            ],
            "middleware": [],
            "shared": [
              "ID_PATTERN constant: /^parent_\\d+(\\.\\d+){0,2}$/",
              "IDComponents interface with prefix, levels, depth properties",
              "validateHierarchicalID() utility function",
              "MAX_DEPTH constant (3 levels: parent, sub_process, implementation)",
              "HierarchicalIDError class for invalid ID format errors"
            ]
          },
          "testable_properties": [],
          "function_id": "HierarchicalIDGenerator.generateID",
          "related_concepts": [
            "unique identifier generation",
            "parent-child relationships",
            "ID format validation",
            "collision detection",
            "depth tracking"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_010.3",
          "description": "Build three-tier requirement hierarchy structure that organizes requirements into parent, sub_process, and implementation levels with bidirectional references and traversal methods",
          "type": "sub_process",
          "parent_id": "REQ_010",
          "children": [],
          "acceptance_criteria": [
            "RequirementHierarchy class maintains three distinct levels: parent (tier 1), sub_process (tier 2), implementation (tier 3)",
            "Each RequirementNode contains parent_id reference for upward traversal",
            "Each RequirementNode contains children[] array for downward traversal",
            "addRequirement() method validates tier level before insertion",
            "Tier 1 (parent) requirements have no parent_id and depth = 1",
            "Tier 2 (sub_process) requirements have parent_id matching tier 1 requirement and depth = 2",
            "Tier 3 (implementation) requirements have parent_id matching tier 2 requirement and depth = 3",
            "getParent() method retrieves parent node by parent_id",
            "getChildren() method returns all child nodes for a given requirement",
            "getSiblings() method returns requirements at same level with same parent",
            "traverseDepthFirst() iterator yields requirements in DFS order",
            "traverseBreadthFirst() iterator yields requirements in BFS order",
            "getByLevel() method filters requirements by tier (1, 2, or 3)",
            "validateHierarchy() method ensures no orphaned nodes and valid parent references",
            "exportToJSON() produces hierarchical JSON representation",
            "Property-based tests verify hierarchy invariants (no cycles, valid depths)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "RequirementHierarchy class with private Map<string, RequirementNode> storage",
              "addRequirement() method with tier validation logic",
              "getParent() method with null handling for root nodes",
              "getChildren() method filtering by parent_id",
              "getSiblings() method comparing parent_id and filtering by depth",
              "traverseDepthFirst() generator function for DFS iteration",
              "traverseBreadthFirst() generator function for BFS iteration",
              "getByLevel() method filtering by depth property",
              "validateHierarchy() method checking orphans and cycles",
              "removeRequirement() method with cascade option for children",
              "moveRequirement() method for reparenting with validation"
            ],
            "middleware": [],
            "shared": [
              "RequirementNode interface with id, parent_id, children, depth, description, category",
              "RequirementTier enum (PARENT = 1, SUB_PROCESS = 2, IMPLEMENTATION = 3)",
              "HierarchyValidationError class for invalid hierarchy operations",
              "TraversalOrder enum (DEPTH_FIRST, BREADTH_FIRST)",
              "HierarchyStats interface with counts per tier"
            ]
          },
          "testable_properties": [],
          "function_id": "RequirementHierarchy.buildThreeTierStructure",
          "related_concepts": [
            "tree data structure",
            "parent-child relationships",
            "hierarchy traversal",
            "requirement organization",
            "level validation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_010.4",
          "description": "Add component breakdown categorization system that analyzes requirements and assigns implementation components to frontend, backend, middleware, and shared categories",
          "type": "sub_process",
          "parent_id": "REQ_010",
          "children": [],
          "acceptance_criteria": [
            "ImplementationComponents interface defines all four categories: frontend, backend, middleware, shared",
            "categorizeComponents() method analyzes requirement description and assigns components",
            "Frontend category includes UI components, pages, forms, validation, user interactions",
            "Backend category includes API endpoints, services, data processing, business logic",
            "Middleware category includes authentication, authorization, request/response processing",
            "Shared category includes data models, utilities, constants, interfaces used across layers",
            "BAML-based classification suggests components based on requirement text",
            "Manual override allows developers to adjust component assignments",
            "validateComponentBreakdown() ensures at least one component assigned per requirement",
            "Component assignments stored as string arrays for each category",
            "Empty arrays allowed for categories not applicable to requirement",
            "getDependencies() method identifies shared components required by multiple layers",
            "Unit tests verify correct categorization for common requirement patterns"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "ComponentBreakdown service class with categorizeComponents() method",
              "BAML function for component classification with category examples",
              "Pattern matching service for keyword-based categorization fallback",
              "validateComponentBreakdown() method checking non-empty assignments",
              "getDependencies() method analyzing shared component usage",
              "mergeComponents() method for combining component breakdowns"
            ],
            "middleware": [
              "Component assignment validation rules",
              "Category normalization for consistent formatting"
            ],
            "shared": [
              "ImplementationComponents interface with frontend, backend, middleware, shared arrays",
              "ComponentCategory enum (FRONTEND, BACKEND, MIDDLEWARE, SHARED)",
              "ComponentSuggestion type with category and description",
              "COMPONENT_KEYWORDS constant mapping keywords to categories",
              "ComponentBreakdownError class for validation failures"
            ]
          },
          "testable_properties": [],
          "function_id": "ComponentBreakdown.categorizeComponents",
          "related_concepts": [
            "component classification",
            "implementation planning",
            "separation of concerns",
            "architectural layers",
            "dependency analysis"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_010.5",
          "description": "Generate testable properties with strategy definitions for property-based testing using invariant, round_trip, idempotence, and oracle property types",
          "type": "sub_process",
          "parent_id": "REQ_010",
          "children": [],
          "acceptance_criteria": [
            "TestableProperty interface includes property_type, strategy, description fields",
            "PropertyType enum defines four types: invariant, round_trip, idempotence, oracle",
            "generateProperties() method creates properties based on requirement characteristics",
            "Invariant properties define conditions that must always hold (e.g., 'length >= 0')",
            "Round-trip properties verify encode/decode cycles preserve data",
            "Idempotence properties verify repeated operations produce same result",
            "Oracle properties compare implementation against reference implementation",
            "Strategy field contains fast-check generator description (e.g., 'fc.array(fc.string())')",
            "BAML-based property suggestion analyzes requirement and proposes relevant properties",
            "Manual property definition allows developers to add custom properties",
            "validateProperty() ensures strategy is syntactically valid fast-check code",
            "Properties stored as array on RequirementNode",
            "exportToTestCode() generates executable fast-check test code",
            "Integration tests verify generated properties catch real bugs",
            "Property-based tests verify property generator itself using meta-properties"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "PropertyGenerator service class with generateProperties() method",
              "BAML function for property suggestion with examples for each type",
              "StrategyBuilder class for constructing fast-check strategy strings",
              "validateProperty() method with strategy syntax checking",
              "exportToTestCode() method generating Vitest test code",
              "PropertyMatcher service mapping requirement patterns to property types",
              "getPropertyTemplate() method returning boilerplate for each property type"
            ],
            "middleware": [],
            "shared": [
              "TestableProperty interface with property_type, strategy, description, example_inputs",
              "PropertyType enum (INVARIANT, ROUND_TRIP, IDEMPOTENCE, ORACLE)",
              "PropertyStrategy type alias for fast-check strategy strings",
              "PROPERTY_TEMPLATES constant with code templates for each type",
              "PropertyValidationError class for invalid strategy errors",
              "PropertyExample interface with input, expected_output for oracle properties"
            ]
          },
          "testable_properties": [],
          "function_id": "PropertyGenerator.generateTestableProperties",
          "related_concepts": [
            "property-based testing",
            "test generation",
            "fast-check strategies",
            "hypothesis testing patterns",
            "test coverage optimization"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_011",
      "description": "The system must implement autonomous loop runner with session management supporting up to 100 sessions, feature queue processing with dependency checking, Claude Code CLI invocation with compiled context, success/failure handling with automatic commits, and 3-second pause between sessions",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_011.1",
          "description": "Initialize LoopRunner class with session counter, project path, model configuration, and maximum session limit (100 sessions default)",
          "type": "sub_process",
          "parent_id": "REQ_011",
          "children": [],
          "acceptance_criteria": [
            "LoopRunner class accepts projectPath string and options object with model field",
            "Session counter initializes to 0",
            "Maximum session limit defaults to 100 and is configurable",
            "Model property validates against allowed values ('sonnet' | 'opus')",
            "Project path is validated as existing directory",
            "Constructor throws error if project path does not exist",
            "All instance properties are correctly typed with TypeScript strict mode"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "LoopRunner class with private sessionCount property initialized to 0",
              "Private maxSessions property with default value 100",
              "Private projectPath property storing validated path",
              "Private model property typed as 'sonnet' | 'opus'",
              "Constructor parameter validation logic",
              "Error throwing for invalid project path"
            ],
            "middleware": [],
            "shared": [
              "LoopRunnerOptions interface with model and optional maxSessions fields",
              "Model type definition as 'sonnet' | 'opus'",
              "Path validation utility function"
            ]
          },
          "testable_properties": [],
          "function_id": "LoopRunner.constructor",
          "related_concepts": [
            "session management",
            "configuration initialization",
            "state tracking",
            "TypeScript class instantiation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_011.2",
          "description": "Retrieve next eligible feature from feature queue by checking completion status (not passes), blocked status (not blocked), and dependency satisfaction using dependency validation",
          "type": "sub_process",
          "parent_id": "REQ_011",
          "children": [],
          "acceptance_criteria": [
            "Method returns Promise<Feature | null>",
            "Loads feature list from project directory asynchronously",
            "Filters out features where passes=true",
            "Filters out features where blocked=true",
            "Validates all feature dependencies are met before returning feature",
            "Returns null when no eligible features exist",
            "Returns first eligible feature matching all criteria",
            "Dependency validation checks that all dependency IDs exist and have passes=true",
            "Handles missing or corrupted feature list file gracefully"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Private async getNextFeature method returning Promise<Feature | null>",
              "Feature list loading from filesystem (JSON file)",
              "Filter logic for passes and blocked status",
              "Dependency validation logic checking dependency satisfaction",
              "dependenciesMet helper method accepting Feature parameter",
              "Error handling for file read failures",
              "Feature list parsing with JSON schema validation"
            ],
            "middleware": [],
            "shared": [
              "Feature interface with id, passes, blocked, dependencies properties",
              "FeatureList interface with features array",
              "Feature schema validation using Zod",
              "File path constants for feature list location"
            ]
          },
          "testable_properties": [],
          "function_id": "LoopRunner.getNextFeature",
          "related_concepts": [
            "feature queue management",
            "dependency graph traversal",
            "feature filtering",
            "async feature loading"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_011.3",
          "description": "Build feature-specific context string by gathering relevant code files, documentation, test files, and feature requirements formatted for Claude Code CLI consumption",
          "type": "sub_process",
          "parent_id": "REQ_011",
          "children": [],
          "acceptance_criteria": [
            "Method accepts Feature parameter and returns Promise<string>",
            "Loads feature description and requirements from feature data",
            "Identifies related source files based on feature metadata",
            "Includes relevant test files if they exist",
            "Includes project documentation snippets relevant to feature",
            "Formats context as structured prompt with clear sections",
            "Context string includes feature ID and acceptance criteria",
            "Context includes current project state and git status",
            "Returns formatted string ready for CLI input",
            "Handles missing files gracefully without failing compilation"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Private async compileContext method accepting Feature parameter",
              "File content loading logic for source files",
              "Test file discovery and loading logic",
              "Documentation snippet extraction logic",
              "Context template builder combining all sections",
              "Git status retrieval for current project state",
              "Error handling for missing or inaccessible files",
              "String concatenation or template literal for final context"
            ],
            "middleware": [],
            "shared": [
              "Context template constants defining structure",
              "Feature metadata interface with relatedFiles property",
              "File loading utility functions",
              "Git status utility function"
            ]
          },
          "testable_properties": [],
          "function_id": "LoopRunner.compileContext",
          "related_concepts": [
            "context compilation",
            "template rendering",
            "file aggregation",
            "feature context building"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_011.4",
          "description": "Execute Claude Code CLI using child_process.spawn with specified model, compiled context as prompt, output stream parsing for success/failure detection, and result extraction",
          "type": "sub_process",
          "parent_id": "REQ_011",
          "children": [],
          "acceptance_criteria": [
            "Method accepts Feature and context string, returns Promise<SessionResult>",
            "Spawns claude CLI process with correct arguments (--model, -p, --dangerously-skip-permissions)",
            "Pipes stdout and stderr for real-time logging",
            "Parses output stream to detect completion markers",
            "Returns SessionResult with success boolean and optional error message",
            "Handles process exit codes correctly (0=success, non-zero=failure)",
            "Captures full output for debugging purposes",
            "Implements timeout mechanism for hanging processes",
            "Handles process spawn errors gracefully",
            "Logs session progress to console with timestamps"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Private async runClaudeSession method with Feature and context parameters",
              "child_process.spawn invocation with claude command",
              "stdout data event handler for output collection",
              "stderr data event handler for error collection",
              "Process close event handler with exit code check",
              "Output parsing logic to detect success/failure patterns",
              "Timeout timer implementation with process kill",
              "Error wrapping for spawn failures",
              "Console logging with timestamp formatting"
            ],
            "middleware": [],
            "shared": [
              "SessionResult interface with success boolean and error optional string",
              "CLI command constants for claude executable path",
              "Timeout duration constant (configurable)",
              "Output pattern regex for success/failure detection"
            ]
          },
          "testable_properties": [],
          "function_id": "LoopRunner.runClaudeSession",
          "related_concepts": [
            "subprocess execution",
            "CLI integration",
            "stream processing",
            "async process handling",
            "output parsing"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_011.5",
          "description": "Process successful feature completion by marking feature as complete (passes=true), performing automatic git add/commit with descriptive message including feature ID, and updating feature list file",
          "type": "sub_process",
          "parent_id": "REQ_011",
          "children": [],
          "acceptance_criteria": [
            "Method accepts Feature parameter and returns Promise<void>",
            "Updates feature object with passes=true",
            "Persists updated feature list to filesystem",
            "Executes git add . to stage all changes",
            "Generates commit message including feature ID and description",
            "Commit message includes footer with \ud83e\udd16 Generated with Claude Code attribution",
            "Executes git commit with generated message using HEREDOC format",
            "Handles git command failures gracefully",
            "Logs success message with feature ID to console",
            "Does not push to remote (only local commit)",
            "Validates feature list write was successful"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Private async handleSuccess method accepting Feature parameter",
              "Feature update logic setting passes=true",
              "Feature list file write operation with atomic write",
              "git add command execution via child_process",
              "Commit message template builder with feature details",
              "git commit command execution with HEREDOC message format",
              "Error handling for git command failures",
              "Console logging for success confirmation",
              "File write validation logic"
            ],
            "middleware": [],
            "shared": [
              "Commit message template constants",
              "Git command utility functions (execGit wrapper)",
              "Feature list persistence utility",
              "Atomic file write utility function"
            ]
          },
          "testable_properties": [],
          "function_id": "LoopRunner.handleSuccess",
          "related_concepts": [
            "git automation",
            "feature state management",
            "commit message generation",
            "file persistence"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_011.6",
          "description": "Process failed feature attempt by logging error details, optionally marking feature as blocked if critical error detected, preserving error information for debugging, and continuing to next feature",
          "type": "sub_process",
          "parent_id": "REQ_011",
          "children": [],
          "acceptance_criteria": [
            "Method accepts Feature and SessionResult parameters, returns Promise<void>",
            "Logs error message with feature ID and error details",
            "Analyzes error type to determine if feature should be blocked",
            "Updates feature with blocked=true if critical error detected",
            "Preserves error message in feature metadata",
            "Persists updated feature list if blocking occurs",
            "Does not throw exception (allows loop to continue)",
            "Logs actionable failure information to console",
            "Includes timestamp in error logs",
            "Returns gracefully to allow next iteration"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Private async handleFailure method with Feature and SessionResult parameters",
              "Error logging logic with structured log format",
              "Critical error detection logic (pattern matching on error message)",
              "Feature blocking logic setting blocked=true and adding error metadata",
              "Feature list file update for blocked features",
              "Console error output with formatting",
              "Timestamp generation for error logs",
              "Graceful return without throwing"
            ],
            "middleware": [],
            "shared": [
              "Critical error patterns constants (regex or string array)",
              "Feature interface with blocked boolean and errorMessage optional fields",
              "Error log formatting utility",
              "Timestamp utility function"
            ]
          },
          "testable_properties": [],
          "function_id": "LoopRunner.handleFailure",
          "related_concepts": [
            "error handling",
            "failure recovery",
            "feature blocking logic",
            "error logging"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_011.7",
          "description": "Execute main autonomous loop controlling session execution, incrementing session counter, implementing 3-second pause between sessions, enforcing maximum session limit (100), and coordinating all loop operations",
          "type": "sub_process",
          "parent_id": "REQ_011",
          "children": [],
          "acceptance_criteria": [
            "Method is async and returns Promise<void>",
            "Initializes sessionCount to 0 at start",
            "Implements while loop with condition sessionCount < maxSessions",
            "Calls getNextFeature to retrieve next feature",
            "Breaks loop with success message when no features remain",
            "Logs session start with session number and feature ID",
            "Awaits compileContext for each feature",
            "Awaits runClaudeSession with feature and context",
            "Calls handleSuccess if result.success is true",
            "Calls handleFailure if result.success is false",
            "Increments sessionCount after each iteration",
            "Implements 3000ms sleep after each session",
            "Logs completion message when loop exits normally",
            "Logs maximum session limit reached if applicable"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Public async start method returning Promise<void>",
              "Session counter initialization logic",
              "While loop with sessionCount < maxSessions condition",
              "getNextFeature invocation with null check",
              "Loop break on null feature with completion log",
              "Session start logging with counter and feature details",
              "compileContext invocation",
              "runClaudeSession invocation",
              "Conditional branching on result.success",
              "handleSuccess invocation in success branch",
              "handleFailure invocation in failure branch",
              "sessionCount increment statement",
              "3-second sleep implementation (await sleep(3000))",
              "Loop exit logging",
              "Maximum session limit logging"
            ],
            "middleware": [],
            "shared": [
              "sleep utility function accepting milliseconds parameter",
              "Console logging utilities for structured output",
              "Session completion status tracking interface"
            ]
          },
          "testable_properties": [],
          "function_id": "LoopRunner.start",
          "related_concepts": [
            "control loop",
            "session orchestration",
            "rate limiting",
            "async iteration",
            "loop termination"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_012",
      "description": "The system must implement checkpoint system with save/restore functionality at any pipeline step, JSON serialization of complex types with Zod validation on load, interactive user control for revise/restart/continue decisions, and migration scripts for backward compatibility with Python checkpoints",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_012.1",
          "description": "Implement checkpoint manager with methods to save pipeline state at any step, storing checkpoint metadata (timestamp, step name, pipeline config) and full context state to disk",
          "type": "sub_process",
          "parent_id": "REQ_012",
          "children": [],
          "acceptance_criteria": [
            "saveCheckpoint() accepts step name and pipeline context as parameters",
            "Checkpoint files are saved to .checkpoints/ directory with timestamp naming pattern (checkpoint_STEP_YYYYMMDD_HHMMSS.json)",
            "Checkpoint data includes: step_name, timestamp, pipeline_config, requirement_hierarchy, context_store_state, metadata",
            "Save operation is atomic (uses temp file + rename to prevent corruption)",
            "Method returns checkpoint file path on success",
            "Throws CheckpointSaveError if disk write fails",
            "Creates .checkpoints/ directory if it doesn't exist",
            "Checkpoint files are validated after write to ensure integrity",
            "Maximum 10 checkpoints per step are retained (older ones auto-deleted)",
            "Save operation completes within 2 seconds for typical pipeline states"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "CheckpointManager class with saveCheckpoint() async method",
              "File system service for atomic write operations",
              "Checkpoint retention policy service (auto-cleanup old checkpoints)",
              "Checkpoint integrity verification after write",
              "Error handling for disk full, permissions, IO errors"
            ],
            "middleware": [],
            "shared": [
              "CheckpointMetadata interface (step_name, timestamp, version)",
              "CheckpointData interface (metadata, pipeline_config, context_state)",
              "CheckpointSaveError custom error class",
              "File path utility for checkpoint naming pattern",
              "Constants: CHECKPOINT_DIR, MAX_CHECKPOINTS_PER_STEP"
            ]
          },
          "testable_properties": [],
          "function_id": "CheckpointManager.saveCheckpoint",
          "related_concepts": [
            "state_persistence",
            "pipeline_recovery",
            "file_system_operations",
            "atomic_writes"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_012.2",
          "description": "Implement checkpoint restore functionality to load saved pipeline state from any step, with file discovery, deserialization, and state reconstruction",
          "type": "sub_process",
          "parent_id": "REQ_012",
          "children": [],
          "acceptance_criteria": [
            "loadCheckpoint() accepts optional step name parameter (loads latest if not specified)",
            "Method discovers available checkpoint files in .checkpoints/ directory",
            "Returns list of available checkpoints sorted by timestamp (newest first) when called without parameters",
            "Loads and deserializes checkpoint JSON when step name provided",
            "Validates checkpoint structure matches expected schema before returning",
            "Throws CheckpointNotFoundError if no checkpoint exists for requested step",
            "Throws CheckpointCorruptedError if JSON is invalid or structure mismatches",
            "Successfully restores: pipeline_config, requirement_hierarchy, context_store entries",
            "Load operation completes within 1 second for typical checkpoint files",
            "Supports resuming pipeline from loaded checkpoint state"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "CheckpointManager.loadCheckpoint() async method",
              "File discovery service to list checkpoint files by step name",
              "JSON deserialization with error handling",
              "State reconstruction service to rebuild pipeline context from checkpoint data",
              "Checkpoint file sorting by timestamp",
              "Error handling for missing files, corrupt JSON, invalid structure"
            ],
            "middleware": [],
            "shared": [
              "CheckpointNotFoundError custom error class",
              "CheckpointCorruptedError custom error class",
              "CheckpointInfo interface (file_path, step_name, timestamp)",
              "PipelineContext restore() method to apply checkpoint state",
              "File system utility for directory scanning and filtering"
            ]
          },
          "testable_properties": [],
          "function_id": "CheckpointManager.loadCheckpoint",
          "related_concepts": [
            "state_restoration",
            "pipeline_recovery",
            "error_recovery",
            "backward_compatibility"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_012.3",
          "description": "Implement JSON serialization for complex pipeline types (RequirementHierarchy, ContextEntry, PipelineConfig) with Zod schema validation to ensure type safety during save/load operations",
          "type": "sub_process",
          "parent_id": "REQ_012",
          "children": [],
          "acceptance_criteria": [
            "All checkpoint data types have corresponding Zod schemas defined",
            "serialize() method converts complex objects to JSON-compatible structures",
            "Date objects are serialized to ISO 8601 strings",
            "Map objects are serialized to Record<string, T> format",
            "Circular references are detected and throw SerializationError",
            "deserialize() method validates JSON against Zod schema before parsing",
            "Validation errors include detailed field-level error messages",
            "Successfully serializes/deserializes RequirementHierarchy with nested children",
            "Successfully serializes/deserializes ContextEntry with all 8 EntryType variants",
            "Successfully serializes/deserializes PipelineConfig with step states",
            "Zod validation catches missing required fields, wrong types, invalid enum values",
            "Performance: serialization completes within 500ms for hierarchy of 100+ requirements"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "CheckpointSerializer class with serialize() and deserialize() methods",
              "Type-specific serialization handlers for Date, Map, complex nested objects",
              "Circular reference detection algorithm",
              "Zod schema validation wrapper with detailed error reporting",
              "Performance optimization for large object graphs"
            ],
            "middleware": [],
            "shared": [
              "CheckpointDataSchema Zod schema (validates entire checkpoint structure)",
              "RequirementHierarchySchema Zod schema",
              "ContextEntrySchema Zod schema (already exists, extend for checkpoint context)",
              "PipelineConfigSchema Zod schema",
              "SerializationError custom error class with field path tracking",
              "DeserializationError custom error class with Zod error details",
              "Type guard utilities: isSerializable(), isCheckpointData()"
            ]
          },
          "testable_properties": [],
          "function_id": "CheckpointSerializer.serializeWithZod",
          "related_concepts": [
            "type_safety",
            "runtime_validation",
            "serialization",
            "data_integrity"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_012.4",
          "description": "Implement checkpoint validation and comprehensive error handling to detect corrupted files, schema mismatches, version incompatibilities, and provide actionable error messages for recovery",
          "type": "sub_process",
          "parent_id": "REQ_012",
          "children": [],
          "acceptance_criteria": [
            "validateCheckpoint() accepts checkpoint file path or deserialized data",
            "Validates JSON structure against CheckpointDataSchema",
            "Checks checkpoint version compatibility (rejects if version > current)",
            "Validates all required fields are present (metadata, pipeline_config, context_state)",
            "Validates nested structures (requirement hierarchy IDs, context entry references)",
            "Detects orphaned context entries (references to non-existent parent IDs)",
            "Detects invalid requirement hierarchy (circular parent-child relationships)",
            "Returns ValidationResult with success boolean and detailed error array",
            "Error messages include: error type, field path, expected vs actual value, suggested fix",
            "Validation runs automatically during loadCheckpoint() before state restoration",
            "Provides repair suggestions for common corruption patterns (missing fields, wrong types)",
            "Performance: validation completes within 1 second for checkpoints with 200+ entries"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "CheckpointManager.validateCheckpoint() method",
              "Schema validation service using Zod parse with error collection",
              "Version compatibility checker (semantic version comparison)",
              "Orphan entry detection algorithm (traverse context references)",
              "Circular dependency detection for requirement hierarchy",
              "Error aggregation and formatting service",
              "Repair suggestion generator for common issues"
            ],
            "middleware": [],
            "shared": [
              "ValidationResult interface (success, errors[], warnings[])",
              "ValidationError interface (type, field_path, message, suggested_fix)",
              "CheckpointVersion constant (current version number)",
              "Version compatibility rules (major/minor/patch version handling)",
              "Error type enum: SCHEMA_MISMATCH, VERSION_INCOMPATIBLE, ORPHANED_REFERENCE, CIRCULAR_DEPENDENCY, MISSING_FIELD",
              "Utility functions: detectOrphans(), detectCircularRefs(), compareVersions()"
            ]
          },
          "testable_properties": [],
          "function_id": "CheckpointManager.validateCheckpoint",
          "related_concepts": [
            "data_validation",
            "error_recovery",
            "version_compatibility",
            "data_integrity"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_012.5",
          "description": "Build interactive prompt system for checkpoint management, providing users with options to continue from checkpoint, restart step, revise previous step, or start fresh at each pipeline step",
          "type": "sub_process",
          "parent_id": "REQ_012",
          "children": [],
          "acceptance_criteria": [
            "Prompt displays when checkpoint exists for upcoming step",
            "Shows checkpoint metadata: step name, timestamp, summary of state (requirement count, context entries)",
            "Provides 4 options: Continue from checkpoint, Restart current step, Revise previous step, Start fresh pipeline",
            "Uses inquirer or prompts library for interactive CLI menu selection",
            "Validates user input and handles invalid selections gracefully",
            "Continue option: loads checkpoint and resumes pipeline from next step",
            "Restart option: deletes current step checkpoint and re-executes step",
            "Revise option: navigates to previous step, loads that checkpoint, allows editing",
            "Start fresh option: confirms with user (Y/N), then deletes all checkpoints and restarts pipeline",
            "Prompt times out after 60 seconds of inactivity (defaults to Continue)",
            "Displays clear visual separators and formatting for readability",
            "All user choices are logged to pipeline execution log",
            "Non-interactive mode: respects --auto-continue flag to skip prompts"
          ],
          "implementation": {
            "frontend": [
              "CLI prompt UI using inquirer or prompts library",
              "Formatted checkpoint summary display (table or list format)",
              "Visual separators (box drawing characters, colors)",
              "Confirmation prompts for destructive actions (start fresh)",
              "Progress indicators during checkpoint operations"
            ],
            "backend": [
              "InteractivePrompt service class with checkpointControl() method",
              "Checkpoint summary generator (extracts key metrics from checkpoint data)",
              "User choice handler that invokes appropriate CheckpointManager methods",
              "Prompt timeout handler with default behavior",
              "Non-interactive mode detection (check for --auto-continue flag)",
              "Execution logger for recording user decisions"
            ],
            "middleware": [],
            "shared": [
              "PromptChoice enum: CONTINUE, RESTART, REVISE, START_FRESH",
              "CheckpointSummary interface (step_name, timestamp, requirement_count, context_entry_count)",
              "PromptConfig interface (timeout_ms, default_choice, allow_revise)",
              "Constants: PROMPT_TIMEOUT_MS, DEFAULT_CHOICE",
              "Utility functions: formatCheckpointSummary(), formatTimestamp()"
            ]
          },
          "testable_properties": [],
          "function_id": "InteractivePrompt.checkpointControl",
          "related_concepts": [
            "user_interaction",
            "cli_prompts",
            "workflow_control",
            "decision_points"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_012.6",
          "description": "Create migration tool to convert Python checkpoint files to TypeScript format, handling data structure differences, type mappings, and ensuring backward compatibility with existing Python checkpoints",
          "type": "sub_process",
          "parent_id": "REQ_012",
          "children": [],
          "acceptance_criteria": [
            "Migration tool accepts Python checkpoint file path as input",
            "Detects Python checkpoint format by checking for Python-specific fields (e.g., __class__, _meta)",
            "Converts Python datetime strings to JavaScript Date ISO format",
            "Maps Python dict structures to TypeScript Record or Map as appropriate",
            "Converts Python None to TypeScript null or undefined based on context",
            "Handles Python-specific requirement hierarchy format (if different from TypeScript)",
            "Preserves all semantic information during conversion (no data loss)",
            "Validates migrated checkpoint against TypeScript CheckpointDataSchema",
            "Outputs TypeScript-compatible checkpoint file with _migrated flag in metadata",
            "Provides migration report: fields converted, warnings, errors",
            "Handles edge cases: empty checkpoints, corrupted Python files, unknown fields",
            "CLI command: silmari migrate-checkpoint <python-checkpoint-path>",
            "Batch migration mode: migrates all Python checkpoints in directory",
            "Performance: migrates typical checkpoint in under 2 seconds"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "CheckpointMigration service class with convertFromPython() method",
              "Python format detector (analyzes JSON structure for Python patterns)",
              "Type mapper for Python \u2192 TypeScript conversions (datetime, dict, None, etc.)",
              "Field renaming service for structure differences (if Python names differ)",
              "Validation service to ensure migrated data matches TypeScript schemas",
              "Migration report generator (success/failure, warnings, field mappings)",
              "Batch migration orchestrator for directory processing",
              "CLI command handler for migration tool"
            ],
            "middleware": [],
            "shared": [
              "PythonCheckpointFormat interface (Python structure definition)",
              "TypeScriptCheckpointFormat interface (TypeScript structure definition)",
              "MigrationResult interface (success, output_path, report, errors[])",
              "MigrationReport interface (fields_converted, warnings[], errors[])",
              "Type mapping rules: PYTHON_TO_TS_TYPE_MAP constant",
              "Field mapping rules: PYTHON_TO_TS_FIELD_MAP constant",
              "Utility functions: isPythonCheckpoint(), convertPythonDatetime(), mapPythonDict()"
            ]
          },
          "testable_properties": [],
          "function_id": "CheckpointMigration.convertFromPython",
          "related_concepts": [
            "data_migration",
            "backward_compatibility",
            "cross_language_compatibility",
            "schema_evolution"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_013",
      "description": "The system must implement context generation step with tech stack extraction detecting languages/frameworks/testing tools from package.json and source files, file group analysis for related components, context saving to disk in structured format, and integration with BAML for tech stack-aware planning",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_013.1",
          "description": "Extract technology stack information from package.json dependencies and devDependencies, identifying languages, frameworks, testing tools, and build tools",
          "type": "sub_process",
          "parent_id": "REQ_013",
          "children": [],
          "acceptance_criteria": [
            "Function reads package.json from filesystem and parses JSON content",
            "Extracts all dependencies and devDependencies into structured format",
            "Detects TypeScript from 'typescript' dependency or tsconfig.json presence",
            "Identifies React/React Native from 'react' or 'react-native' dependencies",
            "Detects testing frameworks: Jest, Vitest, Mocha, Jasmine, Cypress, Playwright",
            "Identifies build tools: webpack, vite, rollup, esbuild, parcel, turbopack",
            "Detects UI frameworks: Next.js, Remix, Astro, SvelteKit, Vue, Angular",
            "Captures version numbers for all detected technologies",
            "Returns empty arrays when package.json missing or no dependencies found",
            "Handles malformed JSON gracefully with error reporting",
            "Categorizes dependencies into: languages, frameworks, testing, build_tools, ui_libraries",
            "Detects linting/formatting tools: ESLint, Prettier, Biome",
            "Identifies state management: Redux, Zustand, Jotai, MobX, Recoil"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "TechStackExtractor service class with extractFromPackageJson method",
              "JSON parser with error handling for malformed files",
              "Dependency categorization logic based on known package names",
              "Version extraction and normalization logic",
              "Framework detection heuristics (e.g., Next.js from 'next' dependency)"
            ],
            "middleware": [
              "File system access validation",
              "Path resolution for package.json location",
              "Error handling for missing or inaccessible files"
            ],
            "shared": [
              "TechStackInfo interface with languages, frameworks, testing, build_tools fields",
              "PackageDependency interface with name and version fields",
              "DependencyCategory enum (language, framework, testing, build_tool, ui_library, linting, state_management)",
              "Known framework/tool registry constant mapping package names to categories",
              "Version parsing utility functions"
            ]
          },
          "testable_properties": [],
          "function_id": "TechStackExtractor.extractFromPackageJson",
          "related_concepts": [
            "package.json parsing",
            "dependency analysis",
            "framework detection",
            "version extraction",
            "TypeScript configuration",
            "Node.js ecosystem"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_013.2",
          "description": "Analyze source files to identify related component groups, detect project structure patterns (components/, pages/, services/), and categorize files by functionality and dependencies",
          "type": "sub_process",
          "parent_id": "REQ_013",
          "children": [],
          "acceptance_criteria": [
            "Scans src/ or project root directory recursively for source files",
            "Groups files by directory structure (e.g., components/, pages/, lib/, utils/)",
            "Detects common patterns: components, pages, services, utilities, hooks, contexts",
            "Parses import statements to identify file dependencies and relationships",
            "Identifies related files by shared imports or common directory",
            "Detects monorepo structure (packages/, apps/, libs/) if present",
            "Categorizes files as: component, page, service, utility, test, config, type_definition",
            "Creates file group objects with file paths, category, and dependency list",
            "Handles TypeScript (.ts, .tsx), JavaScript (.js, .jsx), and config files",
            "Excludes node_modules/, dist/, build/, .git/ from analysis",
            "Detects test file co-location patterns (*.test.ts, *.spec.ts, __tests__/)",
            "Identifies barrel exports (index.ts files) and their exported modules",
            "Maps component hierarchies based on import chains"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "FileGroupAnalyzer service class with analyzeComponentGroups method",
              "Recursive directory scanner with configurable exclusion patterns",
              "Import statement parser using regex or AST parsing (TypeScript compiler API)",
              "File categorization logic based on directory names and file extensions",
              "Dependency graph builder to map file relationships",
              "Monorepo structure detector checking for workspace configuration"
            ],
            "middleware": [
              "File system access with permission checks",
              "Path normalization for cross-platform compatibility",
              "File read operations with error handling for large files"
            ],
            "shared": [
              "FileGroup interface with name, category, files, dependencies fields",
              "FileCategory enum (component, page, service, utility, test, config, type_definition)",
              "FileInfo interface with path, imports, exports, category fields",
              "DirectoryPattern constant defining known project structure patterns",
              "ImportStatement interface with source, specifiers, isDefault fields",
              "DependencyGraph type representing file relationship map"
            ]
          },
          "testable_properties": [],
          "function_id": "FileGroupAnalyzer.analyzeComponentGroups",
          "related_concepts": [
            "file system traversal",
            "directory structure analysis",
            "import statement parsing",
            "component relationship mapping",
            "monorepo detection",
            "module bundling patterns"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_013.3",
          "description": "Save extracted tech stack and file group analysis to disk in structured JSON and Markdown formats for BAML consumption and human review",
          "type": "sub_process",
          "parent_id": "REQ_013",
          "children": [],
          "acceptance_criteria": [
            "Creates context output directory if not exists (e.g., .context/ or context/)",
            "Writes tech-stack.json containing complete TechStackInfo object",
            "Writes file-groups.json containing array of FileGroup objects with dependencies",
            "Generates tech-stack.md with human-readable Markdown summary of technologies",
            "Generates file-groups.md with Mermaid diagram of component relationships",
            "Includes timestamps in output files (created_at, last_updated fields)",
            "Validates JSON structure with Zod schemas before writing",
            "Uses atomic write operations (write to temp, then rename) to prevent corruption",
            "Handles write permission errors gracefully with clear error messages",
            "Creates backup of existing files before overwriting (tech-stack.json.bak)",
            "Formats JSON with 2-space indentation for readability",
            "Includes metadata: project_path, analysis_date, total_files_analyzed",
            "Returns success status and file paths written"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "ContextWriter service class with saveContextToDisk method",
              "JSON file writer with atomic write operations",
              "Markdown generator for tech stack summary with formatting",
              "Mermaid diagram generator for file group relationships",
              "Backup file creator for existing context files",
              "Directory creation logic with recursive mkdir",
              "Timestamp generation and formatting utilities"
            ],
            "middleware": [
              "File system permission validation before write operations",
              "Path resolution for context output directory",
              "Zod schema validation for TechStackInfo and FileGroup data"
            ],
            "shared": [
              "ContextOutput interface with success, files_written, errors fields",
              "ContextMetadata interface with project_path, analysis_date, total_files fields",
              "FileWriteResult interface with path, success, error fields",
              "CONTEXT_OUTPUT_DIR constant defining default output location",
              "JSON formatting utility with configurable indentation",
              "Markdown template functions for tech stack and file groups",
              "Mermaid diagram utility functions for graph generation"
            ]
          },
          "testable_properties": [],
          "function_id": "ContextWriter.saveContextToDisk",
          "related_concepts": [
            "file system operations",
            "JSON serialization",
            "Markdown generation",
            "structured data formatting",
            "context persistence",
            "idempotent writes"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_013.4",
          "description": "Integrate extracted tech stack data with BAML client context for tech stack-aware planning, formatting context strings for BAML function calls with technology-specific guidance",
          "type": "sub_process",
          "parent_id": "REQ_013",
          "children": [],
          "acceptance_criteria": [
            "Loads tech-stack.json and file-groups.json from context directory",
            "Formats tech stack information into BAML-compatible context string",
            "Includes detected languages with versions (e.g., 'TypeScript 5.3.3')",
            "Lists frameworks with usage context (e.g., 'React 18.2 for UI components')",
            "Specifies testing framework recommendations (e.g., 'Use Jest for unit tests')",
            "Adds build tool configuration notes (e.g., 'Vite configured with TypeScript plugin')",
            "Includes file group structure summary for context-aware code generation",
            "Formats context string with sections: TECH_STACK, FILE_STRUCTURE, TESTING, BUILD",
            "Validates context string length stays within BAML token limits (configurable)",
            "Provides truncation logic if context exceeds limits, prioritizing critical info",
            "Returns formatted context ready for BAML client.decompose() or client.plan() calls",
            "Adds framework-specific best practices (e.g., React hooks rules for React projects)",
            "Includes linting/formatting tool guidance (e.g., 'Follow ESLint rules in .eslintrc')"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "BAMLContextIntegrator service class with buildTechStackContext method",
              "Context file loader reading JSON from disk",
              "Context string formatter with section-based template",
              "Token counter for context length validation",
              "Truncation logic prioritizing essential tech stack information",
              "Framework-specific guidance generator based on detected frameworks",
              "Best practices database mapping frameworks to recommended patterns"
            ],
            "middleware": [
              "File system access for reading context JSON files",
              "Error handling for missing or malformed context files",
              "Context length validation against BAML token limits"
            ],
            "shared": [
              "BAMLContext interface with tech_stack, file_structure, testing, build sections",
              "ContextSection type for structured context string segments",
              "TOKEN_LIMIT constant defining max context length for BAML",
              "FrameworkGuidance interface with framework, best_practices, patterns fields",
              "Context template constants for TECH_STACK, FILE_STRUCTURE sections",
              "Token estimation utility function for string length calculation",
              "Truncation strategy enum (prioritize_frameworks, prioritize_testing, balanced)"
            ]
          },
          "testable_properties": [],
          "function_id": "BAMLContextIntegrator.buildTechStackContext",
          "related_concepts": [
            "BAML client integration",
            "context string formatting",
            "LLM prompt engineering",
            "tech stack-aware planning",
            "framework-specific guidance",
            "context window management"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_013.5",
          "description": "Add comprehensive detection for TypeScript, React, Jest, Vitest, ESLint, Prettier, and other common development tools, including configuration file analysis and usage pattern detection",
          "type": "sub_process",
          "parent_id": "REQ_013",
          "children": [],
          "acceptance_criteria": [
            "Detects TypeScript from 'typescript' dependency and tsconfig.json presence",
            "Parses tsconfig.json to extract compiler options (strict, target, module, etc.)",
            "Identifies React version and variant (React, Preact, React Native)",
            "Detects testing framework from dependencies: Jest, Vitest, Mocha, Jasmine",
            "Parses jest.config.js/ts or vitest.config.ts for test configuration",
            "Identifies ESLint configuration (.eslintrc.json, .eslintrc.js, eslint.config.js)",
            "Detects Prettier from .prettierrc or package.json prettier config",
            "Recognizes TypeScript path mappings from tsconfig paths field",
            "Identifies CSS tooling: Tailwind, styled-components, CSS Modules, Sass",
            "Detects bundler plugins and configuration (Vite plugins, webpack loaders)",
            "Recognizes monorepo tools: Turborepo, Nx, Lerna, pnpm workspaces",
            "Identifies runtime environments: Node.js version from .nvmrc or engines field",
            "Detects API frameworks: Express, Fastify, Koa, Hono, tRPC",
            "Returns comprehensive ToolDetectionResult with all detected tools and configurations"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "CommonToolDetector service class with detectToolsAndPatterns method",
              "TypeScript config parser for tsconfig.json with option extraction",
              "ESLint config parser handling multiple config file formats",
              "Jest/Vitest config parser extracting test environment and setup",
              "CSS tooling detector checking package.json and config files",
              "Bundler configuration analyzer for Vite, webpack, Rollup",
              "Monorepo detector checking workspace configuration in package.json",
              "Runtime environment detector from .nvmrc, .node-version, package.json engines",
              "API framework detector from dependencies and file structure"
            ],
            "middleware": [
              "Multi-format config file reader (JSON, JS, TS, YAML)",
              "Error handling for malformed configuration files",
              "Config file priority resolution when multiple configs exist"
            ],
            "shared": [
              "ToolDetectionResult interface with typescript, react, testing, linting, css, bundler, monorepo, runtime fields",
              "TypeScriptConfig interface with strict, target, module, paths, outDir fields",
              "TestingConfig interface with framework, environment, setupFiles, coverage fields",
              "LintingConfig interface with tools (eslint, prettier), rules, extends fields",
              "CSSTooling enum (tailwind, styled_components, css_modules, sass, less, postcss)",
              "BundlerConfig interface with tool, plugins, optimization fields",
              "RuntimeInfo interface with node_version, package_manager, engines fields",
              "Known tool registry mapping package names to tool categories and versions"
            ]
          },
          "testable_properties": [],
          "function_id": "CommonToolDetector.detectToolsAndPatterns",
          "related_concepts": [
            "configuration file parsing",
            "tool usage detection",
            "tsconfig.json analysis",
            "eslintrc parsing",
            "jest.config analysis",
            "framework version compatibility",
            "toolchain integration"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_014",
      "description": "The system must implement property-based testing with fast-check for invariant validation (entry IDs match ctx_[a-f0-9]{8} pattern, compressed entries have no content, parent-child relationship consistency), round-trip properties, idempotence checks, and oracle-based verification with at least 100 test cases per property",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_014.1",
          "description": "Install and configure fast-check library with Vitest integration, including setup files, type definitions, and test runner configuration for property-based testing across all test suites",
          "type": "sub_process",
          "parent_id": "REQ_014",
          "children": [],
          "acceptance_criteria": [
            "fast-check npm package installed as devDependency with version ^3.15.0 or later",
            "@fast-check/vitest integration package installed and configured",
            "vitest.config.ts updated with fast-check/vitest plugin",
            "TypeScript types for fast-check available in all test files without explicit imports",
            "Property test helper utilities created in test/utils/property-helpers.ts",
            "Example property test passes successfully with 100 test cases",
            "CI/CD pipeline runs property tests alongside unit tests",
            "Documentation added to docs/testing/property-based-testing.md explaining setup and usage"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "package.json with fast-check and @fast-check/vitest dependencies",
              "vitest.config.ts with fast-check plugin configuration",
              "test/setup.ts with global property test configurations",
              "test/utils/property-helpers.ts with reusable arbitraries and test wrappers",
              "tsconfig.json updated to include fast-check types",
              ".github/workflows/test.yml updated to run property tests"
            ]
          },
          "testable_properties": [],
          "function_id": "PropertyTesting.setupFastCheckVitest",
          "related_concepts": [
            "vitest configuration",
            "fast-check arbitraries",
            "test setup files",
            "TypeScript type support",
            "test runner integration"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_014.2",
          "description": "Create fast-check property tests for core data model invariants including entry ID pattern validation (ctx_[a-f0-9]{8}), compressed entry content restrictions, and parent-child relationship consistency with minimum 100 test cases per property",
          "type": "sub_process",
          "parent_id": "REQ_014",
          "children": [],
          "acceptance_criteria": [
            "Property test validates entry IDs always match regex /^ctx_[a-f0-9]{8}$/ with 100+ generated cases",
            "Property test ensures compressed entries have content === undefined with 100+ cases",
            "Property test verifies compressed entries always have non-empty summary with 100+ cases",
            "Property test confirms parent-child relationships maintain bidirectional consistency with 100+ cases",
            "Property test validates parent IDs always reference existing entries with 100+ cases",
            "Property test ensures derived_from arrays contain only valid entry IDs with 100+ cases",
            "Property test confirms EntryType enum values are always valid with 100+ cases",
            "Property test validates created_at timestamps are valid Date objects with 100+ cases",
            "Custom arbitrary created for valid ContextEntry generation (fc.record with constraints)",
            "Custom arbitrary created for entry ID generation (fc.hexaString with ctx_ prefix)",
            "All invariant tests execute in under 5 seconds for 100 cases",
            "Test coverage for invariant properties reaches 100% of core model constraints"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "src/context/models.test.ts with invariant property tests",
              "test/arbitraries/context-entry.arbitrary.ts with custom ContextEntry generator",
              "test/arbitraries/entry-id.arbitrary.ts with ID pattern generator",
              "test/utils/invariant-assertions.ts with reusable assertion helpers",
              "src/context/models.ts updated with runtime validation using Zod schemas",
              "test/fixtures/invalid-entries.ts with edge cases for negative testing"
            ]
          },
          "testable_properties": [],
          "function_id": "PropertyTesting.implementInvariantProperties",
          "related_concepts": [
            "ContextEntry model validation",
            "entry ID generation",
            "compression rules",
            "hierarchical relationships",
            "custom arbitraries",
            "property combinators"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_014.3",
          "description": "Implement round-trip property tests for serialization/deserialization of ContextEntry, RequirementNode, and RequirementHierarchy objects ensuring data integrity through JSON conversion cycles with 100+ test cases per type",
          "type": "sub_process",
          "parent_id": "REQ_014",
          "children": [],
          "acceptance_criteria": [
            "Property test validates ContextEntry round-trip: entry -> JSON -> parsed equals original with 100+ cases",
            "Property test validates RequirementNode round-trip with nested children through JSON cycle with 100+ cases",
            "Property test validates RequirementHierarchy round-trip with complete tree structure with 100+ cases",
            "Property test confirms Date objects serialize and deserialize correctly with 100+ cases",
            "Property test ensures optional fields preserve undefined vs null distinction with 100+ cases",
            "Property test validates Map<string, ContextEntry> serialization to Record and back with 100+ cases",
            "Property test confirms Zod schema validation passes after round-trip with 100+ cases",
            "Property test ensures nested object references maintain integrity with 100+ cases",
            "Deep equality comparison function created for complex object validation",
            "All round-trip tests execute in under 10 seconds for 100 cases per type",
            "Edge cases covered: empty arrays, null prototype objects, circular reference detection"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "src/context/models.test.ts with round-trip property tests for ContextEntry",
              "src/planning/models.test.ts with round-trip tests for RequirementNode and RequirementHierarchy",
              "src/context/store.test.ts with round-trip tests for CentralContextStore export/import",
              "test/utils/serialization.ts with JSON round-trip helper functions",
              "test/utils/deep-equal.ts with structural equality comparison",
              "test/arbitraries/requirement-node.arbitrary.ts with hierarchical data generator",
              "src/utils/serialization.ts with custom serializers for Date, Map, and complex types"
            ]
          },
          "testable_properties": [],
          "function_id": "PropertyTesting.implementRoundTripTests",
          "related_concepts": [
            "serialization",
            "deserialization",
            "JSON.stringify/parse",
            "Zod schema validation",
            "data integrity",
            "structural equality",
            "deep comparison"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_014.4",
          "description": "Create property tests validating idempotent operations on CentralContextStore (add same entry twice, compress multiple times, search with identical queries) and pipeline checkpoints with 100+ test cases per operation",
          "type": "sub_process",
          "parent_id": "REQ_014",
          "children": [],
          "acceptance_criteria": [
            "Property test confirms CentralContextStore.add() with duplicate ID is idempotent with 100+ cases",
            "Property test validates CentralContextStore.compress() called multiple times produces same result with 100+ cases",
            "Property test ensures VectorSearchIndex.search() returns identical results for same query with 100+ cases",
            "Property test confirms checkpoint save/load/save cycle produces identical checkpoint with 100+ cases",
            "Property test validates TaskBatcher grouping algorithm is deterministic with 100+ cases",
            "Property test ensures ContextEntry compression twice doesn't change state with 100+ cases",
            "Property test confirms BeadsController.createIssue() with same input fails gracefully on duplicate with 100+ cases",
            "Property test validates pipeline step execution with same checkpoint state is idempotent with 100+ cases",
            "State snapshot comparison utility created for before/after validation",
            "All idempotence tests execute in under 8 seconds for 100 cases per operation",
            "Side effects (file writes, API calls) properly mocked to enable true idempotence testing"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "src/context/store.test.ts with idempotence tests for add, compress, remove operations",
              "src/context/search.test.ts with search query idempotence tests",
              "src/context/batching.test.ts with deterministic grouping tests"
            ],
            "middleware": [],
            "shared": [
              "src/planning/checkpoint-manager.test.ts with checkpoint idempotence tests",
              "src/planning/beads-controller.test.ts with CLI operation idempotence tests",
              "test/utils/state-snapshot.ts with state comparison utilities",
              "test/mocks/fs-mock.ts with file system mocking for checkpoint tests",
              "test/mocks/child-process-mock.ts with subprocess mocking for CLI tests",
              "test/arbitraries/store-operations.arbitrary.ts with operation sequence generator"
            ]
          },
          "testable_properties": [],
          "function_id": "PropertyTesting.implementIdempotenceChecks",
          "related_concepts": [
            "idempotence",
            "state consistency",
            "duplicate detection",
            "compression stability",
            "search determinism",
            "checkpoint recovery"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_014.5",
          "description": "Configure property test execution parameters to run minimum 100 test cases per property with timeout limits, seed-based reproducibility, shrinking configuration, and CI/CD integration for automated property test suites",
          "type": "sub_process",
          "parent_id": "REQ_014",
          "children": [],
          "acceptance_criteria": [
            "Global fast-check configuration sets numRuns to 100 for all property tests",
            "Property test timeout configured to 30 seconds per test suite",
            "Seed-based test execution enabled with --seed flag for reproducible failures",
            "Shrinking algorithm configured with maxShrinks: 1000 for minimal counterexamples",
            "Verbose mode available via environment variable PROPERTY_TEST_VERBOSE=true",
            "CI/CD pipeline runs property tests with --reporter=json for machine-readable output",
            "Failed property tests output seed value for reproduction in logs",
            "Property test statistics (cases run, shrinks performed) included in test reports",
            "Parallel execution disabled for property tests to avoid race conditions",
            "Test execution completes in under 2 minutes for entire property test suite",
            "Property test configuration documented in docs/testing/property-test-config.md with examples",
            "NPM scripts added: npm run test:properties, npm run test:properties:verbose, npm run test:properties:seed"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "vitest.config.ts updated with property test specific timeouts and parallel execution settings",
              "test/setup.ts with global fast-check configuration (numRuns, maxShrinks, verbose)",
              "package.json with property test scripts and environment variable handling",
              ".github/workflows/test.yml with dedicated property test job and reporting",
              "test/utils/fc-config.ts with environment-based configuration loader",
              "docs/testing/property-test-config.md with configuration documentation",
              "test/reporters/property-test-reporter.ts with custom Vitest reporter for property tests",
              ".env.example updated with PROPERTY_TEST_VERBOSE and PROPERTY_TEST_SEED variables"
            ]
          },
          "testable_properties": [],
          "function_id": "PropertyTesting.configureTestExecution",
          "related_concepts": [
            "test configuration",
            "fast-check parameters",
            "test reproducibility",
            "counterexample shrinking",
            "timeout management",
            "CI/CD integration",
            "test reporting"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_015",
      "description": "The system must implement task batching for Implementation LLM with TaskSpec/TaskBatch/TaskBatcher classes, grouping tasks to respect 200-entry limit, intelligent batching based on related context entries, batch execution with context manager lifecycle, and batch result aggregation",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_015.1",
          "description": "Create TaskSpec and TaskBatch data structures",
          "type": "sub_process",
          "parent_id": "REQ_015",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Data.create",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_015.2",
          "description": "Implement TaskBatcher with entry counting logic",
          "type": "sub_process",
          "parent_id": "REQ_015",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.implement",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_015.3",
          "description": "Add intelligent grouping based on context relationships",
          "type": "sub_process",
          "parent_id": "REQ_015",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.add",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_015.4",
          "description": "Integrate with ImplementationLLMContext for execution",
          "type": "sub_process",
          "parent_id": "REQ_015",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.integrate",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_015.5",
          "description": "Implement result aggregation across batches",
          "type": "sub_process",
          "parent_id": "REQ_015",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.implement",
          "related_concepts": [],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_016",
      "description": "The system must provide comprehensive API documentation using TypeDoc with generated HTML documentation, architecture diagrams in Mermaid format showing dual-LLM and pipeline structure, migration guide from Python to TypeScript with pattern mappings, developer onboarding guide with setup instructions, and deployment guide with Docker configuration",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_016.1",
          "description": "Configure TypeDoc for automated API documentation generation with HTML output, including custom themes, navigation structure, and plugin configuration for comprehensive module documentation coverage",
          "type": "sub_process",
          "parent_id": "REQ_016",
          "children": [],
          "acceptance_criteria": [
            "typedoc.json configuration file created with all required options (entryPoints, out, theme, excludePrivate, includeVersion)",
            "Custom TypeDoc theme configured or default theme customized with project branding",
            "Documentation output directory configured to dist/docs or docs/api",
            "All public modules, classes, interfaces, and functions documented with TSDoc comments",
            "Package.json includes 'docs' script that runs 'typedoc --options typedoc.json'",
            "Generated HTML documentation includes navigation sidebar with module hierarchy",
            "Documentation includes README.md content on index page",
            "Private/internal APIs excluded from documentation using @internal tag or excludePrivate option",
            "Documentation build completes without errors or warnings",
            "Generated documentation validates all exported types from src/context/, src/planning/, and src/orchestrator/",
            "Documentation includes search functionality for APIs",
            "TypeDoc plugins configured for enhanced features (typedoc-plugin-markdown if needed)",
            "CI/CD pipeline step added to validate documentation builds on every commit",
            "Generated HTML output is accessible via web browser with proper styling",
            "Documentation coverage report shows 100% of public APIs documented"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "typedoc.json configuration file with project settings",
              "TSDoc comment templates for classes, interfaces, functions, and types",
              "Custom TypeDoc theme files if customization needed (in docs/theme/)",
              "Package.json scripts for documentation generation and validation",
              ".gitignore entry for generated documentation output",
              "TypeDoc plugins installed via npm/pnpm (typedoc-plugin-extras, etc.)"
            ]
          },
          "testable_properties": [],
          "function_id": "Documentation.configureTypeDoc",
          "related_concepts": [
            "TypeDoc configuration",
            "API documentation automation",
            "HTML documentation generation",
            "Documentation plugins",
            "Module documentation",
            "Type annotation preservation",
            "Documentation CI/CD integration"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_016.2",
          "description": "Create comprehensive Mermaid architecture diagrams showing dual-LLM context architecture, 7-step planning pipeline flow, context entry lifecycle, requirement hierarchy structure, and orchestrator interaction patterns",
          "type": "sub_process",
          "parent_id": "REQ_016",
          "children": [],
          "acceptance_criteria": [
            "Dual-LLM architecture diagram created showing Working LLM, Implementation LLM, and CentralContextStore interactions",
            "Diagram includes EntryType enumeration and context entry flow (summaries vs full content)",
            "Planning pipeline flowchart created with all 7 steps: Research \u2192 Memory Sync \u2192 Decomposition \u2192 Context Generation \u2192 Planning \u2192 Phase Decomposition \u2192 Beads Integration",
            "Each pipeline step shows inputs, outputs, and checkpoint save points",
            "Context entry lifecycle diagram showing creation, compression, search indexing, and retrieval",
            "Requirement hierarchy diagram showing parent \u2192 sub_process \u2192 implementation three-tier structure",
            "Component breakdown diagram showing frontend, backend, middleware, and shared components",
            "Orchestrator sequence diagram showing CLI invocation, context compilation, Claude session execution, and result handling",
            "Task batching diagram showing <200 entry bounds enforcement",
            "All diagrams use Mermaid syntax and render correctly in Markdown files",
            "Diagrams saved in docs/architecture/ directory with descriptive filenames",
            "Main architecture.md file includes all diagrams with explanatory text",
            "Diagrams validated for correctness using Mermaid CLI or online editor",
            "Color coding used to distinguish different component types (LLMs, stores, steps)",
            "Legend included for all diagrams explaining symbols and color meanings",
            "Diagrams include annotations for key design decisions (e.g., why <200 entry limit)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "docs/architecture/dual-llm-architecture.md with Mermaid diagram",
              "docs/architecture/planning-pipeline.md with flowchart",
              "docs/architecture/context-lifecycle.md with state diagram",
              "docs/architecture/requirement-hierarchy.md with tree diagram",
              "docs/architecture/orchestrator-flow.md with sequence diagram",
              "docs/architecture.md index file linking all diagrams",
              "Mermaid diagram source files (.mmd) for editing",
              "Package.json script for validating Mermaid diagrams (using @mermaid-js/mermaid-cli)",
              "README.md includes links to architecture documentation"
            ]
          },
          "testable_properties": [],
          "function_id": "Documentation.createMermaidDiagrams",
          "related_concepts": [
            "Architecture visualization",
            "Mermaid diagram syntax",
            "Dual-LLM architecture",
            "Planning pipeline visualization",
            "Context flow diagrams",
            "Component interaction diagrams",
            "System architecture documentation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_016.3",
          "description": "Write comprehensive Python-to-TypeScript migration guide documenting all language feature mappings, architectural pattern changes, testing framework conversions, and code example comparisons for developers porting Python code",
          "type": "sub_process",
          "parent_id": "REQ_016",
          "children": [],
          "acceptance_criteria": [
            "Migration guide created at docs/migration-guide.md with table of contents",
            "Language feature comparison table includes: dataclass\u2192interface+Zod, Enum\u2192enum, List\u2192Array, Dict\u2192Map/Record, Optional\u2192undefined, contextmanager\u2192async generator",
            "Async/await pattern section shows Python subprocess.run() vs TypeScript child_process.spawn() with code examples",
            "Error handling section compares try/except vs Result<T,E> pattern with examples",
            "Testing framework mapping documented: pytest\u2192Vitest, unittest.mock\u2192vitest.mock(), hypothesis\u2192fast-check",
            "Dependency equivalence table maps all 44 Python packages to TypeScript equivalents (pydantic\u2192Zod, httpx\u2192fetch/axios, etc.)",
            "Code example for each major pattern showing Python source and TypeScript equivalent side-by-side",
            "Section on dataclass migration with before/after examples including Zod schema generation",
            "Context manager pattern migration showing Python @contextmanager vs TypeScript try/finally or async generators",
            "BAML integration section comparing baml-py vs @baml/client with API differences",
            "Subprocess handling section with comprehensive spawn() vs run() comparison",
            "Property-based testing migration guide with hypothesis\u2192fast-check strategy mapping",
            "Common pitfalls section documenting known migration issues (e.g., Map vs Object, undefined vs null)",
            "Migration checklist for each module type (models, services, orchestrators)",
            "References to specific files in Python codebase and TypeScript equivalents",
            "Decision matrix for Map vs Record, when to use Zod vs plain types, etc."
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "docs/migration-guide.md with comprehensive language mappings",
              "docs/migration-examples/ directory with side-by-side code examples",
              "docs/migration-examples/dataclass-to-zod.md",
              "docs/migration-examples/subprocess-to-spawn.md",
              "docs/migration-examples/pytest-to-vitest.md",
              "docs/migration-examples/hypothesis-to-fast-check.md",
              "Migration checklist template in docs/migration-checklist.md",
              "Dependency mapping table in docs/dependency-mapping.md",
              "Pattern mapping reference card (PDF or Markdown table)",
              "Links to TypeScript equivalents for all Python stdlib modules used"
            ]
          },
          "testable_properties": [],
          "function_id": "Documentation.writeMigrationGuide",
          "related_concepts": [
            "Python to TypeScript migration",
            "Language feature mapping",
            "Pattern translation",
            "Testing framework migration",
            "Async/await pattern changes",
            "Type system differences",
            "Dependency equivalence mapping"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_016.4",
          "description": "Develop comprehensive developer onboarding guide with environment setup instructions, prerequisite installations, project structure explanation, development workflow, testing procedures, and contribution guidelines",
          "type": "sub_process",
          "parent_id": "REQ_016",
          "children": [],
          "acceptance_criteria": [
            "Developer onboarding guide created at docs/onboarding.md with clear structure",
            "Prerequisites section lists required software: Node.js 20+, pnpm, Git, Claude Code CLI, TypeScript 5.x",
            "Step-by-step environment setup with commands: pnpm install, environment variable configuration (.env.example provided)",
            "Project structure section explains all directories: src/context/, src/planning/, src/orchestrator/, src/loop-runner/, tests/",
            "Module explanation for each core component with purpose and responsibilities",
            "Development workflow documented: branch naming, commit conventions, PR process",
            "Testing guide includes: running unit tests (pnpm test), integration tests (pnpm test:integration), coverage reports (pnpm test:coverage)",
            "Code style and linting instructions: pnpm lint, pnpm format, auto-fix commands",
            "Debugging guide for VS Code with launch.json configuration examples",
            "Common development tasks documented: adding new pipeline step, creating new context entry type, adding new orchestrator command",
            "Troubleshooting section for common setup issues (BAML API key, Claude CLI not found, permission errors)",
            "Contribution guidelines include: code review process, testing requirements, documentation requirements",
            "Architecture overview diagram referenced from docs/architecture.md",
            "Links to API documentation (TypeDoc output), migration guide, deployment guide",
            "Quick start section allowing new developer to run example in <15 minutes",
            "Example commands for common tasks: creating new feature, running specific test suites, building for production"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "docs/onboarding.md comprehensive developer guide",
              ".env.example file with all required environment variables documented",
              "docs/project-structure.md with directory tree and explanations",
              "docs/development-workflow.md with Git workflow and conventions",
              "docs/testing-guide.md with testing strategies and examples",
              "docs/troubleshooting.md with common issues and solutions",
              ".vscode/launch.json with debugging configurations",
              ".vscode/settings.json with recommended workspace settings",
              "CONTRIBUTING.md file with contribution guidelines",
              "docs/quick-start.md with minimal setup for first run",
              "Example project in examples/ directory for testing setup"
            ]
          },
          "testable_properties": [],
          "function_id": "Documentation.developOnboardingGuide",
          "related_concepts": [
            "Developer onboarding",
            "Environment setup",
            "Development workflow",
            "Testing procedures",
            "Contribution guidelines",
            "Project structure documentation",
            "Tooling setup"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_016.5",
          "description": "Document comprehensive Docker deployment procedures including Dockerfile configuration, multi-stage builds, docker-compose setup, environment variable management, volume mounting strategies, and production deployment best practices",
          "type": "sub_process",
          "parent_id": "REQ_016",
          "children": [],
          "acceptance_criteria": [
            "Deployment guide created at docs/deployment.md with Docker-focused sections",
            "Dockerfile documented with explanations for each stage: builder stage (pnpm install, build) and production stage (minimal runtime)",
            "Multi-stage build strategy explained showing size optimization (node:20-alpine base)",
            "docker-compose.yml configuration documented with all services: silmari-ts main service, volume definitions, environment variables",
            "Environment variable management section shows .env file usage, required variables (ANTHROPIC_API_KEY, NODE_ENV), and docker-compose variable interpolation",
            "Volume mounting strategy documented for workspace persistence (/workspace) and Claude configuration (~/.claude)",
            "Production deployment checklist includes: building image, pushing to registry, deploying to server, health checks",
            "Docker commands documented: docker build -t silmari-ts ., docker-compose up -d, docker-compose logs -f",
            "Container security best practices: non-root user, minimal base image, no secrets in image layers",
            "Health check configuration documented with endpoint and parameters",
            "Logging strategy documented: stdout/stderr capture, log rotation, centralized logging",
            "Resource limits documented: memory limits, CPU constraints in docker-compose.yml",
            "Network configuration explained: port mappings, container networking, external access",
            "Backup and recovery procedures for volumes and configuration",
            "CI/CD integration guide for building and deploying Docker images (GitHub Actions example)",
            "Troubleshooting section for Docker-specific issues: permission errors, volume mounting, network connectivity",
            "Example docker-compose.yml for different environments: development, staging, production",
            "Documentation of Claude Code CLI installation in container"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "docs/deployment.md comprehensive deployment guide",
              "docs/docker-guide.md with Docker-specific details",
              "Dockerfile with comments explaining each step",
              "docker-compose.yml with inline comments",
              "docker-compose.dev.yml for development environment",
              "docker-compose.prod.yml for production environment",
              ".dockerignore file optimizing build context",
              "docs/deployment-checklist.md for production deployments",
              "docs/docker-security.md with security best practices",
              "scripts/docker-build.sh for automated builds",
              "scripts/docker-deploy.sh for deployment automation",
              ".github/workflows/docker-build.yml for CI/CD integration",
              "docs/docker-troubleshooting.md with common issues"
            ]
          },
          "testable_properties": [],
          "function_id": "Documentation.documentDockerDeployment",
          "related_concepts": [
            "Docker containerization",
            "Multi-stage builds",
            "Docker Compose orchestration",
            "Environment configuration",
            "Volume management",
            "Production deployment",
            "Container security",
            "CI/CD Docker integration"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_017",
      "description": "The system must implement error handling using Result type pattern with success/failure discriminated unions, custom error types (DecompositionError, ContextWindowArrayError, EntryBoundsError) with error codes, comprehensive error logging, graceful degradation for non-critical failures, and user-friendly error messages",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_017.1",
          "description": "Create a discriminated union Result<T, E> type that represents either a successful operation with a value or a failed operation with an error, enabling type-safe error handling throughout the application",
          "type": "sub_process",
          "parent_id": "REQ_017",
          "children": [],
          "acceptance_criteria": [
            "Result<T, E> type defined with success: true/false discriminator property",
            "Success variant contains { success: true, value: T } structure",
            "Failure variant contains { success: false, error: E } structure",
            "TypeScript compiler correctly narrows types based on success property",
            "Helper functions created: ok<T>(value: T) returns success Result",
            "Helper functions created: err<E>(error: E) returns failure Result",
            "Type guard functions implemented: isOk<T, E>(result: Result<T, E>): result is { success: true, value: T }",
            "Type guard functions implemented: isErr<T, E>(result: Result<T, E>): result is { success: false, error: E }",
            "Utility functions created: map<T, U, E>(result: Result<T, E>, fn: (value: T) => U): Result<U, E>",
            "Utility functions created: mapErr<T, E, F>(result: Result<T, E>, fn: (error: E) => F): Result<T, F>",
            "Utility functions created: flatMap<T, U, E>(result: Result<T, E>, fn: (value: T) => Result<U, E>): Result<U, E>",
            "All Result type utilities exported from shared/errors/result.ts",
            "Unit tests verify type narrowing works correctly in if statements",
            "Unit tests verify helper functions create correct Result structures",
            "Documentation includes usage examples for all utility functions"
          ],
          "implementation": {
            "frontend": [
              "No direct frontend components needed",
              "Result type used in all service layer API calls",
              "Error display logic uses Result type to determine success/failure states"
            ],
            "backend": [
              "No backend-specific implementation - Result type is shared",
              "All service methods return Result<T, E> instead of throwing exceptions",
              "API route handlers unwrap Result types and map to HTTP responses"
            ],
            "middleware": [
              "No middleware-specific implementation needed",
              "Middleware functions may return Result types for validation outcomes"
            ],
            "shared": [
              "Define Result<T, E> discriminated union type in shared/errors/result.ts",
              "Implement ok<T>(value: T): Result<T, never> factory function",
              "Implement err<E>(error: E): Result<never, E> factory function",
              "Implement isOk<T, E> type guard function",
              "Implement isErr<T, E> type guard function",
              "Implement map<T, U, E> transformation function",
              "Implement mapErr<T, E, F> error transformation function",
              "Implement flatMap<T, U, E> monadic bind function",
              "Implement unwrap(result: Result<T, E>): T function that throws if error",
              "Implement unwrapOr<T, E>(result: Result<T, E>, defaultValue: T): T function",
              "Export all Result type utilities from index file",
              "Create comprehensive JSDoc documentation for all functions"
            ]
          },
          "testable_properties": [],
          "function_id": "ErrorTypes.ResultType",
          "related_concepts": [
            "discriminated unions",
            "type safety",
            "functional error handling",
            "TypeScript generics",
            "error propagation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_017.2",
          "description": "Implement domain-specific error classes (DecompositionError, ContextWindowArrayError, EntryBoundsError) with structured error codes, messages, and metadata to provide detailed error context for different failure scenarios",
          "type": "sub_process",
          "parent_id": "REQ_017",
          "children": [],
          "acceptance_criteria": [
            "Base AppError class extends Error with errorCode, timestamp, and metadata properties",
            "AppError constructor accepts message, errorCode, and optional metadata object",
            "AppError.toJSON() method serializes error to { name, message, errorCode, timestamp, metadata, stack? }",
            "DecompositionError class extends AppError with codes: EXTRACTION_FAILED, VALIDATION_FAILED, HIERARCHY_INVALID, BAML_ERROR",
            "DecompositionError includes requirementId in metadata when applicable",
            "ContextWindowArrayError class extends AppError with codes: ENTRY_NOT_FOUND, COMPRESSION_FAILED, SEARCH_FAILED, SERIALIZATION_ERROR",
            "ContextWindowArrayError includes entryId in metadata when applicable",
            "EntryBoundsError class extends AppError with codes: ENTRY_LIMIT_EXCEEDED, INVALID_ENTRY_TYPE, REQUIRED_FIELD_MISSING",
            "EntryBoundsError includes bounds info (current, limit) in metadata",
            "All error classes have static factory methods for common scenarios (e.g., DecompositionError.extractionFailed())",
            "Error codes are typed as string literal unions for type safety",
            "Error classes are exported from shared/errors/index.ts",
            "Unit tests verify error code assignment and metadata capture",
            "Unit tests verify toJSON serialization includes all required fields",
            "Unit tests verify error instances are instanceof Error and custom class"
          ],
          "implementation": {
            "frontend": [
              "No direct frontend components needed",
              "Error display components accept custom error types and render errorCode/metadata",
              "Toast notifications format custom errors using error messages and codes"
            ],
            "backend": [
              "No backend-specific implementation - errors are shared",
              "Service methods throw custom error types for domain-specific failures",
              "API error handlers map custom errors to appropriate HTTP status codes"
            ],
            "middleware": [
              "Error handling middleware catches custom error types",
              "Middleware maps DecompositionError to 422 Unprocessable Entity",
              "Middleware maps ContextWindowArrayError to 400 Bad Request or 404 Not Found",
              "Middleware maps EntryBoundsError to 400 Bad Request",
              "Middleware serializes errors using toJSON() for API responses"
            ],
            "shared": [
              "Create base AppError class in shared/errors/app-error.ts",
              "Define DecompositionErrorCode type as string literal union",
              "Create DecompositionError class in shared/errors/decomposition-error.ts",
              "Add static factory methods to DecompositionError (extractionFailed, validationFailed, etc.)",
              "Define ContextWindowArrayErrorCode type as string literal union",
              "Create ContextWindowArrayError class in shared/errors/context-error.ts",
              "Add static factory methods to ContextWindowArrayError",
              "Define EntryBoundsErrorCode type as string literal union",
              "Create EntryBoundsError class in shared/errors/entry-bounds-error.ts",
              "Add static factory methods to EntryBoundsError",
              "Export all error classes and error code types from shared/errors/index.ts",
              "Create ErrorMetadata type as Record<string, unknown> for flexible metadata"
            ]
          },
          "testable_properties": [],
          "function_id": "ErrorTypes.CustomErrorClasses",
          "related_concepts": [
            "custom error classes",
            "error codes",
            "error metadata",
            "error hierarchies",
            "domain-driven design"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_017.3",
          "description": "Implement comprehensive error logging throughout the application with structured log entries, log levels, contextual information, and integration with logging infrastructure to enable debugging and monitoring",
          "type": "sub_process",
          "parent_id": "REQ_017",
          "children": [],
          "acceptance_criteria": [
            "Logger instance created using Winston or Pino with JSON structured output",
            "Log levels configured: error, warn, info, debug, trace",
            "Logger.error() method logs errors with full stack traces and metadata",
            "Logger captures errorCode from custom error classes automatically",
            "All Result<T, E> error paths log failures with appropriate context",
            "Catch blocks in all async functions log errors before returning err()",
            "Logger includes correlation IDs for request tracing when available",
            "Logger includes module/function names in log entries for source identification",
            "Logger serializes custom error classes using toJSON() method",
            "Logger filters sensitive data (API keys, passwords) from logs",
            "Log output includes timestamp, level, message, errorCode, metadata, stack fields",
            "Development environment logs to console with pretty formatting",
            "Production environment logs to files/stdout with JSON format",
            "Error logs include request context (user ID, session ID) when available",
            "Logger configuration supports log level override via environment variable",
            "Unit tests verify logger is called with correct level and payload",
            "Integration tests verify log entries written to output destination"
          ],
          "implementation": {
            "frontend": [
              "Browser logger implementation captures client-side errors",
              "Console.error() wrapper logs errors with structured format",
              "Errorboundary components log React errors with component stack",
              "API client logs failed requests before returning error Results"
            ],
            "backend": [
              "Create shared logger instance in src/lib/logger.ts using Winston/Pino",
              "Configure logger transports (console, file, remote) based on NODE_ENV",
              "Add logger.error() calls in all catch blocks throughout application",
              "Log decomposition errors in planning pipeline steps",
              "Log context store errors in CRUD operations",
              "Log BAML integration errors in API client wrappers",
              "Log subprocess execution errors in CLI wrappers",
              "Log checkpoint save/load errors in checkpoint manager"
            ],
            "middleware": [
              "Create error logging middleware that runs before error handlers",
              "Middleware extracts request context (method, path, user ID) for logs",
              "Middleware logs unhandled errors with full stack traces",
              "Middleware adds correlation ID to all log entries for request",
              "Middleware sanitizes sensitive headers/body before logging"
            ],
            "shared": [
              "Create Logger interface in shared/lib/logger.interface.ts",
              "Define LogEntry type with timestamp, level, message, metadata fields",
              "Implement createLogger factory function accepting config options",
              "Create sensitive data filter utility for log sanitization",
              "Create error serializer utility for custom error classes",
              "Export logger singleton from shared/lib/logger.ts",
              "Create logger configuration schema with environment variable mapping"
            ]
          },
          "testable_properties": [],
          "function_id": "Logging.ErrorLogging",
          "related_concepts": [
            "structured logging",
            "log levels",
            "error tracking",
            "observability",
            "debugging",
            "monitoring"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_017.4",
          "description": "Implement graceful degradation strategies for non-critical failures, allowing the system to continue operating with reduced functionality when non-essential components fail, rather than halting execution entirely",
          "type": "sub_process",
          "parent_id": "REQ_017",
          "children": [],
          "acceptance_criteria": [
            "Vector search failures fall back to no-search results (empty array) instead of blocking operations",
            "BAML API timeout errors fall back to cached/default responses when available",
            "Checkpoint save failures log warnings but allow pipeline to continue",
            "Context compression failures retain uncompressed entries with warning logs",
            "Beads CLI integration failures return synthetic issue IDs for testing/development",
            "File read errors for non-critical files return empty content with warnings",
            "Network request failures retry up to 3 times with exponential backoff before failing",
            "Feature detection errors assume safe defaults (e.g., no TypeScript detected)",
            "Related concepts extraction failures return empty array instead of blocking decomposition",
            "Summary generation failures fall back to truncated content",
            "Critical failures (database unavailable, auth failed) still halt execution appropriately",
            "Fallback behavior is configurable via feature flags or environment variables",
            "Degraded operation mode is logged clearly for debugging",
            "System health endpoint reports degraded status when fallbacks are active",
            "Unit tests verify fallback execution when primary path fails",
            "Integration tests verify system continues with degraded functionality"
          ],
          "implementation": {
            "frontend": [
              "Search component shows 'Search unavailable' message when vector search fails",
              "UI displays cached data with staleness indicator when API fails",
              "Forms disable non-critical validation when validation service fails",
              "Offline mode activated when network requests fail repeatedly"
            ],
            "backend": [
              "Implement withFallback<T>(primary: () => Promise<Result<T, E>>, fallback: () => T) utility",
              "Wrap vector search calls with fallback to empty results",
              "Wrap BAML calls with timeout and retry logic (3 attempts, exponential backoff)",
              "Implement checkpoint save with error logging but non-blocking behavior",
              "Wrap context compression with try-catch and fallback to uncompressed",
              "Implement Beads CLI wrapper with mock mode for development",
              "Add retry logic to HTTP requests using exponential backoff (100ms, 200ms, 400ms)",
              "Implement circuit breaker for external service calls (open after 5 failures)",
              "Create feature flag system for toggling fallback behaviors"
            ],
            "middleware": [
              "Request timeout middleware allows graceful degradation instead of hard timeout",
              "Rate limiting middleware returns degraded response instead of 429 error",
              "Authentication middleware falls back to read-only mode for certain failures"
            ],
            "shared": [
              "Create withRetry<T>(fn: () => Promise<T>, options: RetryOptions) utility",
              "Create withTimeout<T>(fn: () => Promise<T>, ms: number) utility",
              "Create CircuitBreaker class in shared/lib/circuit-breaker.ts",
              "Define CriticalityLevel enum: CRITICAL, HIGH, MEDIUM, LOW",
              "Create ErrorHandlingStrategy interface with fallback, retry, circuit breaker options",
              "Implement getErrorStrategy(error: AppError): ErrorHandlingStrategy function",
              "Create feature flag configuration schema with degradation toggles",
              "Export fallback utilities from shared/lib/resilience/index.ts"
            ]
          },
          "testable_properties": [],
          "function_id": "ErrorHandling.GracefulDegradation",
          "related_concepts": [
            "fault tolerance",
            "resilience",
            "partial failure handling",
            "fallback mechanisms",
            "circuit breaker pattern",
            "retry logic"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_017.5",
          "description": "Create user-friendly error message formatting system that translates technical error codes and stack traces into clear, actionable messages for end users, with suggestions for resolution when possible",
          "type": "sub_process",
          "parent_id": "REQ_017",
          "children": [],
          "acceptance_criteria": [
            "Error message formatter accepts custom error types and returns formatted string",
            "Technical error codes map to plain language descriptions (e.g., EXTRACTION_FAILED \u2192 'Failed to extract requirements from the input')",
            "Error messages include actionable next steps when available (e.g., 'Try simplifying your requirements description')",
            "Stack traces are hidden from end users but available in debug mode",
            "Error metadata is formatted into readable context (e.g., 'Entry ctx_abc123 not found' instead of raw JSON)",
            "Validation errors show which field failed and why (e.g., 'Title is required and must be between 1-200 characters')",
            "Network errors provide retry suggestions ('Connection failed. Please check your network and try again.')",
            "Permission errors explain what access is needed ('You need write access to modify this project')",
            "Resource not found errors suggest alternatives ('Requirement 1.2.3 not found. Did you mean 1.2.4?')",
            "Error messages avoid technical jargon for non-developer users",
            "Error message templates support interpolation for dynamic values",
            "Message formatter truncates long error details to first 500 characters with '...'",
            "Critical errors display prominent warnings or alerts",
            "Error messages support markdown formatting for CLI output",
            "Unit tests verify each error code has corresponding user message",
            "Unit tests verify message interpolation works correctly"
          ],
          "implementation": {
            "frontend": [
              "Create ErrorDisplay component that accepts custom error types",
              "Component renders user-friendly message from formatter",
              "Component shows 'Show technical details' collapsible for stack traces",
              "Toast notifications use formatted error messages",
              "Form validation errors display field-specific formatted messages",
              "Error boundary component shows formatted error with recovery actions",
              "Create error icon/styling based on error severity (error/warning/info)"
            ],
            "backend": [
              "API error responses include both errorCode and userMessage fields",
              "Error formatter called in middleware before serializing responses",
              "CLI output uses formatted errors with ANSI color codes (red for errors)"
            ],
            "middleware": [
              "Error handling middleware calls formatError() before sending response",
              "Middleware includes userMessage field in error response JSON",
              "Middleware separates technicalDetails (dev mode only) from userMessage",
              "Middleware maps HTTP status codes to user-friendly status messages"
            ],
            "shared": [
              "Create ErrorMessageMap type as Record<ErrorCode, MessageTemplate>",
              "Define message templates for all error codes in shared/errors/messages.ts",
              "Implement formatError(error: AppError): UserErrorMessage function",
              "Create UserErrorMessage interface with title, message, suggestion, severity fields",
              "Implement interpolate(template: string, values: Record<string, unknown>) utility",
              "Create truncateDetails(text: string, maxLength: number) utility",
              "Define ErrorSeverity enum: CRITICAL, ERROR, WARNING, INFO",
              "Implement getErrorSeverity(errorCode: ErrorCode): ErrorSeverity function",
              "Create markdown formatter for CLI error display",
              "Export formatError and related utilities from shared/errors/formatting.ts"
            ]
          },
          "testable_properties": [],
          "function_id": "ErrorMessages.UserFriendlyFormatting",
          "related_concepts": [
            "error messaging",
            "user experience",
            "error recovery",
            "internationalization",
            "error documentation"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_018",
      "description": "The system must support parallel testing of Python and TypeScript implementations with side-by-side execution, output comparison for behavioral equivalence, reference test suite conversion from Python to TypeScript maintaining test coverage, and automated regression testing to detect divergence",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_018.1",
          "description": "Set up parallel test execution infrastructure to run Python pytest and TypeScript Vitest tests concurrently with isolated environments and shared test fixtures",
          "type": "sub_process",
          "parent_id": "REQ_018",
          "children": [],
          "acceptance_criteria": [
            "Python tests run in isolated virtual environment (.venv) with pytest",
            "TypeScript tests run in isolated Node.js environment with Vitest",
            "Both test suites can execute concurrently without conflicts",
            "Test execution coordinator can launch both runners and collect results",
            "Shared test fixtures/data are accessible to both Python and TypeScript tests",
            "Test runs can be triggered individually or in parallel mode",
            "Exit codes properly reflect success/failure of both test suites",
            "Test execution times are measured and reported for both suites",
            "Parallel execution completes in max(python_time, typescript_time) + overhead",
            "Test output is properly captured and separated by language/suite"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "CLI command for parallel test execution (e.g., npm run test:parallel)",
              "Test orchestrator service that spawns Python and TypeScript processes",
              "Result aggregation service to collect test outcomes",
              "Test configuration service to manage environment variables",
              "Process management for concurrent test execution"
            ],
            "middleware": [
              "Environment isolation to prevent cross-contamination",
              "Resource locking mechanism for shared test data",
              "Test data provisioning for both environments"
            ],
            "shared": [
              "Test fixture data files (JSON/YAML) readable by both Python and TypeScript",
              "Test configuration schema defining parallel execution parameters",
              "Test result schema for unified reporting",
              "Common test data generators/factories",
              "Shared constants for test scenarios"
            ]
          },
          "testable_properties": [],
          "function_id": "ParallelTestRunner.setupInfrastructure",
          "related_concepts": [
            "test isolation",
            "concurrent execution",
            "environment management",
            "test orchestration",
            "fixture sharing"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_018.2",
          "description": "Implement output comparison utilities that analyze test results from Python and TypeScript implementations to verify behavioral equivalence across data structures, function outputs, and side effects",
          "type": "sub_process",
          "parent_id": "REQ_018",
          "children": [],
          "acceptance_criteria": [
            "Comparator can parse pytest JSON output format",
            "Comparator can parse Vitest JSON output format",
            "Deep object/structure comparison handles Python dict vs TypeScript object",
            "Deep array comparison handles Python list vs TypeScript array",
            "Date/time comparison normalizes ISO strings and Python datetime objects",
            "Numeric comparison handles float precision differences (configurable epsilon)",
            "String comparison can ignore whitespace differences (configurable)",
            "Null/undefined/None equivalence is properly handled",
            "Comparison reports include diff visualization (unified diff format)",
            "Mismatches are categorized by type (structural, value, type mismatch)",
            "Comparison results are exported in JSON and human-readable formats",
            "Performance: can compare 1000+ test outputs in < 5 seconds",
            "Supports custom comparison rules per test category",
            "Handles nested data structures up to 10 levels deep"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "OutputComparator service with compare() method",
              "Test result parser for pytest JSON reporter format",
              "Test result parser for Vitest JSON reporter format",
              "Deep comparison algorithm for objects/arrays",
              "Type normalization service (Python types \u2192 TypeScript types)",
              "Diff generation service using unified diff algorithm",
              "Mismatch categorization and reporting service",
              "Comparison result aggregator"
            ],
            "middleware": [
              "Normalization rules for cross-language type mapping",
              "Configurable comparison tolerance settings",
              "Custom comparator registration for special types"
            ],
            "shared": [
              "ComparisonResult interface/dataclass with mismatch details",
              "TestOutput interface/dataclass representing normalized test results",
              "DiffReport interface/dataclass for visualization",
              "ComparisonConfig interface/dataclass for tolerance settings",
              "Type mapping constants (Python type \u2192 TypeScript type)",
              "Normalization utility functions",
              "Deep equality utility functions"
            ]
          },
          "testable_properties": [],
          "function_id": "OutputComparator.compareTestResults",
          "related_concepts": [
            "behavioral equivalence",
            "output normalization",
            "diff algorithms",
            "type coercion",
            "deep comparison",
            "test result validation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_018.3",
          "description": "Convert Python pytest test files to TypeScript Vitest format while maintaining test coverage, preserving test logic, handling pytest-specific features (fixtures, parametrize, marks), and mapping assertions",
          "type": "sub_process",
          "parent_id": "REQ_018",
          "children": [],
          "acceptance_criteria": [
            "Converter parses Python test files using AST (ast module)",
            "Test functions (test_*) are converted to Vitest test() or it() functions",
            "pytest fixtures are converted to Vitest fixtures/setup functions",
            "pytest.mark.parametrize is converted to Vitest test.each()",
            "pytest.mark.skip/skipif is converted to test.skip()/test.skipIf()",
            "pytest.mark.asyncio is converted to async test functions",
            "Assertions (assert x == y) are converted to expect(x).toBe(y)",
            "Pytest-specific assertions (pytest.raises) are converted to expect().toThrow()",
            "Mock usage (unittest.mock) is converted to vitest.mock()",
            "Hypothesis property tests are converted to fast-check property tests",
            "Test file imports are mapped to TypeScript equivalents",
            "Converted tests maintain same code coverage as original",
            "Converter generates TypeScript type annotations from Python type hints",
            "Conversion report includes coverage delta and unmapped constructs",
            "Dry-run mode shows conversion preview without writing files",
            "Batch conversion processes entire test directory",
            "Conversion is idempotent (running twice produces same result)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "TestConverter service with convert() method",
              "Python AST parser to extract test structure",
              "TypeScript code generator for Vitest tests",
              "Fixture mapping service (pytest \u2192 Vitest)",
              "Assertion mapping service (assert \u2192 expect)",
              "Parametrize converter (@pytest.mark.parametrize \u2192 test.each)",
              "Mark converter (pytest.mark.* \u2192 Vitest equivalents)",
              "Import mapper (Python imports \u2192 TypeScript imports)",
              "Type annotation converter (Python type hints \u2192 TypeScript types)",
              "Coverage analyzer to verify test coverage parity",
              "Conversion report generator"
            ],
            "middleware": [
              "Validation rules for converted test syntax",
              "Conversion strategy configuration (fixture patterns, assertion styles)",
              "Error handling for unsupported Python constructs"
            ],
            "shared": [
              "TestNode interface representing parsed test structure",
              "FixtureMapping interface for fixture conversion rules",
              "AssertionMapping interface for assertion conversion patterns",
              "ConversionResult interface with success/failure details",
              "ConversionConfig interface for converter settings",
              "Fixture pattern constants (common pytest fixtures \u2192 Vitest equivalents)",
              "Assertion pattern constants (pytest assertions \u2192 Vitest matchers)",
              "Import mapping constants (Python modules \u2192 TypeScript packages)",
              "Utility functions for AST traversal and code generation"
            ]
          },
          "testable_properties": [],
          "function_id": "TestConverter.convertPytestToVitest",
          "related_concepts": [
            "test migration",
            "AST parsing",
            "code generation",
            "fixture conversion",
            "assertion mapping",
            "test coverage preservation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_018.4",
          "description": "Add regression test suite that automatically detects behavioral divergence between Python and TypeScript implementations by running identical test scenarios and flagging mismatches",
          "type": "sub_process",
          "parent_id": "REQ_018",
          "children": [],
          "acceptance_criteria": [
            "Regression suite includes test scenarios for all core modules (context_window_array, planning_pipeline)",
            "Each test scenario executes identical logic in both Python and TypeScript",
            "Test scenarios use shared input data fixtures",
            "Regression tests validate data model equivalence (ContextEntry, RequirementNode)",
            "Regression tests validate search index equivalence (TF-IDF results)",
            "Regression tests validate pipeline step outputs (decomposition, planning)",
            "Regression tests validate orchestrator behavior (session management, feature tracking)",
            "Divergence detection flags any structural or value differences",
            "Regression suite runs on every commit via CI/CD",
            "Test failures block merges when equivalence is broken",
            "Regression test results include detailed diff reports",
            "Supports baseline snapshots (golden files) for expected outputs",
            "Can update baseline snapshots when intentional changes occur",
            "Regression tests cover 100% of critical code paths",
            "Execution time for full regression suite < 10 minutes",
            "Regression test report includes coverage comparison (Python vs TypeScript)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "RegressionTester service with validateEquivalence() method",
              "Test scenario executor for Python implementation",
              "Test scenario executor for TypeScript implementation",
              "Golden file manager for baseline snapshots",
              "Divergence detector comparing Python and TypeScript outputs",
              "Regression test report generator",
              "Baseline update service for intentional changes",
              "Critical path coverage analyzer",
              "Test scenario loader from fixture files"
            ],
            "middleware": [
              "Test scenario validation to ensure identical inputs",
              "Output normalization before comparison",
              "Failure threshold configuration (tolerable vs blocking divergence)"
            ],
            "shared": [
              "TestScenario interface defining input, expected output, and test logic",
              "RegressionResult interface with pass/fail and divergence details",
              "GoldenFile interface for snapshot management",
              "DivergenceReport interface with categorized mismatches",
              "Test scenario fixture files (JSON/YAML) with input data",
              "Expected output baseline files (golden files)",
              "Coverage comparison schema",
              "Utility functions for scenario execution and comparison"
            ]
          },
          "testable_properties": [],
          "function_id": "RegressionTester.validateEquivalence",
          "related_concepts": [
            "regression testing",
            "behavioral equivalence validation",
            "test harness",
            "golden file testing",
            "snapshot testing",
            "continuous validation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_018.5",
          "description": "Create CI/CD pipeline configuration that orchestrates parallel test execution, output comparison, regression testing, and equivalence validation with automated reporting and failure handling",
          "type": "sub_process",
          "parent_id": "REQ_018",
          "children": [],
          "acceptance_criteria": [
            "CI pipeline runs on every pull request and main branch commit",
            "Pipeline includes separate jobs for Python tests and TypeScript tests",
            "Python test job uses Python 3.12.3 and installs dependencies from requirements.txt",
            "TypeScript test job uses Node.js 20+ and installs dependencies from package.json",
            "Both test jobs run in parallel to minimize total pipeline time",
            "Output comparison job runs after both test jobs complete successfully",
            "Regression test job runs after output comparison succeeds",
            "Pipeline fails if any test suite fails (Python or TypeScript)",
            "Pipeline fails if output comparison detects behavioral divergence",
            "Pipeline fails if regression tests detect equivalence violations",
            "Pipeline generates unified test report with coverage metrics",
            "Test artifacts (JSON reports, diffs) are uploaded and accessible",
            "Pipeline sends notifications on failure with actionable details",
            "Pipeline caches dependencies to speed up subsequent runs",
            "Total pipeline execution time < 15 minutes for typical changes",
            "Pipeline supports manual re-run of individual stages",
            "Branch protection requires passing pipeline before merge",
            "Pipeline configuration is version-controlled (GitHub Actions YAML or similar)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "CI pipeline configuration file (e.g., .github/workflows/parallel-validation.yml)",
              "Python test job definition with pytest execution",
              "TypeScript test job definition with Vitest execution",
              "Output comparison job with dependency on test jobs",
              "Regression test job with dependency on comparison job",
              "Test report aggregation script",
              "Artifact upload configuration for test results",
              "Notification service integration (GitHub comments, Slack, email)",
              "Dependency caching configuration",
              "Pipeline status badge generation"
            ],
            "middleware": [
              "Job dependency orchestration (test \u2192 compare \u2192 regression)",
              "Failure handling with detailed error reporting",
              "Artifact retention policy configuration"
            ],
            "shared": [
              "CI configuration schema (YAML/JSON)",
              "Test report schema for unified reporting",
              "Pipeline status interface",
              "Notification template for failures",
              "Cache key generation utilities",
              "Environment variable configuration for both Python and TypeScript jobs"
            ]
          },
          "testable_properties": [],
          "function_id": "CIPipeline.setupParallelValidation",
          "related_concepts": [
            "continuous integration",
            "CI/CD automation",
            "parallel job execution",
            "test orchestration",
            "automated reporting",
            "pipeline stages"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_019",
      "description": "The system must implement async subprocess execution using child_process.spawn with promise-based API, stdout/stderr stream handling, exit code validation, timeout support with configurable duration, process cleanup on errors, and support for both JSON and text output parsing",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_019.1",
          "description": "Create a promise-based wrapper around child_process.spawn that handles process lifecycle and returns a typed result object containing stdout, stderr, exit code, and signal information",
          "type": "sub_process",
          "parent_id": "REQ_019",
          "children": [],
          "acceptance_criteria": [
            "Function accepts command string, arguments array, and optional spawn options",
            "Returns a Promise that resolves with { stdout: string, stderr: string, exitCode: number, signal: string | null }",
            "Promise rejects if process exits with non-zero code (configurable)",
            "Handles process 'error' event (e.g., ENOENT for missing executable)",
            "Properly propagates spawn options like cwd, env, shell",
            "TypeScript interfaces define all input/output types",
            "Unit tests verify promise resolution on successful execution",
            "Unit tests verify promise rejection on process errors",
            "Unit tests verify correct data structure in resolved promise",
            "Documentation includes usage examples with async/await"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "SubprocessExecutor class with spawn() method",
              "Type definitions: SpawnOptions, SpawnResult interfaces",
              "Error handling for ENOENT and spawn failures",
              "Event listener registration for 'error', 'exit', 'close'"
            ],
            "middleware": [],
            "shared": [
              "SpawnOptions interface extending child_process.SpawnOptions",
              "SpawnResult interface with stdout, stderr, exitCode, signal properties",
              "SubprocessError custom error class with exitCode and signal properties",
              "Type guards for validating spawn results"
            ]
          },
          "testable_properties": [],
          "function_id": "SubprocessExecutor.createPromiseWrapper",
          "related_concepts": [
            "child_process.spawn",
            "Promise API",
            "EventEmitter",
            "process lifecycle management",
            "type safety with TypeScript",
            "error propagation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_019.2",
          "description": "Implement buffered stream handling for stdout and stderr that accumulates output data, handles encoding, supports real-time streaming callbacks, and provides backpressure management",
          "type": "sub_process",
          "parent_id": "REQ_019",
          "children": [],
          "acceptance_criteria": [
            "Captures all stdout data chunks and concatenates into single string",
            "Captures all stderr data chunks and concatenates into single string",
            "Handles different encodings (utf8, ascii, base64) via configuration",
            "Supports optional onStdout and onStderr callbacks for real-time output",
            "Implements backpressure handling to prevent memory overflow",
            "Handles 'data', 'end', and 'error' events on both streams",
            "Preserves output even if process terminates abruptly",
            "Unit tests verify complete output capture for large outputs (>1MB)",
            "Unit tests verify real-time callbacks fire for each chunk",
            "Unit tests verify correct encoding handling",
            "Unit tests verify no data loss on rapid output",
            "Integration tests with actual subprocess verify output accuracy"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "StreamCapture class with attachToProcess() method",
              "Buffer accumulation logic for stdout/stderr",
              "Event handler registration for 'data', 'end', 'error'",
              "Callback invocation logic for real-time streaming",
              "Encoding configuration and conversion"
            ],
            "middleware": [],
            "shared": [
              "StreamOptions interface with encoding and callback properties",
              "StreamCallback type definition: (chunk: string) => void",
              "StreamResult interface with buffered output and metadata",
              "Encoding type union: 'utf8' | 'ascii' | 'base64' | 'buffer'",
              "Constants for maximum buffer size and chunk size"
            ]
          },
          "testable_properties": [],
          "function_id": "StreamHandler.captureOutputStreams",
          "related_concepts": [
            "Node.js Readable streams",
            "buffer management",
            "encoding handling",
            "backpressure",
            "stream events",
            "data chunking",
            "real-time output callbacks"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_019.3",
          "description": "Implement exit code validation with configurable success codes, detailed error construction including stdout/stderr context, and support for both strict and permissive validation modes",
          "type": "sub_process",
          "parent_id": "REQ_019",
          "children": [],
          "acceptance_criteria": [
            "Default behavior treats exit code 0 as success, all others as failure",
            "Supports custom success code configuration (e.g., [0, 1] for some CLIs)",
            "Creates detailed error messages including command, exit code, stdout, stderr",
            "Handles signal termination (SIGTERM, SIGKILL) separately from exit codes",
            "Supports 'strict' mode (any non-zero fails) and 'permissive' mode (custom codes)",
            "Error object includes structured data for programmatic handling",
            "Supports ignore-error mode for non-critical subprocesses",
            "Unit tests verify correct validation for exit code 0",
            "Unit tests verify rejection for non-zero exit codes",
            "Unit tests verify custom success codes work correctly",
            "Unit tests verify signal termination error messages",
            "Unit tests verify error object structure includes all context",
            "Integration tests verify real subprocess error handling"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "ExitCodeValidator class with validate() method",
              "Error construction logic with context enrichment",
              "Success code matching logic (array contains check)",
              "Signal detection and handling logic",
              "Validation mode switch (strict/permissive/ignore)"
            ],
            "middleware": [],
            "shared": [
              "ValidationOptions interface with successCodes, mode, ignoreErrors properties",
              "ValidationMode type: 'strict' | 'permissive' | 'ignore'",
              "SubprocessError class extending Error with exitCode, signal, stdout, stderr, command properties",
              "isSuccessCode() utility function",
              "Constants for common exit codes (SUCCESS=0, GENERAL_ERROR=1, SIGTERM=143)",
              "Error message templates for different failure modes"
            ]
          },
          "testable_properties": [],
          "function_id": "ExitCodeValidator.validateAndHandle",
          "related_concepts": [
            "process exit codes",
            "POSIX exit codes",
            "signal handling",
            "error context enrichment",
            "validation strategies",
            "error recovery"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_019.4",
          "description": "Implement configurable timeout mechanism that terminates processes exceeding duration limits, supports graceful shutdown with SIGTERM followed by SIGKILL, and cleans up resources on timeout expiration",
          "type": "sub_process",
          "parent_id": "REQ_019",
          "children": [],
          "acceptance_criteria": [
            "Accepts timeout duration in milliseconds (0 = no timeout)",
            "Starts timeout timer when process spawns",
            "Sends SIGTERM to process when timeout expires",
            "Waits for graceful shutdown period (default 5s) after SIGTERM",
            "Sends SIGKILL if process doesn't exit after graceful period",
            "Clears timeout timer if process completes before timeout",
            "Throws TimeoutError with process details and timeout duration",
            "Supports per-execution timeout override",
            "Cleans up all event listeners on timeout",
            "Unit tests verify timeout triggers after configured duration",
            "Unit tests verify SIGTERM sent on timeout",
            "Unit tests verify SIGKILL sent after graceful period",
            "Unit tests verify timeout cleared on normal completion",
            "Unit tests verify TimeoutError structure",
            "Integration tests with real long-running process verify termination"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "TimeoutManager class with start(), clear(), onTimeout() methods",
              "Timer creation and cleanup logic",
              "SIGTERM/SIGKILL sending logic with delay",
              "Process termination verification",
              "Graceful shutdown wait implementation"
            ],
            "middleware": [],
            "shared": [
              "TimeoutOptions interface with timeout, gracefulShutdownPeriod properties",
              "TimeoutError class extending SubprocessError with timeout and killedByTimeout properties",
              "DEFAULT_TIMEOUT constant (e.g., 120000ms = 2 minutes)",
              "GRACEFUL_SHUTDOWN_PERIOD constant (e.g., 5000ms)",
              "ProcessSignal type: 'SIGTERM' | 'SIGKILL' | 'SIGINT'",
              "killProcess() utility function with signal parameter"
            ]
          },
          "testable_properties": [],
          "function_id": "TimeoutManager.configureAndEnforce",
          "related_concepts": [
            "setTimeout/clearTimeout",
            "process.kill()",
            "graceful shutdown",
            "SIGTERM vs SIGKILL",
            "resource cleanup",
            "timeout errors",
            "AbortController pattern"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_019.5",
          "description": "Implement comprehensive cleanup logic that removes event listeners, kills orphaned processes, releases resources, and ensures no resource leaks on errors, timeouts, or normal completion",
          "type": "sub_process",
          "parent_id": "REQ_019",
          "children": [],
          "acceptance_criteria": [
            "Removes all registered event listeners on process completion or error",
            "Kills process if still running when cleanup is triggered",
            "Clears timeout timers to prevent memory leaks",
            "Closes stdin stream if opened",
            "Handles cleanup idempotently (safe to call multiple times)",
            "Executes cleanup in finally block to ensure execution",
            "Logs cleanup actions for debugging",
            "Supports custom cleanup callbacks for resource-specific cleanup",
            "Handles cleanup errors without masking original error",
            "Unit tests verify all listeners removed after execution",
            "Unit tests verify process killed on error",
            "Unit tests verify timeout cleared on cleanup",
            "Unit tests verify idempotent cleanup behavior",
            "Integration tests verify no orphaned processes after errors",
            "Memory profiling tests verify no leaks after repeated executions"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "CleanupHandler class with cleanup() method",
              "Event listener tracking and removal logic",
              "Process termination check and kill logic",
              "Resource release sequencing",
              "Error-safe cleanup execution",
              "Custom cleanup callback invocation"
            ],
            "middleware": [],
            "shared": [
              "CleanupOptions interface with onCleanup callback property",
              "CleanupCallback type: () => void | Promise<void>",
              "ResourceTracker class for tracking listeners and timers",
              "isProcessRunning() utility function",
              "safeCleanup() wrapper for error-safe execution",
              "CleanupError class for cleanup-specific errors"
            ]
          },
          "testable_properties": [],
          "function_id": "CleanupHandler.handleErrorsAndCleanup",
          "related_concepts": [
            "resource cleanup",
            "event listener removal",
            "orphaned processes",
            "file descriptor cleanup",
            "memory leak prevention",
            "error recovery",
            "try-finally pattern"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_019.6",
          "description": "Build flexible output parsers that handle JSON deserialization with error recovery, text parsing with configurable delimiters and patterns, and automatic format detection based on content inspection",
          "type": "sub_process",
          "parent_id": "REQ_019",
          "children": [],
          "acceptance_criteria": [
            "JSON parser uses JSON.parse() with try-catch error handling",
            "JSON parser validates structure with Zod schema if provided",
            "JSON parser provides detailed error messages with line/column info",
            "Text parser supports line-based splitting with configurable delimiter",
            "Text parser supports regex pattern extraction",
            "Auto-detection attempts JSON parse first, falls back to text",
            "Supports custom parser registration for other formats (XML, YAML)",
            "Handles partial JSON output (extracts valid JSON from mixed output)",
            "Handles empty output gracefully",
            "Unit tests verify valid JSON parsing",
            "Unit tests verify JSON parsing errors with details",
            "Unit tests verify Zod schema validation",
            "Unit tests verify text parsing with various delimiters",
            "Unit tests verify regex pattern extraction",
            "Unit tests verify auto-detection for JSON and text",
            "Unit tests verify partial JSON extraction",
            "Integration tests with real subprocess JSON output"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "OutputParser class with parse() method",
              "JSON parsing logic with error handling",
              "Zod schema validation integration",
              "Text parsing with delimiter splitting",
              "Regex extraction logic",
              "Auto-detection heuristics (JSON start characters)",
              "Custom parser registration and invocation"
            ],
            "middleware": [],
            "shared": [
              "ParseOptions interface with format, schema, delimiter, pattern properties",
              "OutputFormat type: 'json' | 'text' | 'auto' | 'custom'",
              "ParseResult<T> generic type with data and metadata",
              "ParserError class with parsing context",
              "extractJSON() utility for partial JSON extraction",
              "isValidJSON() utility function",
              "DEFAULT_DELIMITERS constant (newline, comma, etc.)",
              "CustomParser interface for extensibility"
            ]
          },
          "testable_properties": [],
          "function_id": "OutputParser.parseFormats",
          "related_concepts": [
            "JSON.parse()",
            "JSON validation",
            "text parsing",
            "regex patterns",
            "format detection",
            "error recovery",
            "Zod schemas",
            "content-type detection"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_020",
      "description": "The system must implement strict TypeScript configuration with strict: true enabling all strict checks, esModuleInterop for seamless CommonJS/ESM interop, resolveJsonModule for JSON imports, skipLibCheck for faster compilation, and target ES2022 with ESNext module resolution using bundler mode",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_020.1",
          "description": "Configure tsconfig.json with strict: true to enable all strict type-checking options including noImplicitAny, strictNullChecks, strictFunctionTypes, strictBindCallApply, strictPropertyInitialization, noImplicitThis, alwaysStrict, and useUnknownInCatchVariables",
          "type": "sub_process",
          "parent_id": "REQ_020",
          "children": [],
          "acceptance_criteria": [
            "tsconfig.json contains 'strict': true in compilerOptions",
            "TypeScript compiler enforces all strict checks on compilation",
            "Code with implicit 'any' types fails to compile",
            "Code with potential null/undefined access without checks fails to compile",
            "All strict sub-options (noImplicitAny, strictNullChecks, etc.) are effectively enabled",
            "npm run type-check or tsc --noEmit command succeeds without errors on existing codebase",
            "Attempt to use 'this' in untyped context produces compilation error"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "tsconfig.json configuration file with strict: true",
              "Type assertion utilities for narrow type guards",
              "Null-safe helper functions for optional chaining alternatives",
              "Documentation on strict mode implications for developers"
            ]
          },
          "testable_properties": [],
          "function_id": "TSConfig.configureStrictMode",
          "related_concepts": [
            "type safety",
            "compile-time error detection",
            "null safety",
            "implicit any prevention",
            "strict function signatures"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_020.2",
          "description": "Enable esModuleInterop to allow default imports from CommonJS modules and resolveJsonModule to support importing JSON files with type inference, ensuring seamless interoperability between module systems",
          "type": "sub_process",
          "parent_id": "REQ_020",
          "children": [],
          "acceptance_criteria": [
            "tsconfig.json contains 'esModuleInterop': true in compilerOptions",
            "tsconfig.json contains 'resolveJsonModule': true in compilerOptions",
            "CommonJS modules can be imported using 'import foo from \"module\"' syntax without errors",
            "JSON files can be imported with 'import data from \"./file.json\"' syntax",
            "Imported JSON has proper type inference (not typed as 'any')",
            "Package imports from node_modules work correctly with default imports",
            "allowSyntheticDefaultImports is implicitly enabled by esModuleInterop",
            "Test file importing package.json successfully compiles and provides type completion"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "tsconfig.json with esModuleInterop and resolveJsonModule flags",
              "Example import patterns documentation for team reference",
              "Type declaration files (*.d.ts) for JSON modules if needed",
              "Migration guide for converting require() to import statements"
            ]
          },
          "testable_properties": [],
          "function_id": "TSConfig.enableInteropAndJsonSupport",
          "related_concepts": [
            "module interoperability",
            "CommonJS/ESM compatibility",
            "JSON type inference",
            "default import syntax",
            "synthetic default imports"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_020.3",
          "description": "Set compilation target to ES2022 for modern JavaScript features and module system to ESNext to use the latest ECMAScript module syntax, ensuring compatibility with modern runtimes like Node.js 18+",
          "type": "sub_process",
          "parent_id": "REQ_020",
          "children": [],
          "acceptance_criteria": [
            "tsconfig.json contains 'target': 'ES2022' in compilerOptions",
            "tsconfig.json contains 'module': 'ESNext' in compilerOptions",
            "Compiled JavaScript uses ES2022 syntax (class fields, top-level await, etc.)",
            "Top-level await in TypeScript files compiles without errors",
            "Private class fields (#privateField) compile correctly",
            "Static initialization blocks in classes are supported",
            "Output JavaScript is compatible with Node.js 18+ runtime",
            "No downleveling of ES2022 features occurs in compiled output"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "tsconfig.json with target and module configuration",
              "Runtime environment documentation specifying Node.js 18+ requirement",
              "package.json engines field specifying minimum Node.js version",
              "CI/CD configuration validating Node.js version compatibility"
            ]
          },
          "testable_properties": [],
          "function_id": "TSConfig.setTargetAndModuleSystem",
          "related_concepts": [
            "ECMAScript 2022 features",
            "top-level await",
            "class fields",
            "private methods",
            "ESNext modules",
            "Node.js compatibility",
            "modern JavaScript runtime"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_020.4",
          "description": "Set moduleResolution to 'bundler' to enable modern module resolution strategy optimized for bundlers like esbuild, tsup, and Vite, supporting package.json exports field and extension-less imports",
          "type": "sub_process",
          "parent_id": "REQ_020",
          "children": [],
          "acceptance_criteria": [
            "tsconfig.json contains 'moduleResolution': 'bundler' in compilerOptions",
            "TypeScript correctly resolves imports using package.json 'exports' field",
            "Extension-less imports (.js extension not required) compile successfully",
            "Subpath imports (e.g., 'package/subpath') resolve correctly",
            "Conditional exports (e.g., import/require conditions) are respected",
            "TypeScript resolution matches bundler behavior for all import paths",
            "No module resolution errors occur during type checking",
            "Import paths work consistently between TypeScript and bundler (tsup/esbuild)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "tsconfig.json with moduleResolution: bundler",
              "package.json with properly configured exports field if publishing library",
              "Path mapping configuration for internal module aliases",
              "Build tool configuration (tsup/esbuild) aligned with TypeScript resolution",
              "Documentation on import path conventions for the project"
            ]
          },
          "testable_properties": [],
          "function_id": "TSConfig.configureBundlerModuleResolution",
          "related_concepts": [
            "bundler module resolution",
            "package.json exports",
            "extension-less imports",
            "subpath imports",
            "conditional exports",
            "tsup compatibility",
            "esbuild compatibility"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_020.5",
          "description": "Add appropriate type definitions including @types packages for Node.js and testing frameworks, configure types array in tsconfig to include necessary ambient declarations, and enable skipLibCheck to skip type checking of declaration files for faster compilation",
          "type": "sub_process",
          "parent_id": "REQ_020",
          "children": [],
          "acceptance_criteria": [
            "tsconfig.json contains 'skipLibCheck': true in compilerOptions",
            "@types/node package installed as devDependency with version matching Node.js runtime",
            "vitest/globals or testing framework types included in types array",
            "tsconfig.json contains 'types': ['vitest/globals'] or appropriate testing types",
            "All third-party packages have type definitions (either bundled or from @types)",
            "TypeScript compilation time is optimized by skipping library checks",
            "IDE provides full type completion for Node.js APIs and test framework APIs",
            "No type errors related to missing declarations during compilation",
            "Custom ambient declarations (*.d.ts) are properly recognized by TypeScript"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "tsconfig.json with skipLibCheck and types configuration",
              "package.json with @types/node devDependency (^20.0.0 or appropriate version)",
              "Global type declaration file (e.g., src/types/global.d.ts) for custom ambient types",
              "Type definition documentation listing required @types packages",
              "CI validation ensuring all @types packages match runtime dependencies",
              "Custom module declaration files for untyped third-party packages if needed"
            ]
          },
          "testable_properties": [],
          "function_id": "TSConfig.addTypeDefinitions",
          "related_concepts": [
            "ambient type declarations",
            "@types packages",
            "DefinitelyTyped",
            "skipLibCheck optimization",
            "vitest types",
            "node types",
            "type declaration files"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_021",
      "description": "The system must implement CI/CD pipeline using GitHub Actions with automated testing on pull requests, type checking validation, linting and formatting checks using Biome, test coverage reporting with 90%+ threshold, and automated builds for distribution",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_021.1",
          "description": "Create comprehensive GitHub Actions workflow YAML file with job definitions for testing, type checking, linting, coverage reporting, and build artifact generation, including proper triggers on pull requests and push to main branch",
          "type": "sub_process",
          "parent_id": "REQ_021",
          "children": [],
          "acceptance_criteria": [
            "Workflow file exists at .github/workflows/ci.yml with valid YAML syntax",
            "Workflow triggers on pull_request events targeting main branch",
            "Workflow triggers on push events to main branch",
            "Node.js version 20.x is specified for all jobs",
            "pnpm package manager is configured with caching enabled",
            "Jobs are configured to fail fast on first error",
            "Workflow includes concurrency control to cancel outdated runs on new commits",
            "Checkout action uses v4 with proper token permissions",
            "Setup node action includes pnpm installation via corepack",
            "Dependencies installation step uses --frozen-lockfile flag",
            "All jobs have clear names and descriptions",
            "Workflow passes validation using GitHub Actions workflow validator"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              ".github/workflows/ci.yml workflow definition file",
              "Workflow job definitions with proper naming conventions",
              "Caching configuration for node_modules using pnpm store path",
              "Concurrency group configuration using github.workflow and github.ref",
              "Environment variable definitions for Node.js and pnpm versions"
            ]
          },
          "testable_properties": [],
          "function_id": "GitHubActions.workflowConfiguration",
          "related_concepts": [
            "CI/CD pipeline orchestration",
            "GitHub Actions syntax and best practices",
            "Workflow job dependencies and parallelization",
            "Node.js version matrix testing",
            "Caching strategies for node_modules and build artifacts",
            "Environment variable and secrets management"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_021.2",
          "description": "Implement automated test execution job that runs the complete Vitest test suite including unit tests, integration tests, and property-based tests with fast-check, ensuring all test markers are respected and test results are properly reported",
          "type": "sub_process",
          "parent_id": "REQ_021",
          "children": [],
          "acceptance_criteria": [
            "Test job depends on dependency installation completion",
            "Test command executes 'pnpm test' with appropriate flags",
            "Tests run in CI mode with --reporter=verbose and --reporter=github-actions",
            "Test execution includes all test files matching **/*.test.ts pattern",
            "Integration tests marked with @integration are executed",
            "Property-based tests with fast-check execute successfully",
            "Test failures cause the workflow to fail with exit code 1",
            "Test results are uploaded as workflow artifacts with 7-day retention",
            "GitHub Actions annotations are created for test failures with file/line references",
            "Test execution completes within 10-minute timeout threshold",
            "Test job runs in parallel with type checking and linting jobs when possible"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "Vitest configuration with CI-specific reporters (vitest.config.ts)",
              "Test execution script in package.json (test:ci command)",
              "GitHub Actions test job definition in ci.yml",
              "Test artifact upload action configuration",
              "Test timeout configuration in workflow job",
              "Environment variables for test execution (NODE_ENV=test)"
            ]
          },
          "testable_properties": [],
          "function_id": "GitHubActions.automatedTestExecution",
          "related_concepts": [
            "Vitest test runner configuration",
            "Test parallelization and sharding",
            "Test result artifact collection",
            "Integration test execution with BAML mocks",
            "Property-based testing with fast-check",
            "Test failure reporting and annotations"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_021.3",
          "description": "Configure parallel type checking validation using TypeScript compiler and linting/formatting checks using Biome, ensuring strict type safety and code quality standards are enforced on every commit",
          "type": "sub_process",
          "parent_id": "REQ_021",
          "children": [],
          "acceptance_criteria": [
            "Type checking job executes 'pnpm type-check' command with tsc --noEmit",
            "Type checking enforces strict mode from tsconfig.json configuration",
            "Type checking validates all TypeScript files in src/ directory",
            "Biome linting job executes 'pnpm lint' command",
            "Biome formatting job executes 'pnpm format --check' command",
            "Type errors generate GitHub Actions annotations with file and line references",
            "Linting errors cause workflow failure with detailed error messages",
            "Formatting violations cause workflow failure with diff output",
            "Type checking and linting jobs run in parallel to minimize CI time",
            "Both jobs complete within 5-minute timeout threshold",
            "Biome configuration (biome.json) is validated for syntax correctness",
            "Type checking excludes node_modules, dist, and *.test.ts files per tsconfig"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "TypeScript type checking job definition in ci.yml",
              "Biome linting job definition in ci.yml",
              "tsconfig.json with strict: true and CI-appropriate settings",
              "biome.json configuration with linting and formatting rules",
              "package.json scripts: type-check, lint, format",
              "GitHub Actions annotation generation for type errors",
              "Job dependency configuration for parallel execution"
            ]
          },
          "testable_properties": [],
          "function_id": "GitHubActions.typeCheckingAndLinting",
          "related_concepts": [
            "TypeScript compiler type checking in CI",
            "Biome linter and formatter integration",
            "Parallel job execution for faster feedback",
            "Code quality gates and failure thresholds",
            "ESLint migration to Biome",
            "Type error reporting and annotations"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_021.4",
          "description": "Implement comprehensive test coverage reporting using Vitest coverage tools with 90% minimum threshold enforcement for statements, branches, functions, and lines, including coverage badge generation and PR comments with coverage differentials",
          "type": "sub_process",
          "parent_id": "REQ_021",
          "children": [],
          "acceptance_criteria": [
            "Coverage job executes 'pnpm test:coverage' command with Vitest",
            "Coverage provider is configured (v8 or istanbul) in vitest.config.ts",
            "Coverage thresholds enforce minimum 90% for statements, branches, functions, and lines",
            "Coverage collection includes all source files in src/ directory",
            "Coverage excludes test files, configuration files, and type definitions",
            "Coverage report generates lcov.info file for external tools",
            "Coverage report generates HTML report uploaded as workflow artifact",
            "Coverage job fails if any threshold is below 90%",
            "Coverage summary is posted as PR comment showing current coverage metrics",
            "Coverage differential shows increase/decrease compared to base branch",
            "Coverage badge SVG is generated and stored in repository",
            "Coverage reports are retained for 30 days as workflow artifacts"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "Vitest coverage configuration in vitest.config.ts with thresholds",
              "Coverage job definition in ci.yml with threshold enforcement",
              "Coverage artifact upload action for HTML and lcov reports",
              "PR comment action for coverage summary (e.g., romeovs/lcov-reporter-action)",
              "Coverage badge generation script or action integration",
              "package.json test:coverage script with appropriate flags",
              "Coverage provider installation (@vitest/coverage-v8 or @vitest/coverage-istanbul)",
              "Coverage exclusion patterns in vitest.config.ts"
            ]
          },
          "testable_properties": [],
          "function_id": "GitHubActions.coverageReportingWithThresholds",
          "related_concepts": [
            "Code coverage metrics and thresholds",
            "v8 or istanbul coverage providers",
            "Coverage reporting formats (lcov, json, html)",
            "Coverage differential calculation for PRs",
            "Coverage badge generation and storage",
            "Coverage trend tracking over time"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_021.5",
          "description": "Configure automated production build process using tsup/esbuild with artifact generation, npm package publishing to registry, Docker image building and publishing to container registry, and GitHub release creation with changelog generation",
          "type": "sub_process",
          "parent_id": "REQ_021",
          "children": [],
          "acceptance_criteria": [
            "Build job executes 'pnpm build' command using tsup or esbuild",
            "Build generates production-ready JavaScript bundles in dist/ directory",
            "Build includes source maps for debugging production issues",
            "Build output is validated for correct entry points and exports",
            "Build artifacts are uploaded as workflow artifacts with 90-day retention",
            "npm publish step authenticates using NPM_TOKEN secret",
            "npm publish executes only on push to main branch with version tag",
            "Docker build job creates multi-stage production image",
            "Docker image is tagged with version number and 'latest' tag",
            "Docker image is pushed to GitHub Container Registry (ghcr.io)",
            "GitHub release is created automatically on version tag push",
            "Release notes are generated from conventional commit messages",
            "Release includes build artifacts (CLI binaries, npm package metadata)",
            "Smoke test validates built artifacts execute successfully"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "Build job definition in ci.yml with tsup execution",
              "tsup.config.ts or esbuild configuration for production builds",
              "package.json build script with optimization flags",
              "Dockerfile for production image creation (from Phase 7 research)",
              "Docker build and push action configuration in ci.yml",
              "npm publish workflow job with authentication setup",
              "GitHub release creation action (e.g., softprops/action-gh-release)",
              "Changelog generation script or action (e.g., conventional-changelog)",
              "Build artifact upload action for dist/ directory",
              "Smoke test script to validate CLI functionality post-build",
              "Version tagging convention documentation (v*.*.* pattern)",
              "Registry authentication configuration for npm and Docker"
            ]
          },
          "testable_properties": [],
          "function_id": "GitHubActions.buildAndArtifactPublishing",
          "related_concepts": [
            "Production build optimization and bundling",
            "Semantic versioning and release automation",
            "npm registry authentication and publishing",
            "Docker multi-stage builds for TypeScript",
            "GitHub Packages container registry",
            "Release changelog generation from commits",
            "Build artifact verification and smoke testing"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_022",
      "description": "The system must implement context entry lifecycle management with creation timestamps, TTL expiration support, parent-child relationships via parent_id, derivation tracking via derived_from array, reference tracking for entry dependencies, and automatic cleanup of expired entries",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_022.1",
          "description": "Add created_at timestamp field to all ContextEntry objects, automatically populated on creation with ISO 8601 format datetime. The timestamp must be immutable after creation and should be preserved during serialization/deserialization.",
          "type": "sub_process",
          "parent_id": "REQ_022",
          "children": [],
          "acceptance_criteria": [
            "All new ContextEntry objects must have a created_at timestamp automatically set on instantiation",
            "The created_at field must use TypeScript Date type and validate as a valid date",
            "Zod schema must enforce created_at as required field of type z.date()",
            "Serialization to JSON must convert created_at to ISO 8601 string format",
            "Deserialization from JSON must parse ISO 8601 string back to Date object",
            "The created_at timestamp must be immutable - attempts to modify should be prevented by TypeScript readonly modifier",
            "Existing entries loaded from storage must preserve their original created_at timestamp",
            "Unit tests must verify timestamp is set within 100ms of object creation",
            "Property-based tests must verify created_at is always a valid Date instance",
            "Migration script must populate created_at for any legacy entries without timestamps using current time as fallback"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Update ContextEntry interface to include created_at: Date field with readonly modifier",
              "Add created_at initialization logic in ContextEntry factory functions",
              "Implement date serialization/deserialization helpers for JSON conversion"
            ],
            "middleware": [],
            "shared": [
              "Update ContextEntrySchema Zod definition to include created_at: z.date()",
              "Add toJSON() method to ContextEntry for ISO 8601 serialization",
              "Add fromJSON() static method for parsing stored entries",
              "Create DateUtils helper for consistent date handling across the system",
              "Update ContextEntry type definition with readonly created_at property"
            ]
          },
          "testable_properties": [],
          "function_id": "ContextEntry.addCreatedAtTimestamp",
          "related_concepts": [
            "ContextEntry data model",
            "Entry serialization",
            "Immutable fields",
            "ISO 8601 datetime format",
            "TypeScript Date type",
            "Zod schema validation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_022.2",
          "description": "Implement Time-To-Live (TTL) expiration mechanism for context entries. Entries with TTL values must be automatically marked as expired when current time exceeds created_at + TTL duration. Provide methods to check expiration status and filter expired entries.",
          "type": "sub_process",
          "parent_id": "REQ_022",
          "children": [],
          "acceptance_criteria": [
            "ContextEntry must support optional ttl field as number of milliseconds",
            "Zod schema must validate ttl as optional positive integer (z.number().int().positive().optional())",
            "isExpired() method must return true when current time > created_at + ttl, false otherwise",
            "isExpired() must return false for entries without TTL set (null or undefined)",
            "CentralContextStore.get() must check expiration and return undefined for expired entries",
            "CentralContextStore.getAll() must exclude expired entries by default",
            "CentralContextStore.getAll() must support includeExpired: boolean flag to retrieve all entries",
            "CentralContextStore.getByType() must exclude expired entries unless explicitly requested",
            "Search operations must exclude expired entries from results",
            "Unit tests must verify expiration logic with various TTL values (0ms, 1000ms, 86400000ms)",
            "Edge case tests must verify behavior at exact expiration boundary (created_at + ttl === now)",
            "Performance tests must ensure expiration checks add <1ms overhead per query"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Add isExpired() method to ContextEntry class/interface",
              "Implement TTL validation in entry creation logic",
              "Add expiration filtering to all retrieval methods in CentralContextStore",
              "Create getExpiredEntries() method for cleanup scheduler access"
            ],
            "middleware": [],
            "shared": [
              "Update ContextEntrySchema to include ttl: z.number().int().positive().optional()",
              "Add TimeUtils helper with getCurrentTimestamp() and calculateExpiration() methods",
              "Define TTL_PRESETS constant object with common durations (HOUR, DAY, WEEK)",
              "Create ExpirationChecker utility class for centralized expiration logic",
              "Update ContextEntry type to include ttl?: number field"
            ]
          },
          "testable_properties": [],
          "function_id": "ContextStore.implementTTLExpiration",
          "related_concepts": [
            "TTL (Time-To-Live)",
            "Entry expiration logic",
            "Duration calculation",
            "Millisecond precision timing",
            "Lazy expiration vs eager cleanup",
            "Query filtering"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_022.3",
          "description": "Implement parent-child relationship tracking through parent_id field. Enable hierarchical context organization where child entries reference their parent entry ID. Support querying entries by parent, retrieving all children of a parent, and validating relationship integrity.",
          "type": "sub_process",
          "parent_id": "REQ_022",
          "children": [],
          "acceptance_criteria": [
            "ContextEntry must include optional parent_id field as string matching ctx_XXXXXXXX pattern",
            "Zod schema must validate parent_id format: z.string().regex(/^ctx_[a-f0-9]{8}$/).optional()",
            "CentralContextStore must provide getChildren(parentId: string) method returning all direct children",
            "CentralContextStore must provide getParent(childId: string) method returning parent entry or undefined",
            "Setting parent_id must validate that parent entry exists in the store",
            "Circular references must be prevented (entry cannot be its own ancestor)",
            "getDescendants(entryId: string) method must recursively retrieve all descendants",
            "getAncestors(entryId: string) method must recursively retrieve all ancestors",
            "Removing a parent entry must support cascade behavior options (delete children, orphan children, or prevent deletion)",
            "Unit tests must verify parent-child linking and unlinking operations",
            "Integration tests must verify multi-level hierarchies (grandparent \u2192 parent \u2192 child \u2192 grandchild)",
            "Property-based tests must verify no circular references can be created",
            "Edge case tests must handle orphaned entries (parent_id points to non-existent entry)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Add parent_id validation logic in CentralContextStore.add() method",
              "Implement getChildren() method with filtering by parent_id",
              "Implement getParent() method as wrapper around get(entry.parent_id)",
              "Create recursive getDescendants() using depth-first or breadth-first traversal",
              "Create recursive getAncestors() following parent_id chain",
              "Add circular reference detection using visited set during validation",
              "Implement cascade deletion logic with CascadeOptions enum (DELETE_CHILDREN, ORPHAN_CHILDREN, PREVENT)"
            ],
            "middleware": [],
            "shared": [
              "Update ContextEntrySchema to include parent_id: z.string().regex(/^ctx_[a-f0-9]{8}$/).optional()",
              "Create RelationshipValidator utility class for parent-child integrity checks",
              "Define CascadeOptions enum for deletion behavior",
              "Add HierarchyUtils helper with tree traversal algorithms",
              "Update ContextEntry type to include parent_id?: string field",
              "Create RelationshipError custom error class for relationship violations"
            ]
          },
          "testable_properties": [],
          "function_id": "ContextEntry.trackParentChildRelationships",
          "related_concepts": [
            "Parent-child relationships",
            "Hierarchical data structures",
            "Foreign key validation",
            "Orphaned entry detection",
            "Tree traversal",
            "Circular reference prevention"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_022.4",
          "description": "Implement derived_from array field to track entry lineage and derivation sources. Enable entries to reference multiple source entries they were derived from (e.g., SUMMARY derived from multiple FILE entries, TASK_RESULT derived from TASK and COMMAND_RESULT). Support lineage queries and impact analysis.",
          "type": "sub_process",
          "parent_id": "REQ_022",
          "children": [],
          "acceptance_criteria": [
            "ContextEntry must include derived_from field as array of entry IDs (string[])",
            "Zod schema must validate derived_from as z.array(z.string().regex(/^ctx_[a-f0-9]{8}$/)).default([])",
            "Setting derived_from must validate that all referenced entries exist in the store",
            "CentralContextStore must provide getDerivedEntries(sourceId: string) method returning all entries derived from source",
            "CentralContextStore must provide getSourceEntries(derivedId: string) method returning all source entries",
            "Lineage graph must be acyclic - circular derivations must be prevented",
            "getDerivationChain(entryId: string) method must return full lineage DAG (sources, sources of sources, etc.)",
            "getImpactScope(entryId: string) method must return all entries that depend on given entry (derived entries and their derivatives)",
            "Removing an entry that other entries are derived from must support cleanup options (cascade delete, mark orphaned, or prevent deletion)",
            "Unit tests must verify derivation linking for single and multiple sources",
            "Integration tests must verify complex derivation chains (A \u2192 B \u2192 C, A \u2192 C, D \u2192 C)",
            "Property-based tests must verify DAG property is maintained (no cycles)",
            "Performance tests must ensure lineage queries complete in O(n) time for n entries in chain"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Add derived_from validation logic in CentralContextStore.add() method",
              "Implement getDerivedEntries() method filtering by derived_from array content",
              "Implement getSourceEntries() method as batch get() operation",
              "Create getDerivationChain() using topological sort or BFS for DAG traversal",
              "Create getImpactScope() using reverse dependency traversal",
              "Add cycle detection algorithm using DFS with visited/visiting states",
              "Implement cleanup logic for removing entries with derivation dependencies"
            ],
            "middleware": [],
            "shared": [
              "Update ContextEntrySchema to include derived_from: z.array(z.string().regex(/^ctx_[a-f0-9]{8}$/)).default([])",
              "Create LineageValidator utility class for DAG validation and cycle detection",
              "Add GraphUtils helper with DAG traversal algorithms (topological sort, DFS, BFS)",
              "Define DerivationType enum for categorizing derivation relationships (SUMMARY, TRANSFORM, AGGREGATE, FILTER)",
              "Update ContextEntry type to include derived_from: string[] field",
              "Create LineageError custom error class for derivation violations",
              "Add LineageGraph class for efficient lineage query operations with memoization"
            ]
          },
          "testable_properties": [],
          "function_id": "ContextEntry.trackDerivationLineage",
          "related_concepts": [
            "Data lineage",
            "Derivation tracking",
            "Provenance",
            "Source attribution",
            "Directed acyclic graph (DAG)",
            "Impact analysis",
            "Entry dependencies"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_022.5",
          "description": "Build automatic cleanup scheduler that periodically removes expired entries based on TTL. Support configurable cleanup intervals, batch deletion, cleanup callbacks for custom logic, and manual cleanup triggers. Ensure thread-safe cleanup operations that don't interfere with active queries.",
          "type": "sub_process",
          "parent_id": "REQ_022",
          "children": [],
          "acceptance_criteria": [
            "CleanupScheduler class must run cleanup tasks at configurable intervals (default 60000ms / 1 minute)",
            "Scheduler must use setInterval for periodic execution or cron-like scheduling",
            "Cleanup operation must identify all expired entries using isExpired() check",
            "Batch deletion must remove expired entries in configurable batch sizes (default 100 entries per batch)",
            "Cleanup must respect relationship constraints (handle parent-child and derivation dependencies)",
            "Scheduler must support pause() and resume() methods for manual control",
            "Cleanup must emit events (cleanupStarted, entriesRemoved, cleanupCompleted) for monitoring",
            "Custom cleanup callbacks must be supported via registerCleanupHook(callback: CleanupHook)",
            "Manual cleanup trigger via runCleanup() method must execute immediately",
            "Scheduler must gracefully shutdown on process termination (clear intervals, finish in-progress cleanup)",
            "Concurrent cleanup operations must be prevented using mutex/lock pattern",
            "Unit tests must verify cleanup removes only expired entries",
            "Integration tests must verify scheduler runs at configured intervals",
            "Performance tests must ensure cleanup of 10,000 expired entries completes in <5 seconds",
            "Edge case tests must verify cleanup handles empty store, no expired entries, and all expired scenarios"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Create CleanupScheduler class with start(), stop(), pause(), resume() methods",
              "Implement getExpiredEntries() in CentralContextStore returning array of expired entry IDs",
              "Add batchDelete(entryIds: string[]) method for efficient bulk removal",
              "Implement cleanup loop with batch processing and delay between batches",
              "Add mutex lock using async-mutex or custom Promise-based lock",
              "Implement event emitter for cleanup lifecycle events",
              "Create cleanup hook registration system with priority ordering",
              "Add graceful shutdown handler using process signals (SIGTERM, SIGINT)",
              "Implement cleanup statistics tracking (entries removed, time taken, errors)"
            ],
            "middleware": [],
            "shared": [
              "Create CleanupScheduler class with configuration interface (interval, batchSize, enabled)",
              "Define CleanupHook type as async callback function: (entries: ContextEntry[]) => Promise<void>",
              "Create CleanupConfig interface with interval, batchSize, enableLogging, hooks fields",
              "Add CleanupStats interface tracking totalRemoved, lastRunTime, averageRunTime, errorCount",
              "Define CleanupEvent enum (STARTED, BATCH_REMOVED, COMPLETED, ERROR)",
              "Create EventEmitter wrapper for type-safe cleanup events",
              "Add SchedulerError custom error class for scheduler failures",
              "Implement ExponentialBackoff utility for retry logic on cleanup failures"
            ]
          },
          "testable_properties": [],
          "function_id": "ContextStore.automaticCleanupScheduler",
          "related_concepts": [
            "Scheduled tasks",
            "Background workers",
            "Batch deletion",
            "Resource cleanup",
            "Node.js timers (setInterval)",
            "Async task queues",
            "Cleanup hooks",
            "Graceful shutdown"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_023",
      "description": "The system must implement BAML integration using @baml/client TypeScript SDK with sync/async clients, type builders for structured outputs, runtime configuration, prompt template management, error handling for API failures, and retry logic with exponential backoff",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_023.1",
          "description": "Install and integrate @baml/client TypeScript SDK with proper type definitions and dependency management",
          "type": "sub_process",
          "parent_id": "REQ_023",
          "children": [],
          "acceptance_criteria": [
            "@baml/client package is added to package.json with version ^0.216.0 or compatible",
            "TypeScript type definitions are available and imported without errors",
            "SDK can be imported successfully in both ESM and CommonJS contexts",
            "No peer dependency warnings or conflicts exist",
            "Package installation completes successfully with pnpm install",
            "TypeScript compiler recognizes all exported types from @baml/client",
            "Development environment can resolve SDK imports without path errors"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Install @baml/client package via pnpm",
              "Create baml client initialization module",
              "Verify SDK exports are accessible"
            ],
            "middleware": [],
            "shared": [
              "Define BAMLClientConfig interface for SDK configuration",
              "Create type definitions for BAML client options",
              "Export SDK types for use across modules"
            ]
          },
          "testable_properties": [],
          "function_id": "BAMLClient.sdkIntegration",
          "related_concepts": [
            "package management",
            "dependency resolution",
            "SDK initialization",
            "TypeScript types"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_023.2",
          "description": "Configure both synchronous and asynchronous BAML client instances with proper runtime configuration, API keys, and environment-specific settings",
          "type": "sub_process",
          "parent_id": "REQ_023",
          "children": [],
          "acceptance_criteria": [
            "Synchronous BAML client instance is created and exported from src/baml/client.ts",
            "Asynchronous BAML client instance is created and exported from src/baml/client.ts",
            "API key is loaded from environment variable ANTHROPIC_API_KEY or BAML_API_KEY",
            "Runtime configuration includes model selection (claude-3-5-sonnet-20241022 or claude-opus-4-20250514)",
            "Client instances support custom timeout configuration (default 120000ms)",
            "Client instances support custom base URL configuration for API endpoint overrides",
            "Both clients share common configuration but maintain separate execution contexts",
            "Configuration validation throws clear errors for missing or invalid API keys",
            "Client instances are lazy-loaded to prevent initialization errors at import time",
            "Documentation exists for configuring client instances in different environments"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Create BAMLClientFactory class with createSync() and createAsync() methods",
              "Implement environment variable loading with dotenv",
              "Add configuration validation logic",
              "Implement singleton pattern for client instances",
              "Create client initialization error handling"
            ],
            "middleware": [
              "Add API key validation middleware",
              "Implement request timeout middleware"
            ],
            "shared": [
              "Define BAMLClientConfig interface with apiKey, model, timeout, baseUrl properties",
              "Create BAMLClientOptions type for optional configuration overrides",
              "Define BAMLRuntimeConfig interface for runtime settings",
              "Create validateConfig() utility function",
              "Define default configuration constants (DEFAULT_MODEL, DEFAULT_TIMEOUT, DEFAULT_BASE_URL)"
            ]
          },
          "testable_properties": [],
          "function_id": "BAMLClient.clientConfiguration",
          "related_concepts": [
            "client initialization",
            "runtime configuration",
            "environment variables",
            "async/await patterns",
            "singleton pattern"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_023.3",
          "description": "Port all BAML function definitions from Python baml_src/ directory to TypeScript, maintaining prompt structures, input/output schemas, and type safety",
          "type": "sub_process",
          "parent_id": "REQ_023",
          "children": [],
          "acceptance_criteria": [
            "All .baml files from Python baml_src/ are ported to TypeScript baml_src/ directory",
            "DecomposeRequirements BAML function is ported with correct input schema (feature_description, tech_stack, project_context)",
            "DecomposeRequirements output schema matches RequirementHierarchy TypeScript type",
            "ExtractTechStack BAML function is ported with file content input and TechStack output",
            "AnalyzeFileGroups BAML function is ported for codebase analysis",
            "GenerateProperties BAML function is ported for property-based test generation",
            "All prompt templates preserve original wording and structure from Python version",
            "BAML type definitions use TypeScript-compatible type syntax (string, int, bool, array, object)",
            "Generated TypeScript types from BAML match manually defined types in src/planning/models.ts",
            "BAML function calls compile without syntax errors using BAML CLI",
            "Type builders are generated automatically via baml-cli generate command",
            "Integration tests verify BAML functions return expected TypeScript types"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Create baml_src/ directory structure matching Python layout",
              "Port decompose_requirements.baml with prompt template",
              "Port extract_tech_stack.baml for tech stack detection",
              "Port analyze_file_groups.baml for codebase analysis",
              "Port generate_properties.baml for test generation",
              "Add BAML CLI to package.json scripts for code generation",
              "Create generate-types npm script to run baml-cli generate",
              "Implement BAML function wrapper classes (DecomposeRequirementsClient, ExtractTechStackClient)"
            ],
            "middleware": [],
            "shared": [
              "Define RequirementHierarchy interface matching BAML output",
              "Define TechStack interface for technology detection",
              "Define FileGroup interface for codebase analysis results",
              "Define TestableProperty interface for property-based tests",
              "Create BAML input/output type mapping utilities",
              "Define BAMLFunctionSchema interface for runtime validation",
              "Create type guard functions (isRequirementHierarchy, isTechStack)"
            ]
          },
          "testable_properties": [],
          "function_id": "BAMLClient.functionDefinitions",
          "related_concepts": [
            "BAML syntax",
            "prompt engineering",
            "schema migration",
            "type generation",
            "code generation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_023.4",
          "description": "Implement comprehensive error handling for BAML API failures including network errors, rate limits, invalid responses, schema validation errors, and timeout handling with exponential backoff retry logic",
          "type": "sub_process",
          "parent_id": "REQ_023",
          "children": [],
          "acceptance_criteria": [
            "Custom BAMLError class extends Error with error codes and metadata",
            "Error codes defined for API_TIMEOUT, NETWORK_ERROR, RATE_LIMIT, INVALID_RESPONSE, SCHEMA_VALIDATION_ERROR, AUTHENTICATION_ERROR",
            "Retry logic implements exponential backoff with configurable max retries (default 3)",
            "Backoff delays follow pattern: attempt 1 = 1000ms, attempt 2 = 2000ms, attempt 3 = 4000ms",
            "Rate limit errors (429) trigger longer backoff (60000ms) before retry",
            "Authentication errors (401, 403) do not trigger retries and fail immediately",
            "Schema validation errors include detailed mismatch information (expected vs actual)",
            "Network errors distinguish between timeout, connection refused, and DNS failures",
            "All BAML client calls are wrapped in try-catch with error classification",
            "Error handling preserves original error stack traces for debugging",
            "Transient errors (network, timeout, rate limit) are retried, permanent errors fail fast",
            "Circuit breaker opens after 5 consecutive failures and stays open for 60000ms",
            "Logging includes error type, retry attempt number, and backoff duration",
            "Error responses include user-friendly messages and technical details separately"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Create BAMLError class hierarchy (BAMLAPIError, BAMLNetworkError, BAMLValidationError, BAMLRateLimitError, BAMLAuthenticationError)",
              "Implement retry wrapper function withRetry(fn, options)",
              "Create exponential backoff calculator calculateBackoff(attempt, baseDelay)",
              "Implement circuit breaker class CircuitBreaker with open/closed/half-open states",
              "Add error classification logic classifyError(error) returning error type and retry eligibility",
              "Create error logging utility logBAMLError(error, context)",
              "Implement retry exhausted handler onRetryExhausted(error, attempts)",
              "Add jitter to backoff delays to prevent thundering herd (\u00b120% random variance)"
            ],
            "middleware": [
              "Create error response transformer middleware",
              "Add retry attempt header injection middleware",
              "Implement error logging middleware"
            ],
            "shared": [
              "Define BAMLErrorCode enum with all error codes",
              "Define RetryOptions interface (maxRetries, baseDelay, maxDelay, retryableErrors)",
              "Define CircuitBreakerConfig interface (failureThreshold, resetTimeout)",
              "Create isRetryableError(error) type guard function",
              "Define ErrorMetadata interface (errorCode, message, retryable, statusCode, originalError)",
              "Create formatErrorMessage(error) utility function",
              "Define DEFAULT_RETRY_OPTIONS constant",
              "Create exponentialBackoff(attempt, baseDelay, maxDelay) utility function"
            ]
          },
          "testable_properties": [],
          "function_id": "BAMLClient.errorHandling",
          "related_concepts": [
            "error handling",
            "retry logic",
            "exponential backoff",
            "circuit breaker",
            "error classification",
            "observability"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_023.5",
          "description": "Implement a prompt template management system for organizing, versioning, loading, and validating BAML prompt templates with support for variable interpolation and template composition",
          "type": "sub_process",
          "parent_id": "REQ_023",
          "children": [],
          "acceptance_criteria": [
            "Prompt templates are stored in baml_src/prompts/ directory with .baml extension",
            "Template management system supports loading templates by name and version",
            "Variable interpolation supports {{variable}} syntax for dynamic content injection",
            "Template validation checks for required variables and type constraints",
            "Templates support composition (base templates + overrides) for reusability",
            "Template versioning uses semantic versioning (v1.0.0, v1.1.0, v2.0.0)",
            "Default version resolution uses latest stable version if not specified",
            "Template registry maintains mapping of template names to file paths and versions",
            "Template compilation validates BAML syntax before execution",
            "Template metadata includes author, description, required variables, and creation date",
            "Template loading is cached to avoid repeated file I/O operations",
            "Hot-reloading support in development mode for template changes",
            "Template validation throws TemplateValidationError with specific error details (missing variables, syntax errors)",
            "Support for template includes/imports for shared prompt components"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Create PromptTemplateManager class with load(), validate(), compile() methods",
              "Implement template file discovery and indexing on initialization",
              "Create template variable extractor parseTemplateVariables(template)",
              "Implement variable interpolation engine interpolate(template, variables)",
              "Add template composition logic compose(baseTemplate, overrides)",
              "Create template version resolver resolveVersion(name, version)",
              "Implement template caching with LRU cache (max 100 templates)",
              "Add hot-reload file watcher for development mode",
              "Create template metadata parser from BAML comment headers"
            ],
            "middleware": [
              "Add template validation middleware for BAML requests",
              "Implement template variable injection middleware"
            ],
            "shared": [
              "Define PromptTemplate interface (name, version, content, metadata, requiredVariables)",
              "Define TemplateMetadata interface (author, description, createdAt, updatedAt, requiredVariables)",
              "Define TemplateVersion type as semver string",
              "Create TemplateValidationError class extending Error",
              "Define TemplateRegistry type as Map<string, PromptTemplate[]>",
              "Create validateTemplateVariables(template, providedVariables) utility function",
              "Define TemplateCompositionOptions interface (overrideStrategy: 'merge' | 'replace')",
              "Create parseTemplateMetadata(content) utility function for extracting metadata from comments"
            ]
          },
          "testable_properties": [],
          "function_id": "BAMLClient.promptTemplateManagement",
          "related_concepts": [
            "template engine",
            "prompt engineering",
            "version control",
            "variable substitution",
            "template validation",
            "prompt composition"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_024",
      "description": "The system must implement feature complexity detection in orchestrator with analysis of feature scope and dependencies, automatic model selection (sonnet for simple, opus for complex), context size estimation based on related entries, dynamic token budget allocation, and complexity scoring algorithm",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_024.1",
          "description": "Create a feature complexity analyzer that evaluates feature scope, dependencies, file impact, and technical difficulty to produce a complexity score",
          "type": "sub_process",
          "parent_id": "REQ_024",
          "children": [],
          "acceptance_criteria": [
            "Analyzer accepts feature description and context entries as input",
            "Analyzer identifies all files mentioned or referenced in the feature",
            "Analyzer detects cross-cutting concerns and architectural changes",
            "Analyzer counts direct and transitive dependencies",
            "Analyzer estimates lines of code to be modified based on similar features",
            "Analyzer produces a numerical complexity score (0-100 scale)",
            "Analyzer provides breakdown of complexity factors (scope, dependencies, technical difficulty)",
            "Analyzer handles edge cases: empty features, malformed input, missing context",
            "Analyzer performance completes within 500ms for typical features",
            "Unit tests cover all scoring factors independently",
            "Property-based tests verify score is monotonic with feature size"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "FeatureComplexityAnalyzer service class with analyzeFeature method",
              "Dependency graph builder to traverse feature references",
              "File impact estimator using context entries",
              "Technical difficulty classifier based on keywords and patterns",
              "Complexity score aggregation algorithm"
            ],
            "middleware": [],
            "shared": [
              "ComplexityAnalysisResult interface with score, breakdown, and factors",
              "FeatureScope type definition (simple, moderate, complex, very_complex)",
              "DependencyGraph data structure",
              "ComplexityFactors interface with weights for each factor"
            ]
          },
          "testable_properties": [],
          "function_id": "FeatureComplexityAnalyzer.analyzeFeature",
          "related_concepts": [
            "complexity scoring algorithm",
            "dependency analysis",
            "scope detection",
            "file impact assessment",
            "technical difficulty estimation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_024.2",
          "description": "Implement automatic model selection logic that chooses between sonnet and opus based on complexity score, with configurable thresholds and override capability",
          "type": "sub_process",
          "parent_id": "REQ_024",
          "children": [],
          "acceptance_criteria": [
            "Selector accepts complexity score and optional user override as input",
            "Selector returns 'sonnet' for complexity scores below threshold (default: 40)",
            "Selector returns 'opus' for complexity scores at or above threshold",
            "Selector respects user-specified model override when provided",
            "Selector logs reasoning for model selection decision",
            "Threshold is configurable via environment variable or config file",
            "Selector provides cost estimation for selected model based on estimated tokens",
            "Selector handles boundary cases: score = 0, score = 100, score = threshold",
            "Unit tests verify all threshold conditions",
            "Integration tests verify model selection affects actual orchestrator calls"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "ModelSelector service class with selectModel method",
              "Threshold configuration loader from environment/config",
              "Cost estimator for sonnet vs opus",
              "Selection reasoning logger",
              "Model selection validation logic"
            ],
            "middleware": [],
            "shared": [
              "ModelType enum ('sonnet', 'opus')",
              "ModelSelectionConfig interface with threshold and override options",
              "ModelSelectionResult interface with selected model, reasoning, and cost estimate",
              "DEFAULT_COMPLEXITY_THRESHOLD constant (40)"
            ]
          },
          "testable_properties": [],
          "function_id": "ModelSelector.selectModel",
          "related_concepts": [
            "model selection strategy",
            "threshold configuration",
            "cost optimization",
            "performance vs accuracy tradeoff",
            "user overrides"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_024.3",
          "description": "Add context size estimation algorithm that calculates total tokens needed based on related context entries, file contents, and feature description",
          "type": "sub_process",
          "parent_id": "REQ_024",
          "children": [],
          "acceptance_criteria": [
            "Estimator accepts feature and list of related context entry IDs as input",
            "Estimator retrieves all referenced context entries from CentralContextStore",
            "Estimator counts tokens in feature description using tokenizer",
            "Estimator counts tokens in all related entry summaries (for working LLM context)",
            "Estimator counts tokens in all related entry full content (for implementation LLM context)",
            "Estimator accounts for prompt template overhead (system messages, formatting)",
            "Estimator returns separate estimates for working context and implementation context",
            "Estimator handles compressed entries correctly (summary only, no content)",
            "Estimator handles missing entries gracefully (logs warning, continues)",
            "Estimator uses same tokenizer as Claude API (cl100k_base or equivalent)",
            "Unit tests verify token counting accuracy within 5% margin",
            "Integration tests compare estimates against actual API token usage"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "ContextSizeEstimator service class with estimateSize method",
              "Tokenizer integration (tiktoken or similar for Claude)",
              "Context entry retrieval from CentralContextStore",
              "Prompt template overhead calculator",
              "Working vs implementation context separator"
            ],
            "middleware": [],
            "shared": [
              "ContextSizeEstimate interface with workingTokens, implementationTokens, totalTokens",
              "TokenizerConfig with model-specific settings",
              "PROMPT_OVERHEAD_TOKENS constant for template boilerplate",
              "TokenCountResult interface with breakdown by entry type"
            ]
          },
          "testable_properties": [],
          "function_id": "ContextSizeEstimator.estimateSize",
          "related_concepts": [
            "token counting",
            "context entry aggregation",
            "file content size estimation",
            "working vs implementation context distinction",
            "tokenizer integration"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_024.4",
          "description": "Build token budget allocation system that dynamically distributes available tokens across feature description, context entries, and response buffer based on model limits",
          "type": "sub_process",
          "parent_id": "REQ_024",
          "children": [],
          "acceptance_criteria": [
            "Allocator accepts model type, context size estimate, and available token limit as input",
            "Allocator reserves minimum response buffer (default: 4000 tokens for opus, 2000 for sonnet)",
            "Allocator prioritizes feature description (always included, up to 2000 tokens)",
            "Allocator prioritizes high-relevance context entries based on search scores",
            "Allocator truncates or excludes low-priority entries when budget exceeded",
            "Allocator maintains minimum context quality threshold (at least top 10 entries)",
            "Allocator returns allocation plan with included/excluded entry IDs and token counts",
            "Allocator handles edge case: budget too small for minimum context (raises error)",
            "Allocator logs warnings when context must be truncated",
            "Unit tests verify budget never exceeds model limits",
            "Property-based tests verify allocation is stable and deterministic"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "TokenBudgetAllocator service class with allocateBudget method",
              "Entry prioritization algorithm based on relevance scores",
              "Budget distribution calculator",
              "Context truncation logic with quality thresholds",
              "Allocation plan builder"
            ],
            "middleware": [],
            "shared": [
              "TokenBudgetAllocation interface with includedEntries, excludedEntries, tokenBreakdown",
              "ModelTokenLimits constant map (sonnet: 200k, opus: 200k)",
              "RESPONSE_BUFFER_TOKENS constant map per model",
              "MIN_CONTEXT_ENTRIES constant (10)",
              "AllocationPriority enum (critical, high, medium, low)"
            ]
          },
          "testable_properties": [],
          "function_id": "TokenBudgetAllocator.allocateBudget",
          "related_concepts": [
            "token budget management",
            "priority-based allocation",
            "context window limits",
            "response buffer reservation",
            "entry prioritization"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_024.5",
          "description": "Develop complexity scoring algorithm with multiple weighted factors including file count, dependency depth, architectural impact, test coverage requirements, and cross-cutting concerns",
          "type": "sub_process",
          "parent_id": "REQ_024",
          "children": [],
          "acceptance_criteria": [
            "Scorer accepts feature analysis data as input (files, dependencies, keywords)",
            "Scorer calculates file count factor: 0-20 points (1-3 files=5, 4-10=10, 11-20=15, 20+=20)",
            "Scorer calculates dependency depth factor: 0-20 points (depth 0-1=5, 2-3=10, 4-5=15, 6+=20)",
            "Scorer calculates architectural impact factor: 0-25 points based on keywords (migration, refactor, architecture, breaking change)",
            "Scorer calculates test complexity factor: 0-20 points based on testing requirements (unit=5, integration=10, e2e=15, property-based=20)",
            "Scorer calculates cross-cutting concern factor: 0-15 points (authentication, logging, error handling, performance)",
            "Scorer applies configurable weights to each factor (default: file=1.0, dependency=1.2, architectural=1.5, test=1.0, cross-cutting=1.3)",
            "Scorer normalizes final score to 0-100 range",
            "Scorer provides detailed breakdown of each factor contribution",
            "Scorer handles missing factors gracefully (assigns default safe value)",
            "Unit tests verify each factor calculation independently",
            "Integration tests verify score correlates with actual implementation effort",
            "Configuration allows customizing factor weights via config file"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "ComplexityScorer service class with calculateScore method",
              "File count analyzer",
              "Dependency depth calculator using graph traversal",
              "Architectural keyword detector with pattern matching",
              "Test complexity estimator based on requirement analysis",
              "Cross-cutting concern identifier using NLP or keyword matching",
              "Weighted score aggregator with normalization"
            ],
            "middleware": [],
            "shared": [
              "ComplexityScore interface with totalScore, factorBreakdown, normalizedScore",
              "ScoringFactors interface with fileCount, dependencyDepth, architecturalImpact, testComplexity, crossCuttingConcerns",
              "FactorWeights interface with configurable weight per factor",
              "DEFAULT_FACTOR_WEIGHTS constant object",
              "ARCHITECTURAL_KEYWORDS constant array",
              "CROSS_CUTTING_KEYWORDS constant array"
            ]
          },
          "testable_properties": [],
          "function_id": "ComplexityScorer.calculateScore",
          "related_concepts": [
            "multi-factor scoring",
            "weighted aggregation",
            "architectural impact detection",
            "test complexity estimation",
            "cross-cutting concern identification"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_025",
      "description": "The system must implement visualization capabilities with Mermaid diagram generation for requirement hierarchies, dependency graph rendering, phase execution timelines, context entry relationship diagrams, and exportable SVG/PNG formats with interactive navigation",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_025.1",
          "description": "Create Mermaid diagram generator for requirements",
          "type": "sub_process",
          "parent_id": "REQ_025",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.create",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_025.2",
          "description": "Implement dependency graph visualization",
          "type": "sub_process",
          "parent_id": "REQ_025",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.implement",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_025.3",
          "description": "Add timeline rendering for phases",
          "type": "sub_process",
          "parent_id": "REQ_025",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.render",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_025.4",
          "description": "Build context relationship diagrams",
          "type": "sub_process",
          "parent_id": "REQ_025",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.build",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_025.5",
          "description": "Add export functionality for multiple formats",
          "type": "sub_process",
          "parent_id": "REQ_025",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.add",
          "related_concepts": [],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_026",
      "description": "The system must implement git integration with automatic commit creation on feature completion, commit message generation following repository conventions, branch detection and management, git status checking before operations, pre-commit hook support, and safety checks to prevent destructive operations",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_026.1",
          "description": "Implement automatic git commit creation when a feature implementation completes successfully, including validation of working directory state, staging of modified files, and execution of commit operation",
          "type": "sub_process",
          "parent_id": "REQ_026",
          "children": [],
          "acceptance_criteria": [
            "System detects feature completion events through session result monitoring",
            "All modified files related to the feature are identified before commit",
            "Working directory is validated to be in a committable state (no conflicts, no unrelated changes)",
            "Modified files are staged using 'git add' for tracked files only",
            "Commit operation is executed with generated message only after successful staging",
            "Commit success/failure status is captured and logged",
            "System handles partial staging scenarios (feature files only, not unrelated changes)",
            "Pre-commit hooks are allowed to run and their exit codes are respected",
            "Commit operation is skipped if no changes are detected",
            "Commit operation fails gracefully if git repository is not initialized",
            "System provides rollback capability if commit fails after staging"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "GitService.detectFeatureCompletion() method to monitor session results",
              "GitService.validateWorkingDirectory() method to check git status",
              "GitService.identifyFeatureFiles() method to determine which files to stage",
              "GitService.stageFiles() method to execute 'git add' commands",
              "GitService.executeCommit() method to perform commit operation",
              "GitService.handleCommitFailure() method for rollback logic",
              "Error handling for git command failures"
            ],
            "middleware": [
              "Feature completion event listener",
              "Git command execution wrapper with error handling",
              "Pre-commit hook execution manager"
            ],
            "shared": [
              "CommitResult interface with success, commitHash, error fields",
              "FeatureCompletionEvent interface with featureId, modifiedFiles, timestamp",
              "GitStatus type definition for working directory states",
              "GitError custom error class for git operation failures"
            ]
          },
          "testable_properties": [],
          "function_id": "GitService.autoCommitOnSuccess",
          "related_concepts": [
            "git staging area",
            "working directory status",
            "commit hooks",
            "feature completion detection",
            "file change tracking",
            "commit transaction safety"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_026.2",
          "description": "Create a commit message generator that analyzes repository conventions, feature changes, and test results to produce properly formatted commit messages following the project's established patterns",
          "type": "sub_process",
          "parent_id": "REQ_026",
          "children": [],
          "acceptance_criteria": [
            "System analyzes recent commits using 'git log' to detect message patterns (conventional commits, semantic, custom)",
            "Commit type is automatically determined from feature changes (feat, fix, docs, refactor, test, etc.)",
            "Commit scope is extracted from feature metadata or file paths",
            "Commit subject line is generated with concise description of changes (max 72 characters)",
            "Commit body includes detailed change summary when modifications are complex",
            "Breaking changes are detected and properly formatted with BREAKING CHANGE footer",
            "Co-authored-by footer is added for Claude Code attribution",
            "Generated with Claude Code footer is appended to all messages",
            "Test results are incorporated into commit message when available",
            "Generated message follows detected repository convention or defaults to conventional commits",
            "Message includes related issue/feature IDs when available",
            "Generator handles multi-file changes with unified message theme"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "CommitMessageGenerator.analyzeConventions() method to parse git log",
              "CommitMessageGenerator.detectCommitType() method to determine feat/fix/etc",
              "CommitMessageGenerator.extractScope() method to identify scope from feature",
              "CommitMessageGenerator.generateSubject() method to create concise summary",
              "CommitMessageGenerator.generateBody() method to create detailed description",
              "CommitMessageGenerator.formatFooters() method to add attribution and metadata",
              "CommitMessageGenerator.validateMessage() method to check format compliance",
              "ConventionDetector.detectPattern() method to identify repository style"
            ],
            "middleware": [
              "Git log parser for convention analysis",
              "Commit message validator against detected conventions",
              "Character limit enforcer for subject lines"
            ],
            "shared": [
              "CommitMessageTemplate interface with subject, body, footers, type, scope",
              "CommitConvention type enum (conventional, semantic, custom)",
              "CommitType enum (feat, fix, docs, style, refactor, test, chore)",
              "MessageGenerationConfig interface for customization options",
              "ConventionPattern interface for detected pattern rules"
            ]
          },
          "testable_properties": [],
          "function_id": "CommitMessageGenerator.generateMessage",
          "related_concepts": [
            "conventional commits",
            "commit message parsing",
            "git log analysis",
            "semantic versioning",
            "change summarization",
            "repository convention detection",
            "commit footer generation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_026.3",
          "description": "Implement branch detection to identify current branch, validate branch state, and ensure operations are safe to perform on the current branch with protection for main/master branches",
          "type": "sub_process",
          "parent_id": "REQ_026",
          "children": [],
          "acceptance_criteria": [
            "System detects current branch using 'git rev-parse --abbrev-ref HEAD'",
            "Main/master branch is identified through configuration or convention",
            "Branch state is validated (not detached HEAD, not in rebase/merge state)",
            "Remote tracking status is checked for current branch",
            "System prevents automatic commits to main/master unless explicitly configured",
            "Branch divergence from remote is detected and reported",
            "Feature branch naming convention is validated if configured",
            "User is warned when committing to protected branches",
            "System checks if branch is ahead/behind remote before operations",
            "Branch validation includes checking for uncommitted changes from other features",
            "Detached HEAD state is detected and handled appropriately"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "BranchManager.getCurrentBranch() method to detect active branch",
              "BranchManager.isProtectedBranch() method to check main/master status",
              "BranchManager.validateBranchState() method to check for detached HEAD, rebase, merge states",
              "BranchManager.checkRemoteTracking() method to verify remote status",
              "BranchManager.checkBranchDivergence() method to compare with remote",
              "BranchManager.validateBranchName() method to check naming conventions",
              "BranchManager.getProtectedBranches() method to retrieve protected branch list"
            ],
            "middleware": [
              "Branch protection validator before commit operations",
              "Branch state checker before git operations",
              "User warning system for protected branch operations"
            ],
            "shared": [
              "BranchInfo interface with name, isProtected, isDetached, remoteTracking, divergence fields",
              "BranchState enum (normal, detached, rebasing, merging, conflicted)",
              "BranchValidationResult interface with isValid, warnings, errors fields",
              "BranchProtectionConfig interface for protected branch configuration",
              "BranchDivergence interface with ahead, behind, upToDate fields"
            ]
          },
          "testable_properties": [],
          "function_id": "BranchManager.detectAndValidate",
          "related_concepts": [
            "branch protection rules",
            "main branch detection",
            "branch naming conventions",
            "remote tracking branches",
            "branch state validation",
            "feature branch workflows"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_026.4",
          "description": "Implement comprehensive git status checking before any git operations to ensure repository is in a valid state, detect conflicts, identify untracked files, and verify working directory cleanliness",
          "type": "sub_process",
          "parent_id": "REQ_026",
          "children": [],
          "acceptance_criteria": [
            "System executes 'git status --porcelain' before all git operations",
            "Merge conflicts are detected and prevent automatic operations",
            "Untracked files are identified and categorized (feature-related vs unrelated)",
            "Modified files are distinguished from staged files",
            "Deleted files are detected and reported",
            "Renamed/moved files are identified correctly",
            "Repository state is checked for ongoing operations (rebase, merge, cherry-pick)",
            "Submodule status is checked if submodules are present",
            "Status check failures are logged with detailed error messages",
            "System provides summary of working directory changes before operations",
            "Status checks timeout after configurable duration to prevent hanging"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "GitStatusChecker.getStatus() method to execute git status --porcelain",
              "GitStatusChecker.parseStatusOutput() method to parse porcelain format",
              "GitStatusChecker.detectConflicts() method to identify merge conflicts",
              "GitStatusChecker.categorizeFiles() method to group by status (modified, staged, untracked, deleted)",
              "GitStatusChecker.checkOngoingOperations() method to detect rebase/merge states",
              "GitStatusChecker.checkSubmodules() method to validate submodule status",
              "GitStatusChecker.generateSummary() method to create human-readable status report"
            ],
            "middleware": [
              "Pre-operation status validator",
              "Git command timeout manager",
              "Status check error handler"
            ],
            "shared": [
              "GitStatusResult interface with modified, staged, untracked, deleted, renamed, conflicted file arrays",
              "FileStatus enum (modified, staged, untracked, deleted, renamed, conflicted, ignored)",
              "RepositoryState enum (clean, dirty, conflicted, rebasing, merging, cherryPicking)",
              "StatusCheckConfig interface with timeout, includeSubmodules options",
              "StatusSummary interface with totalChanges, conflicts, warnings fields"
            ]
          },
          "testable_properties": [],
          "function_id": "GitStatusChecker.preOperationValidation",
          "related_concepts": [
            "working directory status",
            "staging area state",
            "merge conflicts",
            "untracked files",
            "git porcelain status",
            "repository health checks"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_026.5",
          "description": "Implement comprehensive safety validations to prevent destructive git operations including force push prevention, hard reset protection, branch deletion safeguards, and user confirmation for potentially dangerous commands",
          "type": "sub_process",
          "parent_id": "REQ_026",
          "children": [],
          "acceptance_criteria": [
            "Force push operations (--force, --force-with-lease) are blocked on main/master branches",
            "Hard reset operations (git reset --hard) require explicit user confirmation",
            "Branch deletion is prevented for main/master and protected branches",
            "System maintains whitelist of safe git operations for automation",
            "Destructive operations require user confirmation with detailed impact description",
            "Force push to feature branches shows warning but can proceed with confirmation",
            "Git commands are validated before execution against safety rules",
            "System prevents operations that would lose uncommitted work",
            "Stash operations are suggested before destructive operations",
            "Safety checks can be overridden with explicit --force flag (with multiple confirmations)",
            "All blocked operations are logged with reason and timestamp",
            "User receives clear explanation of why operation was blocked"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "GitSafetyValidator.validateCommand() method to check command against safety rules",
              "GitSafetyValidator.isDestructive() method to identify destructive operations",
              "GitSafetyValidator.requiresConfirmation() method to determine if user confirmation needed",
              "GitSafetyValidator.checkForcePush() method to validate push --force operations",
              "GitSafetyValidator.checkHardReset() method to validate reset --hard operations",
              "GitSafetyValidator.checkBranchDeletion() method to validate branch delete operations",
              "GitSafetyValidator.suggestSafeAlternative() method to recommend safer approaches",
              "SafetyRuleEngine.evaluateRules() method to apply safety rule set"
            ],
            "middleware": [
              "Git command interceptor for safety validation",
              "User confirmation prompt manager",
              "Safety violation logger"
            ],
            "shared": [
              "SafetyValidationResult interface with isAllowed, requiresConfirmation, warnings, suggestedAlternative fields",
              "DestructiveOperation enum (forcePush, hardReset, branchDelete, amendPushed, rebasePublic)",
              "SafetyRule interface with operation, condition, action, message fields",
              "SafetyConfig interface with protectedBranches, allowedOperations, confirmationRequired options",
              "ConfirmationPrompt interface with message, severity, defaultResponse fields"
            ]
          },
          "testable_properties": [],
          "function_id": "GitSafetyValidator.validateOperation",
          "related_concepts": [
            "destructive git operations",
            "force push protection",
            "branch protection",
            "hard reset safety",
            "user confirmation prompts",
            "operation whitelisting",
            "git command filtering"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_027",
      "description": "The system must implement distributed execution support with session state persistence to SQLite, session deduplication based on context hash, resumable sessions after interruption, concurrent session handling with locking, and session history tracking with metadata",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_027.1",
          "description": "Implement SQLite persistence layer",
          "type": "sub_process",
          "parent_id": "REQ_027",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.implement",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_027.2",
          "description": "Add session deduplication algorithm",
          "type": "sub_process",
          "parent_id": "REQ_027",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.add",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_027.3",
          "description": "Create session resume functionality",
          "type": "sub_process",
          "parent_id": "REQ_027",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.create",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_027.4",
          "description": "Build concurrent session locking mechanism",
          "type": "sub_process",
          "parent_id": "REQ_027",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.build",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_027.5",
          "description": "Implement session history with metadata storage",
          "type": "sub_process",
          "parent_id": "REQ_027",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Data.implement",
          "related_concepts": [],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_028",
      "description": "The system must support incremental migration from Python with module-by-module porting strategy, parallel execution of Python and TypeScript versions during transition, shared test suite for behavioral validation, feature flag system for gradual rollout, and deprecation timeline documentation",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_028.1",
          "description": "Create module-by-module migration plan",
          "type": "sub_process",
          "parent_id": "REQ_028",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.create",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_028.2",
          "description": "Set up parallel execution infrastructure",
          "type": "sub_process",
          "parent_id": "REQ_028",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.set",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_028.3",
          "description": "Port test suites with validation",
          "type": "sub_process",
          "parent_id": "REQ_028",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Validator.port",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_028.4",
          "description": "Implement feature flag system",
          "type": "sub_process",
          "parent_id": "REQ_028",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.implement",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_028.5",
          "description": "Document deprecation timeline and migration path",
          "type": "sub_process",
          "parent_id": "REQ_028",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.document",
          "related_concepts": [],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_029",
      "description": "The system must implement CLI distribution as npm package with @anthropic-ai/silmari-context-engine package name, semantic versioning following semver, npm/pnpm installation support, global CLI installation with 'silmari' command, and optional standalone binary generation using pkg or bun for systems without Node.js",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_029.1",
          "description": "Configure package.json with bin entry for global CLI installation, mapping 'silmari' command to compiled TypeScript entry point",
          "type": "sub_process",
          "parent_id": "REQ_029",
          "children": [],
          "acceptance_criteria": [
            "package.json contains 'bin' field mapping 'silmari' to './dist/cli.js'",
            "dist/cli.js starts with shebang line '#!/usr/bin/env node'",
            "dist/cli.js has executable permissions (chmod +x) in build process",
            "npm link command successfully creates global 'silmari' command",
            "silmari --version returns correct version from package.json",
            "silmari --help displays command usage information",
            "Works on Linux, macOS, and Windows (via npm's automatic .cmd wrapper)",
            "Build script ensures dist/cli.js is compiled from src/cli.ts before packaging",
            "CLI entry point imports and executes commander program parsing"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Create src/cli.ts as main CLI entry point with commander setup",
              "Add shebang '#!/usr/bin/env node' to src/cli.ts",
              "Configure tsup/esbuild to preserve shebang in dist/cli.js output",
              "Add chmod +x dist/cli.js to build script"
            ],
            "middleware": [],
            "shared": [
              "Update package.json with bin field: { \"silmari\": \"./dist/cli.js\" }",
              "Add 'files' field to package.json to include dist/ directory",
              "Create build script that compiles TypeScript and sets permissions",
              "Add prepublishOnly script to ensure build runs before npm publish"
            ]
          },
          "testable_properties": [],
          "function_id": "PackageConfig.configureBinEntry",
          "related_concepts": [
            "npm bin field",
            "shebang line",
            "executable permissions",
            "CLI entry point resolution",
            "cross-platform compatibility"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_029.2",
          "description": "Implement semantic versioning workflow following semver specification with automated version bumping, changelog generation, and git tag creation",
          "type": "sub_process",
          "parent_id": "REQ_029",
          "children": [],
          "acceptance_criteria": [
            "package.json version follows semver format (e.g., 1.0.0)",
            "Version bumping workflow supports major, minor, and patch increments",
            "Breaking changes trigger major version bump (e.g., 1.0.0 \u2192 2.0.0)",
            "New features trigger minor version bump (e.g., 1.0.0 \u2192 1.1.0)",
            "Bug fixes trigger patch version bump (e.g., 1.0.0 \u2192 1.0.1)",
            "Git tags created automatically on version bump (e.g., v1.0.0)",
            "CHANGELOG.md updated automatically with version changes",
            "npm version command works correctly (npm version patch/minor/major)",
            "CI/CD pipeline validates version format before publish",
            "Pre-release versions supported (e.g., 1.0.0-beta.1)",
            "Version command displays current version (silmari --version)",
            "Package published to npm includes correct version metadata"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "Initialize package.json with version 1.0.0 or 0.1.0 for initial release",
              "Add npm scripts: 'version:patch', 'version:minor', 'version:major'",
              "Configure standard-version or release-it for automated changelog generation",
              "Add .versionrc or release-it.config.js for versioning rules",
              "Set up conventional commits specification for commit message format",
              "Add commitlint to enforce conventional commit format",
              "Configure husky pre-commit hook for commit message validation",
              "Add postversion script to push tags: 'git push --follow-tags'",
              "Create CHANGELOG.md template with initial version entry",
              "Add GitHub Actions workflow for automated release on version tag push",
              "Configure npm provenance for package integrity",
              "Document versioning workflow in CONTRIBUTING.md"
            ]
          },
          "testable_properties": [],
          "function_id": "Versioning.setupSemanticVersioning",
          "related_concepts": [
            "semver specification (major.minor.patch)",
            "breaking changes detection",
            "changelog generation",
            "git tags",
            "npm version command",
            "conventional commits",
            "release automation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_029.3",
          "description": "Add comprehensive npm/pnpm installation instructions with setup verification, dependency installation, and global CLI configuration guidance",
          "type": "sub_process",
          "parent_id": "REQ_029",
          "children": [],
          "acceptance_criteria": [
            "README.md contains installation section with npm and pnpm commands",
            "Global installation command documented: 'npm install -g @anthropic-ai/silmari-context-engine'",
            "Local installation command documented: 'npm install @anthropic-ai/silmari-context-engine'",
            "pnpm global installation documented: 'pnpm add -g @anthropic-ai/silmari-context-engine'",
            "pnpm local installation documented: 'pnpm add @anthropic-ai/silmari-context-engine'",
            "Installation verification steps provided (silmari --version)",
            "Prerequisites documented (Node.js version requirement >= 18.0.0)",
            "Package published to npm registry under @anthropic-ai scope",
            "Package.json includes 'engines' field specifying Node.js version",
            "Package.json includes 'keywords' for npm search discoverability",
            "Package.json includes 'repository', 'bugs', 'homepage' fields",
            "Installation troubleshooting section in README",
            "Scoped package access configured correctly (@anthropic-ai organization)",
            "Package description clearly states CLI tool purpose"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "Create README.md Installation section with npm/pnpm commands",
              "Add package.json 'engines' field: { \"node\": \">=18.0.0\" }",
              "Configure package.json 'name': \"@anthropic-ai/silmari-context-engine\"",
              "Add package.json 'description': \"Context Engine CLI for Claude Code\"",
              "Add package.json 'keywords': [\"cli\", \"claude\", \"context-engine\", \"baml\", \"anthropic\"]",
              "Add package.json 'repository' field pointing to GitHub repo",
              "Add package.json 'bugs' field with GitHub issues URL",
              "Add package.json 'homepage' field with documentation URL",
              "Create INSTALL.md with detailed installation guide",
              "Add installation verification script for CI/CD testing",
              "Document global vs local installation use cases",
              "Add pnpm workspace configuration if needed",
              "Create installation troubleshooting guide (permission errors, path issues)"
            ]
          },
          "testable_properties": [],
          "function_id": "Installation.configurePackageManagers",
          "related_concepts": [
            "npm install",
            "pnpm install",
            "global installation",
            "local installation",
            "package registry",
            "installation verification",
            "peer dependencies",
            "optional dependencies"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_029.4",
          "description": "Configure global CLI entry point with command routing, argument parsing, help documentation, and version display functionality",
          "type": "sub_process",
          "parent_id": "REQ_029",
          "children": [],
          "acceptance_criteria": [
            "silmari command available globally after npm install -g",
            "silmari --version displays version from package.json",
            "silmari --help displays comprehensive command documentation",
            "silmari orchestrate subcommand routes to Orchestrator.run()",
            "silmari loop subcommand routes to LoopRunner.start()",
            "silmari plan subcommand routes to PlanningOrchestrator.run()",
            "Global options (--verbose, --quiet) apply to all subcommands",
            "Invalid commands display helpful error message and suggest --help",
            "Command aliases work (e.g., silmari o for orchestrate)",
            "Environment variables loaded from .env before command execution",
            "CLI exits with appropriate exit codes (0 for success, 1 for errors)",
            "Colored output for errors and success messages",
            "Spinner/progress indicators for long-running operations",
            "Graceful handling of SIGINT (Ctrl+C) during command execution"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Implement command handlers for orchestrate, loop, plan subcommands",
              "Create error handler middleware for CLI errors",
              "Implement progress indicator utility for async operations",
              "Add signal handler for graceful SIGINT shutdown"
            ],
            "middleware": [
              "Add environment variable validation middleware",
              "Add global option processing middleware (--verbose, --quiet)",
              "Add authentication check middleware (ANTHROPIC_API_KEY validation)"
            ],
            "shared": [
              "Create src/cli.ts with commander program setup",
              "Configure commander.Command with name, description, version",
              "Add subcommands using .command() for orchestrate, loop, plan",
              "Implement .option() for global flags (--verbose, --model, --config)",
              "Add .action() handlers for each subcommand",
              "Configure .helpOption() and .versionOption() for standard flags",
              "Add .alias() for common command shortcuts",
              "Install and configure dotenv for environment loading",
              "Install chalk or picocolors for colored terminal output",
              "Install ora for spinner/progress indicators",
              "Create CLIError class for formatted error messages",
              "Add error handler wrapper for all command actions",
              "Create help formatter for consistent documentation style"
            ]
          },
          "testable_properties": [],
          "function_id": "CLI.configureGlobalEntryPoint",
          "related_concepts": [
            "commander.js",
            "CLI argument parsing",
            "command routing",
            "help generation",
            "version flag",
            "subcommands",
            "global options",
            "error handling"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_029.5",
          "description": "Research and evaluate pkg and bun for standalone binary generation, comparing bundle size, startup performance, cross-platform support, and maintenance overhead",
          "type": "sub_process",
          "parent_id": "REQ_029",
          "children": [],
          "acceptance_criteria": [
            "Research document created comparing pkg vs bun build --compile",
            "Evaluation covers bundle size for each approach (target: <50MB)",
            "Evaluation covers startup time performance (target: <500ms)",
            "Evaluation covers cross-platform support (Linux, macOS, Windows)",
            "Evaluation covers build process complexity and CI/CD integration",
            "Evaluation covers maintenance burden (dependency updates, security patches)",
            "Evaluation covers feature compatibility (ESM, native modules, dynamic imports)",
            "Decision matrix created with weighted criteria for tool selection",
            "Proof-of-concept builds generated for both pkg and bun",
            "Binary verification tested on all target platforms",
            "Documentation created for binary build process",
            "Recommendation made with justification for chosen approach",
            "If pkg chosen: package.json configured with pkg field and targets",
            "If bun chosen: build script configured with bun build --compile --target",
            "GitHub Actions workflow created for binary releases on git tags"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "Install pkg as devDependency: npm install -D pkg",
              "Test pkg build: pkg . --targets node18-linux-x64,node18-macos-x64,node18-win-x64",
              "Measure pkg binary size and startup time",
              "Install bun: curl -fsSL https://bun.sh/install | bash",
              "Test bun build: bun build --compile --target=bun-linux-x64 src/cli.ts",
              "Measure bun binary size and startup time",
              "Create comparison matrix in research/binary-generation-comparison.md",
              "Test ESM compatibility with both approaches",
              "Test native module compatibility (if any used in project)",
              "Verify cross-platform execution on Docker containers (alpine, ubuntu, windows)",
              "Document pkg configuration in package.json if selected",
              "Document bun build script in package.json if selected",
              "Create GitHub Actions workflow: .github/workflows/release-binaries.yml",
              "Configure artifact upload for binaries (silmari-linux, silmari-macos, silmari-win.exe)",
              "Add binary distribution instructions to README.md",
              "Create decision record: docs/decisions/003-standalone-binary-tool.md"
            ]
          },
          "testable_properties": [],
          "function_id": "Binary.researchStandaloneGeneration",
          "related_concepts": [
            "pkg (vercel/pkg)",
            "bun build --compile",
            "standalone executables",
            "binary distribution",
            "Node.js embedding",
            "cross-compilation",
            "binary size optimization",
            "platform-specific builds"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_029.6",
          "description": "Configure npm publishing workflow with authentication, provenance, access control for scoped package, and automated release process",
          "type": "sub_process",
          "parent_id": "REQ_029",
          "children": [],
          "acceptance_criteria": [
            "Package successfully published to npm registry under @anthropic-ai scope",
            "Package accessible via npm install @anthropic-ai/silmari-context-engine",
            "npm provenance enabled for package integrity verification",
            "Access level configured (public or restricted based on requirements)",
            "Automated publishing configured via GitHub Actions on version tags",
            "Publish dry-run script available for testing (npm publish --dry-run)",
            "prepublishOnly script ensures build and tests pass before publishing",
            "npm token securely stored in GitHub Secrets",
            "Publishing workflow requires manual approval for production releases",
            "Package metadata correctly displayed on npmjs.com package page",
            "README.md rendered correctly on npm package page",
            "Package includes only necessary files (dist/, README, LICENSE)",
            ".npmignore configured to exclude source files and tests"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "Configure package.json 'publishConfig': { \"access\": \"public\" }",
              "Add package.json 'files' field: [\"dist\", \"README.md\", \"LICENSE\"]",
              "Create .npmignore with exclusions: src/, tests/, *.test.ts, .github/, thoughts/",
              "Add prepublishOnly script: \"npm run build && npm test\"",
              "Add publish:dry-run script: \"npm publish --dry-run\"",
              "Generate npm token from npmjs.com account settings",
              "Add NPM_TOKEN to GitHub repository secrets",
              "Create .github/workflows/publish.yml for automated publishing",
              "Configure workflow trigger: on push of tags matching v*.*.*",
              "Add workflow step: npm publish --provenance --access public",
              "Add workflow environment protection for production releases",
              "Test publishing to npm registry with dry-run",
              "Verify package installation from npm after first publish",
              "Document publishing process in CONTRIBUTING.md",
              "Add npm badge to README.md showing package version"
            ]
          },
          "testable_properties": [],
          "function_id": "Distribution.setupNpmPublishing",
          "related_concepts": [
            "npm publish",
            "npm access",
            "scoped packages",
            "npm provenance",
            "npm token",
            "package access control",
            "automated publishing",
            "publish dry-run"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_030",
      "description": "The system must implement comprehensive logging with structured JSON logs, configurable log levels (debug, info, warn, error), context-aware logging with entry IDs, performance metrics collection (processing time, token usage), log rotation and retention policies, and integration with monitoring tools",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_030.1",
          "description": "Set up structured JSON logging library with TypeScript support, including logger initialization, formatter configuration, and transport setup for different output targets (console, file, remote)",
          "type": "sub_process",
          "parent_id": "REQ_030",
          "children": [],
          "acceptance_criteria": [
            "Logger library selected and installed (recommend pino or winston for TypeScript compatibility)",
            "Logger configuration file created with JSON formatter enabled",
            "Logger singleton service class implemented with TypeScript types",
            "All log outputs are valid JSON with consistent schema (timestamp, level, message, context)",
            "Logger supports multiple transports (console for development, file for production)",
            "Logger instance can be imported and used across all modules",
            "Zod schema defined for log entry structure validation",
            "Unit tests verify JSON output format matches schema",
            "Integration tests confirm logger initializes without errors"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Install logging library (pino recommended for performance)",
              "Create src/logging/logger.ts with LoggerService class",
              "Define LogEntry interface with timestamp, level, message, metadata fields",
              "Create Zod schema for LogEntry validation",
              "Implement logger factory function with configuration options",
              "Configure JSON formatter with pretty-print for development",
              "Set up console transport for development environment",
              "Set up file transport for production environment",
              "Add environment variable LOG_FORMAT (json|pretty) support",
              "Export singleton logger instance"
            ],
            "middleware": [],
            "shared": [
              "Define LogLevel type ('debug' | 'info' | 'warn' | 'error')",
              "Create LogEntry interface with required fields",
              "Create LoggerConfig interface for initialization options",
              "Define Zod schema LogEntrySchema for runtime validation"
            ]
          },
          "testable_properties": [],
          "function_id": "LoggingService.setupStructuredLogger",
          "related_concepts": [
            "winston",
            "pino",
            "bunyan",
            "JSON serialization",
            "log transports",
            "logger singleton pattern",
            "dependency injection"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_030.2",
          "description": "Implement configurable log levels (debug, info, warn, error) with environment-based defaults, runtime level changes, and level filtering for different transports",
          "type": "sub_process",
          "parent_id": "REQ_030",
          "children": [],
          "acceptance_criteria": [
            "Environment variable LOG_LEVEL supported (debug|info|warn|error)",
            "Default log level set to 'info' if LOG_LEVEL not specified",
            "Log level hierarchy enforced (debug < info < warn < error)",
            "Logger only outputs messages at or above configured level",
            "Runtime log level changes supported via setLogLevel() method",
            "Different transports can have different log levels (e.g., console=debug, file=warn)",
            "Invalid log level values rejected with clear error message",
            "Unit tests verify level filtering for all four levels",
            "Integration tests confirm environment variable override works"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Add LOG_LEVEL to .env.example with default 'info'",
              "Create src/logging/config.ts for log configuration",
              "Implement getLogLevel() function to read from environment",
              "Add validateLogLevel() function with Zod schema",
              "Implement setLogLevel() method on LoggerService",
              "Configure logger to respect log level for filtering",
              "Add transport-specific level configuration support",
              "Create CLI command for runtime log level changes",
              "Add LOG_LEVEL_CONSOLE and LOG_LEVEL_FILE env vars for transport-specific levels"
            ],
            "middleware": [],
            "shared": [
              "Create LogLevel enum or const object with debug/info/warn/error",
              "Define LOG_LEVEL_HIERARCHY constant for level comparison",
              "Create Zod schema for log level validation",
              "Add LogLevelConfig interface for transport-specific levels"
            ]
          },
          "testable_properties": [],
          "function_id": "LoggingService.configureLogLevels",
          "related_concepts": [
            "log level hierarchy",
            "environment configuration",
            "dotenv",
            "runtime configuration",
            "level filtering",
            "transport-specific levels"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_030.3",
          "description": "Add context-aware logging with entry IDs, including automatic context ID injection, correlation ID tracking, and context metadata enrichment for all log entries",
          "type": "sub_process",
          "parent_id": "REQ_030",
          "children": [],
          "acceptance_criteria": [
            "All log entries include entry_id field when context available",
            "Context IDs match format ctx_XXXXXXXX (8 hex characters)",
            "Logger supports withContext(entryId) method for scoped logging",
            "Correlation IDs automatically propagated across async operations",
            "Parent-child relationships tracked via parent_id in logs",
            "Context metadata (source, type, timestamp) included in log entries",
            "AsyncLocalStorage used for automatic context propagation",
            "Context missing scenarios handled gracefully (no crash, warning logged)",
            "Unit tests verify context injection for all log levels",
            "Integration tests confirm context propagation across async boundaries"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Create src/logging/context.ts for context management",
              "Implement LogContext class with entryId, parentId, metadata fields",
              "Add AsyncLocalStorage instance for context storage",
              "Implement withContext(context, callback) wrapper function",
              "Add getLogContext() function to retrieve current context",
              "Modify logger to auto-inject context fields from AsyncLocalStorage",
              "Create child logger factory with inherited context",
              "Add setLogContext(context) method for manual context setting",
              "Implement clearLogContext() for context cleanup",
              "Add context validation to ensure entry_id format correctness"
            ],
            "middleware": [
              "Create logging middleware to initialize context per request",
              "Generate correlation IDs for incoming requests",
              "Inject correlation ID into AsyncLocalStorage",
              "Clean up context after request completion"
            ],
            "shared": [
              "Define LogContext interface with entryId, parentId, metadata",
              "Create ContextMetadata interface for additional context fields",
              "Add Zod schema for LogContext validation",
              "Define ENTRY_ID_PATTERN constant for regex validation"
            ]
          },
          "testable_properties": [],
          "function_id": "LoggingService.addContextAwareLogging",
          "related_concepts": [
            "context propagation",
            "correlation IDs",
            "AsyncLocalStorage",
            "request tracing",
            "entry ID tracking",
            "metadata enrichment",
            "structured context"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_030.4",
          "description": "Create performance metrics collection system for processing time, token usage, and operation statistics with aggregation, export capabilities, and real-time monitoring",
          "type": "sub_process",
          "parent_id": "REQ_030",
          "children": [],
          "acceptance_criteria": [
            "Processing time recorded for all pipeline operations",
            "Token usage tracked for all LLM API calls (input + output tokens)",
            "Metrics include operation name, duration_ms, tokens_used, status",
            "Metrics logged as structured JSON entries with level 'info'",
            "Support for metric aggregation (count, min, max, avg, p50, p95, p99)",
            "Metrics can be exported to monitoring systems (Prometheus format)",
            "PerformanceTimer utility class for automatic duration tracking",
            "TokenCounter utility for LLM response token calculation",
            "Metrics persisted to file for historical analysis",
            "Unit tests verify metric recording for all operation types",
            "Integration tests confirm metric aggregation accuracy"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Create src/metrics/performance.ts for metrics collection",
              "Implement PerformanceMetric interface with operation, duration, tokens, timestamp",
              "Create MetricsCollector class with singleton pattern",
              "Implement recordMetric(metric) method to store metrics",
              "Add PerformanceTimer class with start(), stop(), getDuration() methods",
              "Create TokenCounter utility to count tokens from LLM responses",
              "Implement getMetricsSummary() for aggregated statistics",
              "Add exportPrometheusFormat() method for Prometheus integration",
              "Create metrics file transport to persist metrics to disk",
              "Implement metrics rotation policy (daily rotation)",
              "Add getMetricsByOperation(operationName) query method"
            ],
            "middleware": [
              "Create performance tracking middleware for HTTP requests",
              "Auto-start timer at request start",
              "Auto-record metric at request completion",
              "Include request path and status code in metrics"
            ],
            "shared": [
              "Define PerformanceMetric interface with operation, duration_ms, tokens_used, status, timestamp",
              "Create MetricsSummary interface for aggregated data",
              "Define OperationType enum for categorizing operations",
              "Create Zod schema for PerformanceMetric validation",
              "Add MetricsExportFormat type for export options"
            ]
          },
          "testable_properties": [],
          "function_id": "MetricsService.collectPerformanceMetrics",
          "related_concepts": [
            "performance monitoring",
            "metrics aggregation",
            "prometheus",
            "StatsD",
            "token counting",
            "duration tracking",
            "histogram metrics",
            "counter metrics"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_030.5",
          "description": "Configure log rotation and retention policies with automated cleanup, size-based and time-based rotation, and integration with external monitoring tools (Prometheus, Grafana, CloudWatch)",
          "type": "sub_process",
          "parent_id": "REQ_030",
          "children": [],
          "acceptance_criteria": [
            "Log files rotate daily at midnight (time-based rotation)",
            "Log files rotate when exceeding 100MB (size-based rotation)",
            "Rotated logs compressed with gzip automatically",
            "Retention policy deletes logs older than 30 days",
            "Retention period configurable via LOG_RETENTION_DAYS environment variable",
            "Prometheus metrics endpoint exposes log counts and error rates",
            "CloudWatch integration available for AWS deployments",
            "Monitoring dashboard template provided (Grafana JSON)",
            "Alerting rules defined for error rate thresholds",
            "Unit tests verify rotation trigger conditions",
            "Integration tests confirm retention policy enforcement"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Install log rotation library (winston-daily-rotate-file or pino-roll)",
              "Create src/logging/rotation.ts for rotation configuration",
              "Configure daily rotation transport with datePattern YYYY-MM-DD",
              "Set maxSize option to '100m' for size-based rotation",
              "Enable zippedArchive: true for compression",
              "Set maxFiles option based on LOG_RETENTION_DAYS env var (default 30)",
              "Create cleanup job to enforce retention policy",
              "Add rotation event handlers for logging rotation events",
              "Create src/monitoring/prometheus.ts for metrics export",
              "Implement Prometheus exporter with log_entries_total counter",
              "Add log_errors_total counter for error tracking",
              "Create /metrics HTTP endpoint for Prometheus scraping",
              "Implement CloudWatch logger transport for AWS",
              "Add monitoring/grafana-dashboard.json template",
              "Define alerting rules in monitoring/alerts.yml"
            ],
            "middleware": [
              "Create metrics middleware to increment request counters",
              "Track response status codes for monitoring",
              "Expose metrics at /metrics endpoint"
            ],
            "shared": [
              "Define RotationConfig interface with maxSize, maxFiles, datePattern",
              "Create RetentionPolicy interface with days, maxSizeMB",
              "Add MonitoringConfig interface for external integrations",
              "Define Zod schemas for all monitoring configuration"
            ]
          },
          "testable_properties": [],
          "function_id": "LoggingService.configureRotationAndMonitoring",
          "related_concepts": [
            "log rotation",
            "retention policy",
            "winston-daily-rotate-file",
            "pino-roll",
            "monitoring integration",
            "prometheus exporter",
            "CloudWatch logs",
            "log compression",
            "disk space management"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    }
  ],
  "metadata": {
    "source": "agent_sdk_decomposition",
    "research_length": 43722,
    "decomposition_stats": {
      "requirements_found": 31,
      "subprocesses_expanded": 171,
      "total_nodes": 202,
      "extraction_time_ms": 107448,
      "expansion_time_ms": 2075018
    },
    "source_research": "/home/maceo/Dev/silmari-Context-Engine/thoughts/searchable/research/2026-01-04-typescript-port-research.md",
    "decomposed_at": "2026-01-05T04:50:35.409858"
  }
}