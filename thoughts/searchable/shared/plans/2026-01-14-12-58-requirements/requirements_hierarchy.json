{
  "requirements": [
    {
      "id": "REQ_000",
      "description": "The system must implement a multi-language architecture combining Python for planning pipeline and agent orchestration, Go for runtime implementation, and BAML for type-safe LLM prompt definitions",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_000.1",
          "description": "Implement Python-based planning pipeline in planning_pipeline/ directory with autonomous loop, decomposition logic, Claude runner, step execution, context generation, checkpoint management, and phase execution capabilities",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "autonomous_loop.py must implement main autonomous loop with start, stop, pause, and resume capabilities",
            "decomposition.py must break down high-level features into granular implementation steps",
            "claude_runner.py must manage Claude Code session execution with error handling and retry logic",
            "step_decomposition.py must decompose individual steps into actionable subtasks",
            "steps.py must execute individual steps with success/failure tracking",
            "context_generation.py must compile relevant context from working context array for each step",
            "checkpoint_manager.py must save and restore pipeline state at critical points",
            "checkpoints.py must provide checkpoint CRUD operations with versioning",
            "beads_controller.py must integrate with BEADS synchronization system",
            "integrated_orchestrator.py must coordinate between planning pipeline and RLM-ACT phases",
            "property_generator.py must generate testable properties for verification",
            "visualization.py must provide visual representations of pipeline state and progress",
            "phase_execution/ directory must contain separate modules for each execution phase",
            "All modules must have comprehensive error handling with typed exceptions",
            "All modules must include type hints and docstrings",
            "Test suite in tests/ must achieve minimum 80% code coverage",
            "Pipeline must support resuming from any checkpoint after interruption",
            "Pipeline must emit progress events for monitoring and debugging",
            "Pipeline must integrate with context_window_array for efficient context management",
            "Pipeline must support concurrent step execution where dependencies allow"
          ],
          "implementation": {
            "frontend": [
              "No direct frontend components (CLI/API driven)",
              "Progress visualization components for debugging dashboard (if applicable)",
              "Pipeline state inspector UI for checkpoint examination"
            ],
            "backend": [
              "autonomous_loop.py: Main loop orchestration with event loop and state machine",
              "decomposition.py: Feature decomposition algorithm with dependency graph",
              "claude_runner.py: Claude Code process management with stdio communication",
              "step_decomposition.py: Step-level decomposition with subtask generation",
              "steps.py: Step execution engine with parallel execution support",
              "context_generation.py: Context compilation from multiple sources",
              "checkpoint_manager.py: Checkpoint persistence with file-based storage",
              "checkpoints.py: Checkpoint data models and serialization",
              "beads_controller.py: BEADS API client integration",
              "integrated_orchestrator.py: Cross-component orchestration logic",
              "property_generator.py: Property-based test generation",
              "visualization.py: Pipeline state visualization with graphing",
              "phase_execution/: Phase-specific execution strategies",
              "helpers.py: Shared utility functions for pipeline operations",
              "models.py: Pydantic data models for pipeline state and steps",
              "pipeline.py: Main pipeline class with lifecycle management"
            ],
            "middleware": [
              "Error handling middleware for catching and transforming exceptions",
              "Logging middleware for structured logging of pipeline events",
              "Retry middleware for transient failure recovery",
              "Timeout middleware for preventing hung operations",
              "Rate limiting middleware for external API calls"
            ],
            "shared": [
              "models.py: PipelineState, Step, Phase, Checkpoint, ExecutionResult models",
              "helpers.py: Path utilities, file operations, string formatting",
              "Type definitions for pipeline events and callbacks",
              "Constants for timeout values, retry policies, checkpoint intervals",
              "Exception classes: PipelineError, DecompositionError, CheckpointError, ClaudeRunnerError"
            ]
          },
          "testable_properties": [],
          "function_id": "PlanningPipeline.implementPythonOrchestration",
          "related_concepts": [
            "autonomous_loop",
            "feature_decomposition",
            "claude_code_execution",
            "checkpoint_management",
            "phase_execution",
            "context_generation",
            "step_decomposition",
            "integrated_orchestrator",
            "BEADS_integration",
            "property_generation",
            "visualization_utilities"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_000.2",
          "description": "Implement Go-based runtime in go/ directory with context engine CLI, loop runner CLI, planning orchestration, execution management, concurrent processing, path utilities, file system operations, and JSON handling",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "cmd/context-engine/ must provide CLI interface for context engine operations",
            "cmd/loop-runner/ must provide CLI interface for running autonomous loops",
            "internal/planning/ must implement planning logic and orchestration in Go",
            "internal/cli/ must handle command-line argument parsing and validation",
            "internal/exec/ must manage execution of external processes and commands",
            "internal/models/ must define Go structs matching Python data models",
            "internal/path/ and internal/paths/ must provide cross-platform path utilities",
            "internal/fs/ must implement file system operations with error handling",
            "internal/concurrent/ must provide goroutine pools and synchronization primitives",
            "internal/json/ and internal/jsonutil/ must handle JSON serialization/deserialization",
            "internal/build/ must provide build utilities and version information",
            "Makefile must support build, test, install, and clean targets",
            "build-and-install.sh must build for multiple platforms (darwin, linux, windows)",
            "build/ directory must contain compiled binaries for all supported platforms",
            "All CLI commands must have --help flags with comprehensive documentation",
            "All internal packages must have Go doc comments",
            "Unit tests must achieve minimum 80% code coverage",
            "Integration tests must verify interoperability with Python pipeline",
            "CLI must support JSON output format for programmatic consumption",
            "Runtime must communicate with Python pipeline via stdio or IPC",
            "Error messages must be actionable and include context"
          ],
          "implementation": {
            "frontend": [
              "No direct frontend components (CLI driven)",
              "CLI progress bars for long-running operations",
              "Colored terminal output for error/warning/success states"
            ],
            "backend": [
              "cmd/context-engine/main.go: Main CLI entry point with cobra command structure",
              "cmd/loop-runner/main.go: Loop runner CLI with start/stop/status commands",
              "internal/planning/orchestrator.go: Main orchestration logic",
              "internal/planning/planner.go: Planning algorithm implementation",
              "internal/cli/parser.go: Command-line argument parsing",
              "internal/cli/flags.go: Flag definitions and validation",
              "internal/exec/runner.go: Process execution with streaming output",
              "internal/exec/manager.go: Process lifecycle management",
              "internal/models/pipeline.go: Pipeline state models",
              "internal/models/step.go: Step execution models",
              "internal/path/resolver.go: Path resolution utilities",
              "internal/paths/utils.go: Path manipulation utilities",
              "internal/fs/operations.go: File system operations",
              "internal/fs/watcher.go: File system watching for hot reload",
              "internal/concurrent/pool.go: Worker pool implementation",
              "internal/concurrent/sync.go: Synchronization primitives",
              "internal/json/encoder.go: Custom JSON encoding",
              "internal/jsonutil/helpers.go: JSON utility functions",
              "internal/build/version.go: Build version and metadata"
            ],
            "middleware": [
              "Panic recovery middleware for graceful error handling",
              "Logging middleware with structured logging (zerolog/zap)",
              "Metrics middleware for performance monitoring",
              "Context propagation middleware for request tracing"
            ],
            "shared": [
              "internal/models/: Go structs for PipelineState, Step, Phase, ExecutionResult",
              "internal/errors/: Custom error types and error wrapping utilities",
              "internal/config/: Configuration loading and validation",
              "Constants for timeouts, buffer sizes, worker pool sizes",
              "Interface definitions for Planner, Executor, StorageBackend"
            ]
          },
          "testable_properties": [],
          "function_id": "GoRuntime.implementContextEngine",
          "related_concepts": [
            "context_engine_runtime",
            "CLI_tools",
            "planning_orchestration",
            "execution_management",
            "concurrency_primitives",
            "path_utilities",
            "filesystem_operations",
            "json_utilities",
            "cross_platform_builds",
            "makefile_configuration"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_000.3",
          "description": "Define BAML type-safe LLM prompt definitions in baml_src/ directory with function definitions, type schemas, client configurations, and generator settings for all LLM interactions",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "baml_src/functions.baml must define all LLM function signatures with input/output types",
            "baml_src/types.baml must define all structured data types used in prompts and responses",
            "baml_src/clients.baml must configure LLM client settings (API keys, models, timeouts)",
            "baml_src/generators.baml must specify code generation targets (Python, TypeScript, Go)",
            "baml_src/schema/ must contain modular schema definitions for different domains",
            "All BAML functions must have clear docstrings explaining their purpose",
            "All BAML types must use appropriate field types (string, int, bool, enum, class)",
            "All BAML types must include validation constraints where applicable",
            "BAML functions must specify retry policies and fallback behaviors",
            "BAML functions must define expected token counts for cost estimation",
            "BAML schema must support both streaming and non-streaming responses",
            "BAML definitions must be validated with baml-cli before code generation",
            "Generated clients must pass type checking in target languages",
            "BAML functions must map to actual use cases in planning_pipeline and silmari_rlm_act",
            "Schema changes must trigger automatic client regeneration in CI/CD"
          ],
          "implementation": {
            "frontend": [
              "No direct frontend components",
              "BAML playground/debugger for testing prompts (if available)",
              "Documentation site for BAML schema reference"
            ],
            "backend": [
              "baml_src/functions.baml: Function definitions for decompose_feature, generate_test, review_code, debug_error, verify_implementation",
              "baml_src/types.baml: Type definitions for Feature, Step, TestCase, CodeReview, DebugAnalysis, VerificationResult",
              "baml_src/clients.baml: Client configurations for Claude, GPT-4, local models",
              "baml_src/generators.baml: Generator configs for python, typescript, golang targets",
              "baml_src/schema/planning.baml: Schema for planning-related types",
              "baml_src/schema/testing.baml: Schema for testing-related types",
              "baml_src/schema/verification.baml: Schema for verification-related types",
              "baml_src/schema/research.baml: Schema for research phase types",
              "Prompt templates with variable interpolation for dynamic context",
              "Validation functions for ensuring type constraints",
              "Error handling definitions for LLM failures"
            ],
            "middleware": [
              "Request/response logging middleware for LLM calls",
              "Caching middleware for identical prompts",
              "Rate limiting middleware for API quota management",
              "Token counting middleware for cost tracking"
            ],
            "shared": [
              "Type definitions mirroring Python/Go models for consistency",
              "Enum definitions for status codes, phase types, error categories",
              "Constants for max token counts, temperature settings, top_p values",
              "Shared validation schemas for input sanitization"
            ]
          },
          "testable_properties": [],
          "function_id": "BAMLSchema.defineTypeSafePrompts",
          "related_concepts": [
            "type_safe_prompts",
            "LLM_function_definitions",
            "structured_outputs",
            "client_configuration",
            "code_generation",
            "schema_definitions",
            "prompt_templates",
            "validation_rules"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_000.4",
          "description": "Generate multi-language type-safe clients in baml_client/ directory from BAML source definitions for Python, TypeScript, and Go with automatic code generation, type checking, and runtime validation",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "baml_client/ must contain auto-generated subdirectories for each target language (python, typescript, golang)",
            "Generated Python client must be importable and type-checkable with mypy",
            "Generated TypeScript client must be importable and type-checkable with tsc",
            "Generated Go client must compile and pass go vet checks",
            "Generated clients must provide async/await interfaces for non-blocking LLM calls",
            "Generated clients must handle streaming responses with callback support",
            "Generated clients must validate input parameters against BAML type constraints",
            "Generated clients must deserialize LLM responses into strongly-typed objects",
            "Generated clients must throw typed exceptions for validation and API errors",
            "Generated clients must include inline documentation from BAML docstrings",
            "Code generation must be idempotent (running twice produces identical output)",
            "Generated code must not require manual modifications",
            "Build scripts must automatically regenerate clients when BAML sources change",
            "Generated clients must support dependency injection for testing",
            "Generated clients must include retry logic with exponential backoff",
            "Generated clients must support custom HTTP headers and authentication"
          ],
          "implementation": {
            "frontend": [
              "No direct frontend components",
              "Generated TypeScript client can be used in frontend if needed"
            ],
            "backend": [
              "baml_client/python/: Generated Python client with __init__.py, client.py, types.py, exceptions.py",
              "baml_client/typescript/: Generated TypeScript client with index.ts, client.ts, types.ts, errors.ts",
              "baml_client/golang/: Generated Go client with client.go, types.go, errors.go",
              "Code generation templates for each language with proper indentation and formatting",
              "Type mapping logic from BAML types to native language types",
              "Serialization/deserialization logic for complex nested types",
              "HTTP client wrappers for making API requests",
              "Streaming response handlers with callback interfaces",
              "Retry logic with configurable backoff strategies",
              "Error handling with typed exception hierarchies",
              "Documentation generation from BAML docstrings",
              "Build scripts (generate.sh or Makefile targets) for triggering regeneration"
            ],
            "middleware": [
              "Generated middleware for request/response logging",
              "Generated middleware for API key injection",
              "Generated middleware for response validation",
              "Generated middleware for error transformation"
            ],
            "shared": [
              "baml_client/shared/: Shared utilities and constants across languages",
              "Type definitions that are consistent across Python, TypeScript, and Go",
              "Common error codes and messages",
              "Validation schemas compiled from BAML constraints",
              "Configuration models for client initialization"
            ]
          },
          "testable_properties": [],
          "function_id": "BAMLClient.generateMultiLanguageClients",
          "related_concepts": [
            "code_generation",
            "multi_language_support",
            "type_safety",
            "runtime_validation",
            "API_client_generation",
            "serialization_deserialization",
            "error_handling",
            "build_integration"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_001",
      "description": "The system must provide specialized subagent prompt definitions including code-reviewer, debugger, feature-verifier, and test-runner agents for autonomous development",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_001.1",
          "description": "Define code-reviewer agent prompt in agents/code-reviewer.md that analyzes code changes for quality, style, security, and correctness issues",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "agents/code-reviewer.md file exists with complete agent prompt definition",
            "Prompt includes clear instructions for reviewing code changes in git diffs",
            "Agent can identify common code smells (duplicate code, long methods, complex conditionals)",
            "Agent can detect security vulnerabilities (SQL injection, XSS, insecure dependencies)",
            "Agent validates adherence to project coding standards and style guides",
            "Agent checks for proper error handling and edge case coverage",
            "Agent verifies code comments and documentation quality",
            "Agent assesses test coverage and suggests missing test cases",
            "Prompt specifies output format for review findings (severity, location, suggestion)",
            "Agent can prioritize issues by severity (critical, high, medium, low)",
            "Prompt includes examples of good vs bad code patterns",
            "Agent provides actionable remediation suggestions for each issue found"
          ],
          "implementation": {
            "frontend": [
              "No frontend components required - agent operates via Claude Code interface"
            ],
            "backend": [
              "Markdown prompt file defining agent behavior and instructions",
              "Review logic for analyzing code diffs and identifying issues",
              "Severity classification system for findings",
              "Pattern matching rules for common code issues",
              "Integration points with git diff output"
            ],
            "middleware": [
              "No middleware required - invoked directly by Claude Code orchestration"
            ],
            "shared": [
              "Code review criteria definitions (security, quality, style)",
              "Severity level enumeration (critical, high, medium, low, info)",
              "Issue category taxonomy (security, performance, maintainability, style)",
              "Code smell pattern library",
              "Best practices reference documentation"
            ]
          },
          "testable_properties": [],
          "function_id": "CodeReviewerAgent.define",
          "related_concepts": [
            "static code analysis",
            "code quality metrics",
            "security vulnerability detection",
            "best practices enforcement",
            "diff analysis",
            "linting integration",
            "code smell detection"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_001.2",
          "description": "Define debugger agent prompt in agents/debugger.md that analyzes errors, stack traces, and runtime issues to suggest root causes and fixes",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "agents/debugger.md file exists with complete agent prompt definition",
            "Prompt includes instructions for parsing and analyzing stack traces",
            "Agent can identify root cause from error messages and context",
            "Agent traces error propagation through call stack",
            "Agent suggests specific debugging steps to isolate the issue",
            "Agent provides concrete fix recommendations with code examples",
            "Agent can analyze logs to identify error patterns and anomalies",
            "Prompt specifies how to handle different error types (runtime, compile-time, logic)",
            "Agent considers environment-specific issues (dependencies, configurations)",
            "Agent validates proposed fixes won't introduce new issues",
            "Prompt includes strategies for reproducing intermittent errors",
            "Agent output includes confidence level for each diagnosis"
          ],
          "implementation": {
            "frontend": [
              "No frontend components required - agent operates via Claude Code interface"
            ],
            "backend": [
              "Markdown prompt file defining debugging analysis instructions",
              "Stack trace parsing and interpretation logic",
              "Root cause analysis decision tree",
              "Fix suggestion generation with code examples",
              "Error pattern matching against known issues",
              "Integration with error logs and runtime output"
            ],
            "middleware": [
              "No middleware required - invoked directly by Claude Code orchestration"
            ],
            "shared": [
              "Error type taxonomy (syntax, runtime, logic, environment)",
              "Common error pattern library",
              "Debugging strategy templates",
              "Fix confidence scoring model",
              "Stack trace format definitions for multiple languages"
            ]
          },
          "testable_properties": [],
          "function_id": "DebuggerAgent.define",
          "related_concepts": [
            "error analysis",
            "stack trace parsing",
            "root cause analysis",
            "debugging strategies",
            "runtime error handling",
            "log analysis",
            "exception handling patterns"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_001.3",
          "description": "Define feature-verifier agent prompt in agents/feature-verifier.md that performs end-to-end verification of implemented features against requirements",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "agents/feature-verifier.md file exists with complete agent prompt definition",
            "Prompt includes instructions for mapping implementation to requirements",
            "Agent verifies all acceptance criteria are met for the feature",
            "Agent performs end-to-end functional testing of the feature",
            "Agent validates integration points with existing system components",
            "Agent checks for regression issues in related functionality",
            "Agent verifies edge cases and error scenarios are handled",
            "Prompt specifies how to generate test scenarios from requirements",
            "Agent validates user-facing documentation matches implementation",
            "Agent checks for performance and scalability considerations",
            "Agent output includes pass/fail status with specific evidence",
            "Prompt includes guidelines for determining feature readiness for release"
          ],
          "implementation": {
            "frontend": [
              "No frontend components required - agent operates via Claude Code interface"
            ],
            "backend": [
              "Markdown prompt file defining feature verification instructions",
              "Requirements-to-implementation mapping logic",
              "End-to-end test scenario generation",
              "Integration point validation rules",
              "Regression detection strategies",
              "Feature completeness scoring algorithm"
            ],
            "middleware": [
              "No middleware required - invoked directly by Claude Code orchestration"
            ],
            "shared": [
              "Feature verification checklist template",
              "Acceptance criteria evaluation framework",
              "Test coverage metrics definitions",
              "Regression risk assessment model",
              "Feature readiness criteria matrix"
            ]
          },
          "testable_properties": [],
          "function_id": "FeatureVerifierAgent.define",
          "related_concepts": [
            "end-to-end testing",
            "feature completeness verification",
            "requirements traceability",
            "user acceptance criteria",
            "integration testing",
            "regression testing",
            "feature flag validation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_001.4",
          "description": "Define test-runner agent prompt in agents/test-runner.md that executes tests, analyzes failures, and provides actionable debugging information",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "agents/test-runner.md file exists with complete agent prompt definition",
            "Prompt includes instructions for running different test types (unit, integration, e2e)",
            "Agent executes test suite and captures all output",
            "Agent parses test results to identify failures and successes",
            "Agent analyzes failure messages and stack traces for root causes",
            "Agent identifies flaky tests through pattern recognition",
            "Agent suggests specific fixes for failing tests",
            "Prompt specifies how to handle test timeouts and hung processes",
            "Agent generates test coverage reports with gap analysis",
            "Agent prioritizes test failures by impact and severity",
            "Agent detects test dependencies and ordering issues",
            "Prompt includes instructions for validating test quality and effectiveness"
          ],
          "implementation": {
            "frontend": [
              "No frontend components required - agent operates via Claude Code interface"
            ],
            "backend": [
              "Markdown prompt file defining test execution and analysis instructions",
              "Test runner integration for multiple test frameworks (pytest, jest, go test)",
              "Test output parsing and result aggregation logic",
              "Failure analysis engine with root cause detection",
              "Flaky test detection algorithm using historical data",
              "Test coverage calculation and reporting",
              "Fix suggestion generation based on failure patterns"
            ],
            "middleware": [
              "No middleware required - invoked directly by Claude Code orchestration"
            ],
            "shared": [
              "Test result data models (pass/fail/skip status, duration, output)",
              "Test type enumeration (unit, integration, e2e, performance)",
              "Failure category taxonomy (assertion, timeout, error, crash)",
              "Test coverage metrics (line, branch, function coverage)",
              "Flaky test detection thresholds and criteria"
            ]
          },
          "testable_properties": [],
          "function_id": "TestRunnerAgent.define",
          "related_concepts": [
            "test execution",
            "test failure analysis",
            "test coverage reporting",
            "flaky test detection",
            "test suite optimization",
            "assertion analysis",
            "test report generation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_001.5",
          "description": "Integrate specialized agents with Claude Code orchestration to invoke them automatically during appropriate development phases",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "code-reviewer agent automatically invoked after code changes are made",
            "debugger agent automatically invoked when errors or test failures occur",
            "feature-verifier agent invoked at feature completion milestones",
            "test-runner agent invoked during testing phases and before commits",
            "Agent invocation integrated with planning_pipeline/autonomous_loop.py",
            "Agent invocation integrated with silmari_rlm_act/pipeline.py phases",
            "Context from planning pipeline properly passed to invoked agents",
            "Agent outputs captured and stored in .agent/artifacts/ directory",
            "Agent findings influence next steps in autonomous development cycle",
            "Failed agent checks block progression to next phase",
            "Agent invocation configurable via phase execution rules",
            "Integration supports parallel agent execution where appropriate",
            "Agent retry logic handles transient failures gracefully",
            "Integration logs all agent invocations and results for debugging"
          ],
          "implementation": {
            "frontend": [
              "No frontend components required - orchestration operates programmatically"
            ],
            "backend": [
              "Agent invocation hooks in planning_pipeline/autonomous_loop.py",
              "Agent invocation hooks in silmari_rlm_act/phases/ modules",
              "Context preparation logic to format data for each agent type",
              "Agent output parsing and interpretation logic",
              "Phase progression rules based on agent check results",
              "Agent execution scheduling and coordination",
              "Retry and error handling for agent invocations",
              "Agent result storage and retrieval from .agent/artifacts/",
              "Integration with Claude Code Task tool for agent spawning",
              "Agent configuration loading from .claude/agents/ directory"
            ],
            "middleware": [
              "Agent routing logic to select appropriate agent for each situation",
              "Context transformation layer to prepare inputs for agents",
              "Result aggregation middleware to combine outputs from multiple agents",
              "Authorization checks to ensure agents have required permissions"
            ],
            "shared": [
              "Agent invocation data models (agent_type, context, parameters, results)",
              "Phase-to-agent mapping configuration",
              "Agent execution status enumeration (pending, running, completed, failed)",
              "Agent result schema (findings, recommendations, pass/fail status)",
              "Context data models for agent input/output",
              "Agent configuration schema for .claude/agents/ definitions",
              "Integration point interfaces between orchestration and agents"
            ]
          },
          "testable_properties": [],
          "function_id": "AgentIntegration.claudeCode",
          "related_concepts": [
            "agent orchestration",
            "phase-based execution",
            "autonomous development workflow",
            "agent task routing",
            "context passing between agents",
            "agent output aggregation",
            "workflow state management"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_002",
      "description": "The system must implement a four-layer memory architecture for autonomous feature building with layers for working context, planning pipeline, agent orchestration, and runtime execution",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_002.1",
          "description": "Implement Layer 1 working context management system in context_window_array/ to provide efficient context batching, indexing, storage, and retrieval for LLM interactions",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "batching.py implements context chunking algorithm that splits large contexts into manageable batches within LLM token limits",
            "search_index.py provides fast semantic search over context items using vector embeddings or keyword indexing",
            "store.py persists context state to disk and loads it reliably across sessions",
            "working_context.py maintains current active context and supports add/remove/update operations",
            "implementation_context.py tracks context specific to current implementation phase with metadata",
            "models.py defines typed data structures for ContextBatch, ContextItem, WorkingContext, and SearchIndex",
            "Context batching algorithm handles edge cases: oversized items, empty batches, and boundary conditions",
            "Search index supports filtering by metadata (timestamp, phase, relevance score)",
            "Store implementation handles concurrent access safely with file locking or database transactions",
            "Working context enforces maximum size limits and implements LRU or priority-based eviction",
            "All components have comprehensive unit tests with >80% code coverage",
            "Integration tests verify context flows correctly between batching, indexing, and storage",
            "Performance benchmarks show context retrieval completes in <100ms for 10,000 items",
            "Memory usage remains bounded even with large context histories (max 500MB)",
            "API documentation clearly explains usage patterns for each module"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "batching.py: create_batches(items, max_tokens) -> List[ContextBatch]",
              "batching.py: optimize_batch_boundaries(batches) -> List[ContextBatch]",
              "search_index.py: build_index(context_items) -> SearchIndex",
              "search_index.py: search(query, filters, top_k) -> List[ContextItem]",
              "search_index.py: update_index(new_items) for incremental updates",
              "store.py: save_context(context, path) for persistence",
              "store.py: load_context(path) -> WorkingContext",
              "store.py: list_checkpoints() -> List[ContextCheckpoint]",
              "working_context.py: add_item(item) with deduplication",
              "working_context.py: remove_item(item_id)",
              "working_context.py: get_active_context() -> str",
              "working_context.py: evict_low_priority_items()",
              "implementation_context.py: set_phase_context(phase, items)",
              "implementation_context.py: get_phase_context(phase) -> List[ContextItem]"
            ],
            "middleware": [
              "Token counting middleware for accurate batch size calculation",
              "Context validation middleware to ensure well-formed items",
              "Compression middleware for efficient storage",
              "Versioning middleware for backward compatibility"
            ],
            "shared": [
              "models.py: ContextBatch(items, token_count, metadata)",
              "models.py: ContextItem(content, type, timestamp, relevance, metadata)",
              "models.py: WorkingContext(items, max_size, eviction_policy)",
              "models.py: SearchIndex(index_type, embeddings, metadata_filters)",
              "models.py: ContextCheckpoint(timestamp, context_state, phase)",
              "Utility functions for token counting (count_tokens)",
              "Utility functions for context serialization/deserialization",
              "Constants for max batch sizes, token limits, eviction policies"
            ]
          },
          "testable_properties": [],
          "function_id": "ContextWindowArray.implementWorkingContext",
          "related_concepts": [
            "context batching",
            "search indexing",
            "context storage",
            "working memory",
            "context retrieval",
            "LLM token management",
            "context window limits",
            "memory persistence"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.2",
          "description": "Implement Layer 2 planning pipeline in planning_pipeline/ to orchestrate autonomous feature decomposition, step execution, and Claude Code coordination",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "decomposition.py breaks down high-level features into granular implementation steps with dependencies",
            "step_decomposition.py further decomposes steps into atomic actions with preconditions and postconditions",
            "autonomous_loop.py implements main control loop that iterates through plan steps and monitors progress",
            "claude_runner.py spawns Claude Code sessions with appropriate context and captures outputs",
            "pipeline.py coordinates all pipeline stages: decomposition -> planning -> execution -> verification",
            "checkpoint_manager.py creates recovery points after each major phase completion",
            "steps.py defines step execution logic with retry mechanisms and error handling",
            "context_generation.py compiles relevant context from working memory for each step",
            "phase_execution/ modules handle research, implementation, testing, and verification phases",
            "integrated_orchestrator.py integrates with BEADS sync and external systems",
            "Decomposition algorithm identifies dependencies between steps and creates valid execution order",
            "Step execution handles failures gracefully with automatic retry (max 3 attempts) and fallback strategies",
            "Autonomous loop detects stuck states and requests human intervention when needed",
            "Claude runner captures all outputs (code changes, test results, errors) and updates working context",
            "Checkpoint system allows resuming from any saved state without data loss",
            "Property generator creates testable properties for each implementation step",
            "Integration tests verify complete pipeline execution for sample features",
            "Pipeline supports parallel step execution where dependencies allow",
            "Monitoring dashboard shows current phase, completed steps, and estimated time remaining",
            "All pipeline stages have comprehensive logging for debugging and auditing"
          ],
          "implementation": {
            "frontend": [
              "CLI interface for pipeline status monitoring",
              "Progress visualization showing decomposition tree",
              "Interactive checkpoint browser for recovery"
            ],
            "backend": [
              "decomposition.py: decompose_feature(spec) -> FeaturePlan",
              "decomposition.py: analyze_dependencies(steps) -> DependencyGraph",
              "step_decomposition.py: decompose_step(step) -> List[AtomicAction]",
              "autonomous_loop.py: run_autonomous_loop(plan, context)",
              "autonomous_loop.py: monitor_progress() -> ProgressReport",
              "autonomous_loop.py: detect_stuck_state() -> bool",
              "claude_runner.py: spawn_claude_session(context, task) -> Session",
              "claude_runner.py: execute_in_claude(session, command) -> Result",
              "claude_runner.py: capture_claude_output(session) -> Output",
              "pipeline.py: execute_pipeline(feature_spec) -> ExecutionResult",
              "pipeline.py: validate_pipeline_state() -> ValidationResult",
              "checkpoint_manager.py: create_checkpoint(state) -> CheckpointId",
              "checkpoint_manager.py: restore_checkpoint(id) -> PipelineState",
              "steps.py: execute_step(step, context) -> StepResult",
              "steps.py: retry_failed_step(step, max_retries)",
              "context_generation.py: compile_context(step, working_memory) -> Context",
              "phase_execution/research.py: execute_research_phase()",
              "phase_execution/implementation.py: execute_implementation_phase()",
              "phase_execution/testing.py: execute_testing_phase()",
              "integrated_orchestrator.py: sync_with_beads(state)",
              "property_generator.py: generate_properties(step) -> List[Property]"
            ],
            "middleware": [
              "Pipeline state validation middleware",
              "Claude session authentication and authorization",
              "Rate limiting for Claude API calls",
              "Output sanitization for captured results"
            ],
            "shared": [
              "models.py: FeaturePlan(steps, dependencies, metadata)",
              "models.py: PlanningStep(description, type, preconditions, postconditions)",
              "models.py: AtomicAction(command, expected_result, validation)",
              "models.py: DependencyGraph(nodes, edges, topological_order)",
              "models.py: PipelineState(current_phase, completed_steps, context)",
              "models.py: StepResult(success, output, errors, duration)",
              "models.py: ExecutionResult(status, completed_steps, failed_steps, artifacts)",
              "helpers.py: utility functions for validation, parsing, formatting",
              "Constants for retry limits, timeouts, phase names"
            ]
          },
          "testable_properties": [],
          "function_id": "PlanningPipeline.implementPlanningLayer",
          "related_concepts": [
            "feature decomposition",
            "step planning",
            "task orchestration",
            "Claude Code integration",
            "checkpoint management",
            "phase execution",
            "property generation",
            "autonomous loop control"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.3",
          "description": "Implement Layer 3 agent orchestration in silmari_rlm_act/ using Research-Learn-Model-Act pattern for autonomous TDD with comprehensive phase management",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "pipeline.py implements complete RLM-ACT cycle orchestration with phase transitions",
            "phases/research.py conducts automated research by analyzing codebase, docs, and external resources",
            "phases/decomposition.py breaks features into testable units aligned with TDD principles",
            "phases/tdd_planning.py creates test plans with test cases before implementation",
            "phases/implementation.py executes implementation guided by failing tests",
            "phases/beads_sync.py synchronizes state with BEADS system for persistence",
            "phases/multi_doc.py handles processing of multiple documentation sources",
            "agents/ directory contains specialized agent implementations for different tasks",
            "commands/ provides CLI commands for phase control and inspection",
            "context/ manages context specific to agent execution",
            "hooks/ implements git hooks and Claude Code hooks for automation",
            "checkpoints/ provides checkpoint system for phase recovery",
            "validation/ validates inputs and outputs at each phase boundary",
            "CLI interface allows starting, pausing, resuming RLM-ACT pipeline",
            "Research phase identifies all relevant code files, tests, and documentation",
            "Learning phase extracts patterns, conventions, and architectural decisions",
            "Modeling phase creates abstract models of changes needed",
            "Action phase executes changes with continuous test feedback",
            "Phase transitions only occur when exit criteria are met",
            "Checkpoint system enables recovery from any phase without losing progress",
            "BEADS sync keeps external knowledge base updated with learnings",
            "Integration tests verify complete RLM-ACT cycle for sample features",
            "Agent coordination prevents conflicting actions and maintains consistency",
            "Multi-doc processing consolidates information from diverse sources",
            "TDD planning generates comprehensive test suites before implementation"
          ],
          "implementation": {
            "frontend": [
              "CLI dashboard showing current phase and progress",
              "Interactive phase inspector for debugging",
              "Checkpoint browser for phase state inspection"
            ],
            "backend": [
              "pipeline.py: execute_rlm_act_cycle(feature_spec) -> CycleResult",
              "pipeline.py: transition_phase(from_phase, to_phase) -> bool",
              "pipeline.py: check_phase_exit_criteria(phase) -> bool",
              "phases/research.py: execute_research(context) -> ResearchFindings",
              "phases/research.py: analyze_codebase(scope) -> CodebaseAnalysis",
              "phases/research.py: gather_documentation(topics) -> DocCollection",
              "phases/decomposition.py: decompose_for_tdd(feature) -> TestableUnits",
              "phases/tdd_planning.py: create_test_plan(units) -> TestPlan",
              "phases/tdd_planning.py: generate_test_cases(unit) -> List[TestCase]",
              "phases/implementation.py: implement_with_tdd(test_plan) -> ImplementationResult",
              "phases/implementation.py: run_tests_continuously() -> TestResults",
              "phases/beads_sync.py: sync_state(phase_state) -> SyncResult",
              "phases/beads_sync.py: retrieve_relevant_knowledge(context) -> Knowledge",
              "phases/multi_doc.py: process_documents(docs) -> ConsolidatedDoc",
              "agents/code_agent.py: execute_code_action(action) -> ActionResult",
              "agents/test_agent.py: run_tests(test_suite) -> TestReport",
              "agents/review_agent.py: review_changes(diff) -> ReviewFeedback",
              "commands/status.py: show_pipeline_status()",
              "commands/next.py: advance_to_next_phase()",
              "context/agent_context.py: get_agent_context(agent_id) -> Context",
              "hooks/git_hooks.py: pre_commit_validation()",
              "hooks/claude_hooks.py: on_code_change_callback()",
              "checkpoints/phase_checkpoint.py: save_phase_state(phase, state)",
              "checkpoints/phase_checkpoint.py: restore_phase_state(checkpoint_id)",
              "validation/input_validation.py: validate_feature_spec(spec) -> ValidationResult",
              "validation/output_validation.py: validate_phase_output(output) -> bool"
            ],
            "middleware": [
              "Phase transition guards ensuring prerequisites are met",
              "Agent authentication and authorization middleware",
              "Rate limiting for external API calls (BEADS, docs)",
              "Output validation middleware for phase boundaries"
            ],
            "shared": [
              "models.py: RlmActCycle(phases, current_phase, state)",
              "models.py: PhaseState(phase_name, status, inputs, outputs, metadata)",
              "models.py: ResearchFindings(code_analysis, documentation, patterns)",
              "models.py: TestableUnit(feature_part, test_cases, implementation_steps)",
              "models.py: TestPlan(test_suites, test_cases, execution_order)",
              "models.py: TestCase(name, description, preconditions, expected_result)",
              "models.py: ImplementationResult(code_changes, test_results, coverage)",
              "models.py: AgentAction(agent_id, action_type, parameters, result)",
              "models.py: PhaseCheckpoint(phase, timestamp, state, metadata)",
              "cli.py: CLI command definitions and argument parsing",
              "Constants for phase names, transition rules, timeout values"
            ]
          },
          "testable_properties": [],
          "function_id": "SilmariRlmAct.implementAgentOrchestration",
          "related_concepts": [
            "RLM-ACT pattern",
            "test-driven development",
            "research phase",
            "learning phase",
            "modeling phase",
            "action phase",
            "agent coordination",
            "BEADS integration",
            "multi-document processing",
            "TDD planning"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.4",
          "description": "Implement Layer 4 runtime execution in go/ to provide high-performance concurrent execution engine with CLI tools for planning orchestration",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "cmd/context-engine/ provides CLI for context engine operations with subcommands",
            "cmd/loop-runner/ provides CLI for autonomous loop execution",
            "internal/planning/ implements planning logic and orchestration in Go",
            "internal/cli/ handles command-line argument parsing and validation",
            "internal/exec/ manages process execution with proper error handling",
            "internal/models/ defines Go data structures matching Python models",
            "internal/path/ and internal/paths/ provide cross-platform path utilities",
            "internal/fs/ implements file system operations with atomic writes",
            "internal/concurrent/ provides concurrency primitives (worker pools, channels)",
            "internal/json/ and internal/jsonutil/ handle JSON serialization efficiently",
            "internal/build/ automates build processes and binary compilation",
            "Makefile supports building for multiple platforms (darwin, linux, windows)",
            "build-and-install.sh script automates build and installation process",
            "Context engine CLI supports: init, add, remove, list, search operations",
            "Loop runner CLI executes autonomous loops with real-time progress reporting",
            "Concurrent execution uses goroutines efficiently without resource leaks",
            "Process execution handles timeouts, cancellation, and signal handling",
            "File system operations are atomic and handle concurrent access safely",
            "Path utilities normalize paths correctly across Windows, macOS, and Linux",
            "JSON serialization handles large payloads efficiently (<100ms for 10MB)",
            "Build process produces optimized binaries for all target platforms",
            "Integration with Python layers uses JSON-based IPC or gRPC",
            "Error handling propagates detailed error context up the stack",
            "Logging provides structured logs compatible with analysis tools",
            "Performance benchmarks show >10x speedup vs. Python for CPU-bound tasks"
          ],
          "implementation": {
            "frontend": [
              "CLI help text and usage documentation",
              "Progress bars for long-running operations",
              "Color-coded output for status messages"
            ],
            "backend": [
              "cmd/context-engine/main.go: CLI entry point and command routing",
              "cmd/context-engine/commands.go: subcommand implementations (init, add, remove, list, search)",
              "cmd/loop-runner/main.go: loop runner CLI entry point",
              "cmd/loop-runner/execute.go: autonomous loop execution logic",
              "internal/planning/planner.go: planning algorithm implementation",
              "internal/planning/orchestrator.go: orchestration coordination",
              "internal/cli/parser.go: argument parsing with validation",
              "internal/cli/flags.go: flag definitions and defaults",
              "internal/exec/executor.go: process execution with timeout support",
              "internal/exec/command.go: command building and execution",
              "internal/models/context.go: Context struct and methods",
              "internal/models/plan.go: Plan and Step structs",
              "internal/models/result.go: ExecutionResult structs",
              "internal/path/resolver.go: path resolution logic",
              "internal/paths/normalize.go: path normalization",
              "internal/fs/atomic.go: atomic file operations",
              "internal/fs/watch.go: file system watching",
              "internal/concurrent/pool.go: worker pool implementation",
              "internal/concurrent/pipeline.go: concurrent pipeline execution",
              "internal/json/marshal.go: efficient JSON marshaling",
              "internal/jsonutil/schema.go: JSON schema validation",
              "internal/build/compiler.go: build automation logic",
              "Makefile: build targets for all platforms",
              "build-and-install.sh: installation automation"
            ],
            "middleware": [
              "CLI middleware for logging and telemetry",
              "Authentication middleware for secure operations",
              "Rate limiting middleware for resource-intensive operations",
              "Input sanitization middleware"
            ],
            "shared": [
              "internal/models/context.go: Context, ContextItem, WorkingContext structs",
              "internal/models/plan.go: Plan, Step, Dependency structs",
              "internal/models/result.go: Result, Error, Status types",
              "internal/models/config.go: Configuration structs",
              "Constants in internal/constants/: timeouts, limits, defaults",
              "Utility functions in internal/util/: string manipulation, validation",
              "Error types in internal/errors/: custom error definitions"
            ]
          },
          "testable_properties": [],
          "function_id": "GoRuntime.implementRuntimeExecution",
          "related_concepts": [
            "Go runtime",
            "concurrent execution",
            "CLI tools",
            "planning orchestration",
            "context engine",
            "loop runner",
            "file system operations",
            "path utilities",
            "build automation",
            "cross-platform support"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.5",
          "description": "Implement mechanisms across all four layers to prevent context degradation during long autonomous sessions through intelligent context management, prioritization, and persistence",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "Layer 1 implements LRU cache with priority scoring to retain most relevant context items",
            "Layer 1 compresses old context items using summarization or encoding techniques",
            "Layer 2 tracks context usage patterns and pre-loads anticipated context",
            "Layer 2 implements context pruning strategy based on phase transitions",
            "Layer 3 maintains phase-specific context buckets with automatic cleanup",
            "Layer 3 implements context refresh mechanism when staleness is detected",
            "Layer 4 provides fast persistence and retrieval for context snapshots",
            "Cross-layer context synchronization prevents duplication and inconsistency",
            "Relevance scoring algorithm prioritizes context based on: recency, frequency, phase-relevance",
            "Context compression reduces token count by 50% while preserving semantic meaning",
            "Automatic context pruning triggers when usage exceeds 80% of limits",
            "Context refresh mechanism re-indexes and re-prioritizes every N steps (N=10)",
            "Session continuity testing shows no information loss after 100+ step execution",
            "Memory usage remains stable (no leaks) over 1000+ operation cycles",
            "Context retrieval latency stays <100ms even with 50,000+ historical items",
            "Integration tests simulate 8-hour autonomous sessions without degradation",
            "Monitoring dashboard shows context health metrics: size, age, relevance distribution",
            "Context snapshots enable resuming sessions with full context restoration",
            "Documentation explains context lifecycle and degradation prevention strategies",
            "Performance benchmarks show <5% accuracy degradation over long sessions"
          ],
          "implementation": {
            "frontend": [
              "Context health dashboard showing size, age, and relevance metrics",
              "Visualization of context pruning and refresh operations",
              "Alerts for context degradation warnings"
            ],
            "backend": [
              "context_window_array/prioritization.py: score_context_relevance(item, current_phase) -> float",
              "context_window_array/compression.py: compress_context(items) -> CompressedContext",
              "context_window_array/pruning.py: prune_low_priority_items(threshold)",
              "context_window_array/staleness.py: detect_stale_context() -> List[ContextItem]",
              "planning_pipeline/context_predictor.py: predict_needed_context(next_steps) -> List[ContextId]",
              "planning_pipeline/context_preloader.py: preload_context(predicted_items)",
              "planning_pipeline/phase_context_manager.py: cleanup_phase_context(completed_phase)",
              "silmari_rlm_act/context/refresh.py: refresh_context(phase) -> RefreshResult",
              "silmari_rlm_act/context/bucket_manager.py: manage_phase_buckets()",
              "go/internal/persistence/snapshot.go: CreateSnapshot(context) -> SnapshotId",
              "go/internal/persistence/restore.go: RestoreSnapshot(id) -> Context",
              "Cross-layer coordinator service: sync_context_across_layers()",
              "Deduplication service: deduplicate_context() -> DeduplicationReport",
              "Monitoring service: collect_context_metrics() -> ContextHealthReport",
              "Alerting service: check_degradation_threshold() -> Alert"
            ],
            "middleware": [
              "Context validation middleware ensuring integrity",
              "Compression middleware for automatic context encoding",
              "Synchronization middleware for cross-layer updates",
              "Monitoring middleware for metrics collection"
            ],
            "shared": [
              "models for ContextPriority(score, factors, timestamp)",
              "models for CompressedContext(original_size, compressed_size, encoding)",
              "models for ContextHealth(size, age, relevance_score, staleness)",
              "models for PruningStrategy(threshold, policy, retained_count)",
              "models for RefreshResult(items_refreshed, items_removed, new_items)",
              "models for ContextSnapshot(id, timestamp, context_state, metadata)",
              "Utility functions for relevance scoring algorithms",
              "Utility functions for compression/decompression",
              "Constants for thresholds, limits, refresh intervals"
            ]
          },
          "testable_properties": [],
          "function_id": "MemoryArchitecture.preventContextDegradation",
          "related_concepts": [
            "context degradation prevention",
            "long-term session management",
            "context prioritization",
            "memory persistence",
            "context compression",
            "relevance scoring",
            "context pruning",
            "cross-layer coordination",
            "session continuity"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_003",
      "description": "The system must provide context window array management with batching, indexing, storage, and working context handling for efficient large context management",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_003.1",
          "description": "Implement context batching logic to split large context data into manageable chunks that fit within LLM token limits while maintaining semantic coherence and relationships between context items",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "Batching logic must accurately count tokens for different LLM models (Claude, GPT-4, etc.)",
            "Batches must not exceed configurable token limits (default: model max - safety margin)",
            "Context items must be grouped semantically to avoid splitting related information",
            "Batch boundaries must respect document/code block/function boundaries",
            "System must handle edge cases where single context item exceeds batch size",
            "Batching performance must process 10MB of context data in under 2 seconds",
            "Must provide batch metadata (size, token count, item count, priority score)",
            "Must support different batching strategies (sequential, priority-based, semantic-based)",
            "Must maintain ordering relationships between dependent context items",
            "Must provide API to manually override automatic batch boundaries"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "TokenCounter class with multi-model support (tiktoken for OpenAI, Anthropic tokenizer)",
              "BatchStrategy abstract class with implementations: SequentialBatchStrategy, PriorityBatchStrategy, SemanticBatchStrategy",
              "ContextBatcher class with batch() method accepting context items and returning batches",
              "BatchMetadata dataclass with fields: batch_id, token_count, item_count, priority, boundaries",
              "create_batches() function that applies batching strategy to context array",
              "optimize_batch_size() function that dynamically adjusts batch size based on content",
              "merge_small_batches() function to consolidate undersized batches",
              "split_oversized_items() function to handle context items exceeding limits",
              "validate_batch_boundaries() function to ensure semantic coherence"
            ],
            "middleware": [
              "Batch size validation against model-specific token limits",
              "Context item size validation before batching",
              "Strategy selection based on context type and use case"
            ],
            "shared": [
              "ContextItem dataclass with fields: id, content, type, priority, dependencies, metadata",
              "Batch dataclass with fields: id, items, token_count, strategy, metadata",
              "BatchConfig dataclass with fields: max_tokens, strategy, safety_margin, allow_splitting",
              "TokenCount type alias for int with validation",
              "BatchingStrategy enum: SEQUENTIAL, PRIORITY, SEMANTIC, CUSTOM",
              "get_model_token_limit() utility function returning limits for each model",
              "count_tokens() utility function with caching for performance"
            ]
          },
          "testable_properties": [],
          "function_id": "ContextWindowArray.Batching",
          "related_concepts": [
            "token_counting",
            "semantic_chunking",
            "context_overflow_handling",
            "batch_size_optimization",
            "context_prioritization",
            "sliding_window_algorithm"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_003.2",
          "description": "Implement efficient search indexing system to enable fast retrieval of relevant context items based on semantic similarity, keywords, file paths, and metadata queries across large context arrays",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "Search index must support semantic similarity search using vector embeddings",
            "Search index must support keyword-based search with inverted index",
            "Search index must support file path and pattern matching queries",
            "Search index must support metadata filtering (date, type, priority, tags)",
            "Search queries must return results in under 100ms for indexes with 10,000+ items",
            "Index must automatically update when context items are added/removed/modified",
            "Search results must be ranked by relevance score with configurable scoring algorithm",
            "Index must persist to disk and load efficiently on startup",
            "Must support hybrid search combining semantic and keyword signals",
            "Must provide index statistics (size, item count, last updated, hit rate)",
            "Must handle concurrent read operations without blocking",
            "Must support incremental index updates without full rebuild"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "SearchIndex class with add(), remove(), update(), search() methods",
              "VectorIndexer class using embedding model (sentence-transformers or OpenAI embeddings)",
              "KeywordIndexer class with inverted index implementation",
              "MetadataIndexer class with filtering capabilities",
              "HybridSearchEngine class combining multiple search strategies",
              "RelevanceScorer class with configurable scoring algorithms (BM25, cosine similarity, hybrid)",
              "IndexBuilder class for initial index construction from context array",
              "IndexUpdater class for incremental updates",
              "IndexPersistence class for disk serialization (using pickle, msgpack, or SQLite)",
              "QueryParser class to parse and optimize search queries",
              "IndexStatistics class tracking performance metrics",
              "EmbeddingCache class to cache vector embeddings and avoid recomputation"
            ],
            "middleware": [
              "Query validation and sanitization",
              "Rate limiting for expensive embedding operations",
              "Caching layer for frequently accessed search results",
              "Concurrent read lock management"
            ],
            "shared": [
              "SearchQuery dataclass with fields: text, filters, limit, offset, strategy, scoring_config",
              "SearchResult dataclass with fields: item, score, rank, highlights, explanation",
              "IndexConfig dataclass with fields: embedding_model, index_type, persistence_path, update_mode",
              "SearchStrategy enum: SEMANTIC, KEYWORD, HYBRID, METADATA",
              "ScoringAlgorithm enum: BM25, COSINE_SIMILARITY, HYBRID_WEIGHTED",
              "IndexMetrics dataclass with fields: size_bytes, item_count, last_updated, query_count, avg_query_time",
              "compute_similarity() utility function for vector similarity",
              "tokenize_query() utility function for keyword extraction",
              "normalize_score() utility function to normalize relevance scores to 0-1 range"
            ]
          },
          "testable_properties": [],
          "function_id": "ContextWindowArray.SearchIndex",
          "related_concepts": [
            "vector_embeddings",
            "semantic_search",
            "inverted_index",
            "fuzzy_matching",
            "relevance_ranking",
            "query_optimization",
            "index_persistence"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_003.3",
          "description": "Implement persistent storage management system for context arrays with efficient serialization, versioning, compression, and retrieval mechanisms to handle large context datasets across sessions",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "Storage system must persist context arrays to disk with configurable backend (filesystem, SQLite, Redis)",
            "Storage must support efficient serialization formats (JSON, MessagePack, Protocol Buffers)",
            "Storage must implement compression to reduce disk usage (gzip, zstd, lz4)",
            "Storage must version context arrays to support rollback and history tracking",
            "Storage must handle context arrays up to 1GB with sub-second save/load times",
            "Storage must implement LRU cache eviction for memory-constrained environments",
            "Storage must provide atomic write operations to prevent corruption",
            "Storage must support incremental saves to avoid rewriting entire array",
            "Storage must validate data integrity using checksums (SHA-256)",
            "Storage must provide migration path for schema changes",
            "Storage must support concurrent read access without blocking",
            "Storage must implement automatic backup and recovery mechanisms",
            "Storage must provide storage metrics (size, version, last_saved, compression_ratio)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "StorageManager class with save(), load(), delete(), list_versions() methods",
              "StorageBackend abstract class with implementations: FileSystemBackend, SQLiteBackend, RedisBackend",
              "Serializer abstract class with implementations: JSONSerializer, MessagePackSerializer, ProtobufSerializer",
              "Compressor class with support for gzip, zstd, lz4 compression algorithms",
              "VersionManager class tracking context array versions with metadata",
              "CacheManager class implementing LRU eviction policy",
              "ChecksumValidator class for data integrity verification",
              "AtomicWriter class ensuring atomic write operations using temp files",
              "IncrementalSaver class saving only changed portions of context array",
              "MigrationManager class handling schema migrations between versions",
              "BackupManager class for automated backups with retention policies",
              "StorageMetrics class tracking storage usage and performance"
            ],
            "middleware": [
              "Write lock management for atomic operations",
              "Validation of storage paths and permissions",
              "Compression level selection based on CPU/storage trade-offs",
              "Automatic cleanup of old versions based on retention policy"
            ],
            "shared": [
              "StorageConfig dataclass with fields: backend, serializer, compression, version_limit, cache_size",
              "StorageBackendType enum: FILESYSTEM, SQLITE, REDIS, CUSTOM",
              "SerializationFormat enum: JSON, MESSAGEPACK, PROTOBUF, PICKLE",
              "CompressionAlgorithm enum: NONE, GZIP, ZSTD, LZ4",
              "VersionMetadata dataclass with fields: version_id, timestamp, checksum, size, changes",
              "StorageMetrics dataclass with fields: total_size, version_count, compression_ratio, last_saved",
              "generate_checksum() utility function for SHA-256 hashing",
              "atomic_write() utility function for safe file writing",
              "compress_data() and decompress_data() utility functions",
              "validate_storage_path() utility function for path validation"
            ]
          },
          "testable_properties": [],
          "function_id": "ContextWindowArray.Storage",
          "related_concepts": [
            "data_persistence",
            "serialization",
            "compression",
            "versioning",
            "cache_eviction",
            "storage_optimization",
            "backup_recovery"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_003.4",
          "description": "Implement working context handler that maintains the active subset of context items currently relevant to the agent's task, with automatic pruning, priority management, and dynamic updates based on task progression",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "Working context must maintain active context items within configurable token budget",
            "Working context must automatically prune low-priority or stale items when approaching limits",
            "Working context must dynamically load relevant items based on current task/phase",
            "Working context must track item access patterns to inform priority decisions",
            "Working context must support manual pinning of critical items that should not be pruned",
            "Working context must provide add(), remove(), get(), update_priority() operations in O(log n) time",
            "Working context must emit events when items are added/removed for observability",
            "Working context must persist state between agent restarts within same session",
            "Working context must integrate with search index for relevance-based loading",
            "Working context must maintain coherence by including dependent context items",
            "Working context must provide metrics (utilization, prune_count, cache_hits, avg_age)",
            "Working context must support context snapshots for rollback scenarios"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "WorkingContext class managing active context items with priority queue",
              "ContextPruner class implementing pruning strategies (LRU, priority-based, age-based)",
              "PriorityManager class updating item priorities based on access patterns and relevance",
              "ContextLoader class dynamically loading items from storage based on task needs",
              "DependencyResolver class ensuring dependent items are loaded together",
              "ContextSnapshot class for capturing and restoring working context state",
              "EventEmitter class publishing context change events (item_added, item_removed, item_updated)",
              "AccessTracker class monitoring item access frequency and recency",
              "ContextCoherence class validating that context remains semantically coherent",
              "WorkingContextMetrics class tracking utilization and performance",
              "PinManager class handling manually pinned items that bypass pruning",
              "ContextBudgetManager class enforcing token limits and memory constraints"
            ],
            "middleware": [
              "Priority validation ensuring priority values are within valid range",
              "Dependency validation preventing orphaned dependent items",
              "Token budget enforcement with configurable limits",
              "Event throttling for high-frequency context updates"
            ],
            "shared": [
              "WorkingContextItem dataclass with fields: context_item, priority, last_accessed, access_count, pinned, dependencies",
              "PruningStrategy enum: LRU, PRIORITY_BASED, AGE_BASED, HYBRID",
              "ContextEvent dataclass with fields: event_type, item_id, timestamp, metadata",
              "EventType enum: ITEM_ADDED, ITEM_REMOVED, ITEM_UPDATED, PRIORITY_CHANGED, PRUNED",
              "WorkingContextConfig dataclass with fields: max_tokens, pruning_strategy, auto_load, persistence_enabled",
              "WorkingContextMetrics dataclass with fields: item_count, token_usage, utilization_percent, prune_count, load_count",
              "ContextSnapshot dataclass with fields: snapshot_id, timestamp, items, metrics",
              "calculate_priority() utility function computing item priority from access patterns",
              "is_stale() utility function determining if context item is outdated",
              "resolve_dependencies() utility function finding all dependent items"
            ]
          },
          "testable_properties": [],
          "function_id": "ContextWindowArray.WorkingContext",
          "related_concepts": [
            "context_relevance",
            "memory_management",
            "priority_queue",
            "context_expiration",
            "task_tracking",
            "dynamic_context_loading",
            "context_freshness"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_003.5",
          "description": "Implement specialized context manager for implementation phases that maintains code structure, test results, error history, and phase-specific context with automatic relevance filtering and task-aware context selection",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "Implementation context must track current implementation phase (planning, coding, testing, debugging)",
            "Implementation context must maintain code structure context (files being modified, functions being implemented)",
            "Implementation context must store test execution history with pass/fail status and error messages",
            "Implementation context must track error history with stack traces, resolution attempts, and outcomes",
            "Implementation context must automatically include relevant context based on current phase",
            "Implementation context must integrate with batching to create phase-appropriate context batches",
            "Implementation context must support context templates for common implementation patterns",
            "Implementation context must maintain bidirectional links between code, tests, and errors",
            "Implementation context must provide phase transition hooks to update context",
            "Implementation context must track implementation progress (completed steps, remaining tasks)",
            "Implementation context must support context inheritance from parent planning context",
            "Implementation context must export phase summaries for documentation and review",
            "Implementation context must validate context completeness for each phase"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "ImplementationContext class extending WorkingContext with phase-specific capabilities",
              "PhaseManager class tracking current phase and handling phase transitions",
              "CodeStructureTracker class maintaining file/function/class hierarchy being implemented",
              "TestResultStore class storing test execution history with timestamps and outcomes",
              "ErrorHistoryTracker class recording errors, stack traces, and resolution attempts",
              "ContextTemplateEngine class providing pre-built context templates for common scenarios",
              "RelevanceFilter class selecting context items based on current phase and task",
              "ContextLinker class maintaining relationships between code, tests, errors, and documentation",
              "PhaseTransitionHandler class updating context when moving between phases",
              "ProgressTracker class monitoring implementation progress against plan",
              "ContextInheritance class managing parent-child context relationships",
              "PhaseSummaryGenerator class exporting phase summaries and context snapshots",
              "ContextValidator class ensuring required context is present for each phase"
            ],
            "middleware": [
              "Phase validation ensuring valid phase transitions",
              "Context completeness checks before phase transitions",
              "Automatic context pruning of phase-irrelevant items",
              "Template parameter validation for context templates"
            ],
            "shared": [
              "ImplementationPhase enum: PLANNING, CODING, TESTING, DEBUGGING, REVIEWING, COMPLETED",
              "CodeStructure dataclass with fields: file_path, functions, classes, dependencies, modified_lines",
              "TestResult dataclass with fields: test_name, status, duration, error_message, timestamp",
              "ErrorRecord dataclass with fields: error_type, message, stack_trace, context_snapshot, resolution_attempts, resolved",
              "PhaseContext dataclass with fields: phase, required_items, optional_items, templates",
              "ContextTemplate dataclass with fields: template_id, name, description, required_fields, default_items",
              "ImplementationProgress dataclass with fields: total_steps, completed_steps, current_step, blockers",
              "PhaseSummary dataclass with fields: phase, duration, changes_made, tests_run, errors_encountered, next_phase",
              "ContextLink dataclass with fields: source_item, target_item, link_type, bidirectional",
              "LinkType enum: CODE_TO_TEST, TEST_TO_ERROR, ERROR_TO_FIX, CODE_TO_DOC",
              "get_phase_template() utility function returning default template for phase",
              "filter_by_phase() utility function selecting relevant context for phase",
              "validate_phase_context() utility function checking context completeness"
            ]
          },
          "testable_properties": [],
          "function_id": "ContextWindowArray.ImplementationContext",
          "related_concepts": [
            "phase_aware_context",
            "code_structure_tracking",
            "test_result_history",
            "error_tracking",
            "task_context_mapping",
            "implementation_state",
            "context_templating"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_004",
      "description": "The system must implement autonomous TDD pipeline following Research-Learn-Model-Act phases with comprehensive phase management and checkpointing",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_004.1",
          "description": "Implement the main RLM-ACT pipeline orchestrator that coordinates all phases (Research, Learn, Model, Act) with state management, phase transitions, checkpointing, and error recovery",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "Pipeline initializes with valid configuration from silmari_rlm_act/models.py",
            "Pipeline executes phases in correct order: Research \u2192 Learn \u2192 Model \u2192 Act",
            "Each phase transition is validated and logged with timestamps",
            "Pipeline state is persisted to .rlm-act-checkpoints/ after each phase",
            "Pipeline can resume from any checkpoint after interruption",
            "Pipeline emits events for phase start, phase complete, phase error, and pipeline complete",
            "Pipeline handles phase failures with retry logic (configurable max retries)",
            "Pipeline validates phase outputs before proceeding to next phase",
            "Pipeline aggregates metrics from all phases (duration, token usage, success rate)",
            "Pipeline supports dry-run mode for testing without side effects",
            "Pipeline integrates with planning_pipeline/autonomous_loop.py for orchestration",
            "Pipeline provides CLI interface through silmari_rlm_act/cli.py",
            "All phase transitions are atomic and rollback-safe"
          ],
          "implementation": {
            "frontend": [
              "N/A - CLI-based system with terminal output"
            ],
            "backend": [
              "RLMActPipeline class with execute_pipeline() method in silmari_rlm_act/pipeline.py",
              "Phase execution coordinator that calls each phase implementation",
              "State machine for tracking current phase and valid transitions",
              "Checkpoint save/load logic integrating with checkpoint system",
              "Phase validator that checks prerequisites and outputs",
              "Event emitter for pipeline lifecycle events",
              "Error handler with retry logic and graceful degradation",
              "Metrics aggregator collecting stats from all phases",
              "Configuration loader from pyproject.toml and environment",
              "Integration hooks for autonomous_loop.py callbacks"
            ],
            "middleware": [
              "Phase transition validator ensuring prerequisites are met",
              "Checkpoint middleware for automatic state persistence",
              "Error recovery middleware with exponential backoff",
              "Logging middleware for structured pipeline events",
              "Resource cleanup middleware for phase teardown"
            ],
            "shared": [
              "PipelineState model tracking current phase and metadata",
              "PipelineConfig model with phase configurations and retry settings",
              "PhaseResult model with outputs, metrics, and success status",
              "PipelineEvent enum for all event types",
              "TransitionValidator utility for phase flow validation",
              "CheckpointManager interface for state persistence",
              "MetricsCollector utility for aggregating phase metrics"
            ]
          },
          "testable_properties": [],
          "function_id": "RLMActPipeline.execute_pipeline",
          "related_concepts": [
            "Pipeline orchestration",
            "Phase state machine",
            "Context persistence",
            "Error recovery",
            "Phase transitions",
            "Event emission",
            "Progress tracking",
            "Checkpoint integration"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_004.2",
          "description": "Implement the Research phase that analyzes requirements, explores codebase context, identifies relevant files, and generates comprehensive research documentation",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "Research phase accepts requirement specification as input",
            "Phase uses context_window_array/ to search and index relevant code",
            "Phase identifies all files related to the requirement using search_index.py",
            "Phase scores file relevance based on imports, function calls, and type usage",
            "Phase generates research document in thoughts/searchable/research/ with timestamp",
            "Research document includes: summary, relevant files, architecture patterns, dependencies, open questions",
            "Phase integrates with baml_client for LLM-powered code analysis",
            "Phase uses Glob and Grep tools to find code patterns",
            "Phase analyzes existing tests to understand testing patterns",
            "Phase identifies integration points with planning_pipeline/",
            "Phase outputs ResearchResult with file list, analysis, and confidence scores",
            "Research phase completes within configurable timeout (default 5 minutes)",
            "Phase creates checkpoint after successful completion"
          ],
          "implementation": {
            "frontend": [
              "N/A - CLI-based system with terminal output and markdown reports"
            ],
            "backend": [
              "ResearchPhase class with execute() method in silmari_rlm_act/phases/research.py",
              "Codebase explorer using context_window_array/search_index.py",
              "File relevance scorer using graph analysis and static analysis",
              "Research document generator creating markdown reports",
              "BAML function caller for LLM-powered code understanding",
              "Pattern detector identifying architectural patterns (MVC, pipeline, etc)",
              "Dependency analyzer building import graph",
              "Test pattern analyzer examining existing test structure",
              "Integration point identifier for cross-module dependencies",
              "Context compiler aggregating findings into structured output"
            ],
            "middleware": [
              "Context window validator ensuring research fits within token limits",
              "File access validator checking permissions before reading",
              "Timeout enforcer with configurable limits",
              "Progress reporter emitting incremental research status"
            ],
            "shared": [
              "ResearchResult model with file_list, analysis_summary, patterns_detected, dependencies, open_questions",
              "FileRelevanceScore model with file_path, score, reasoning",
              "ArchitecturePattern enum defining detectable patterns",
              "ResearchConfig model with timeout, max_files, search_depth",
              "CodebaseIndex wrapper around context_window_array indexing",
              "ResearchDocumentTemplate for consistent markdown generation",
              "DependencyGraph model representing import relationships"
            ]
          },
          "testable_properties": [],
          "function_id": "ResearchPhase.execute",
          "related_concepts": [
            "Codebase exploration",
            "Context window management",
            "File relevance scoring",
            "Research documentation generation",
            "Dependency analysis",
            "Architecture pattern detection",
            "Context indexing",
            "Search optimization"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_004.3",
          "description": "Implement the TDD planning phase that decomposes requirements into test-driven implementation steps, generates test specifications, and creates detailed implementation plans",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "TDD planning phase accepts ResearchResult from research phase as input",
            "Phase decomposes requirement into granular implementation steps",
            "Each step has corresponding test specification (unit, integration, or e2e)",
            "Phase generates test plan document with test names, descriptions, and assertions",
            "Phase creates implementation plan with ordered steps and dependencies",
            "Phase identifies test fixtures, mocks, and test data requirements",
            "Phase maps acceptance criteria from requirement to specific test cases",
            "Phase determines test execution order based on dependencies",
            "Phase estimates complexity and effort for each step",
            "Phase validates that test coverage meets requirement completeness",
            "Phase outputs TDDPlan with step list, test specifications, and execution order",
            "Planning phase integrates with planning_pipeline/decomposition.py patterns",
            "Phase creates checkpoint after generating complete plan"
          ],
          "implementation": {
            "frontend": [
              "N/A - CLI-based system with markdown plan output"
            ],
            "backend": [
              "TDDPlanningPhase class with execute() method in silmari_rlm_act/phases/tdd_planning.py",
              "Requirement decomposer breaking down features into implementation steps",
              "Test specification generator creating detailed test cases",
              "Test hierarchy builder organizing unit \u2192 integration \u2192 e2e tests",
              "Dependency analyzer determining step execution order",
              "Fixture identifier analyzing required test setup and teardown",
              "Coverage analyzer ensuring all acceptance criteria have tests",
              "Complexity estimator scoring implementation difficulty",
              "Plan validator checking completeness and consistency",
              "Plan document generator creating markdown implementation guide"
            ],
            "middleware": [
              "Decomposition validator ensuring steps are atomic and testable",
              "Coverage validator checking test-to-requirement mapping",
              "Dependency resolver detecting circular dependencies",
              "Complexity threshold enforcer flagging overly complex steps"
            ],
            "shared": [
              "TDDPlan model with steps, test_specs, execution_order, fixtures",
              "ImplementationStep model with description, test_spec, dependencies, complexity",
              "TestSpecification model with test_name, test_type, assertions, setup, teardown",
              "TestType enum (unit, integration, e2e, property)",
              "DependencyEdge model representing step dependencies",
              "FixtureRequirement model with fixture_name, scope, setup_code",
              "CoverageMap linking acceptance criteria to test cases",
              "ComplexityScore enum (trivial, simple, moderate, complex)"
            ]
          },
          "testable_properties": [],
          "function_id": "TDDPlanningPhase.execute",
          "related_concepts": [
            "Test-driven development",
            "Requirement decomposition",
            "Test specification generation",
            "Implementation planning",
            "Acceptance criteria mapping",
            "Test hierarchy design",
            "Coverage planning",
            "Integration planning"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_004.4",
          "description": "Implement the Implementation phase that executes TDD plan steps, runs tests iteratively, generates code, validates implementation, and ensures all tests pass",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "Implementation phase accepts TDDPlan from planning phase as input",
            "Phase executes implementation steps in dependency order from plan",
            "For each step: write test \u2192 run test (expect fail) \u2192 implement code \u2192 run test (expect pass)",
            "Phase uses planning_pipeline/claude_runner.py to execute Claude Code sessions",
            "Phase integrates with agents/test-runner.md for test execution",
            "Phase validates each implementation step against acceptance criteria",
            "Phase handles test failures with automatic debugging using agents/debugger.md",
            "Phase tracks implementation progress and updates checkpoint after each step",
            "Phase runs agents/code-reviewer.md after each implementation",
            "Phase ensures all tests pass before marking step as complete",
            "Phase aggregates code changes and generates implementation summary",
            "Phase validates final implementation with agents/feature-verifier.md",
            "Implementation phase outputs ImplementationResult with code changes, test results, and verification status"
          ],
          "implementation": {
            "frontend": [
              "N/A - CLI-based system with terminal output and progress tracking"
            ],
            "backend": [
              "ImplementationPhase class with execute() method in silmari_rlm_act/phases/implementation.py",
              "Step executor implementing red-green-refactor cycle",
              "Claude Code session manager wrapping planning_pipeline/claude_runner.py",
              "Test executor running tests via agents/test-runner.md",
              "Test failure analyzer using agents/debugger.md for diagnostics",
              "Code reviewer integrating agents/code-reviewer.md",
              "Feature verifier using agents/feature-verifier.md for end-to-end validation",
              "Progress tracker updating checkpoint after each step",
              "Code change aggregator collecting all modifications",
              "Implementation validator checking acceptance criteria satisfaction",
              "Quality gate enforcer ensuring code quality standards"
            ],
            "middleware": [
              "Test execution middleware capturing test output and exit codes",
              "Code review middleware enforcing review before proceeding",
              "Verification middleware running feature verification at phase end",
              "Checkpoint middleware persisting progress after each step",
              "Rollback middleware reverting changes on critical failures"
            ],
            "shared": [
              "ImplementationResult model with code_changes, test_results, verification_status, metrics",
              "StepExecution model tracking test writes, test runs, implementation, review",
              "TestResult model with test_name, status, output, duration",
              "CodeChange model with file_path, change_type, diff, line_count",
              "VerificationStatus enum (not_started, in_progress, passed, failed)",
              "ImplementationMetrics model with steps_completed, tests_passed, review_issues, duration",
              "FailureAnalysis model with error_type, root_cause, suggested_fixes",
              "QualityMetrics model with code_coverage, complexity, maintainability"
            ]
          },
          "testable_properties": [],
          "function_id": "ImplementationPhase.execute",
          "related_concepts": [
            "Test-driven implementation",
            "Red-green-refactor cycle",
            "Code generation",
            "Test execution",
            "Implementation validation",
            "Incremental development",
            "Test failure analysis",
            "Code quality checking"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_004.5",
          "description": "Implement the checkpoint system that saves pipeline state, phase results, and context to disk for recovery and resume functionality",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "Checkpoint system saves state to .rlm-act-checkpoints/ directory",
            "Each checkpoint includes: pipeline state, current phase, phase results, timestamp, git commit hash",
            "Checkpoints are saved atomically using temp file + rename pattern",
            "Checkpoint files are named with timestamp and phase: YYYYMMDD-HHMMSS-phase.json",
            "System supports loading latest checkpoint on pipeline resume",
            "System validates checkpoint integrity before loading (schema validation)",
            "System supports listing available checkpoints for selection",
            "System automatically cleans up old checkpoints (configurable retention)",
            "Checkpoints include context window state from context_window_array/",
            "System integrates with planning_pipeline/checkpoint_manager.py patterns",
            "Checkpoint load restores full pipeline state including phase progress",
            "System handles checkpoint corruption gracefully with fallback options"
          ],
          "implementation": {
            "frontend": [
              "N/A - CLI-based system with checkpoint list/restore commands"
            ],
            "backend": [
              "CheckpointManager class in silmari_rlm_act/checkpoints/__init__.py",
              "Checkpoint serializer converting pipeline state to JSON",
              "Checkpoint deserializer loading and validating checkpoint files",
              "Atomic file writer using temp files and atomic renames",
              "Checkpoint lister scanning .rlm-act-checkpoints/ directory",
              "Checkpoint validator checking schema and data integrity",
              "Checkpoint cleanup service removing old checkpoints based on retention policy",
              "State restorer rebuilding pipeline state from checkpoint",
              "Context saver persisting context_window_array state",
              "Git state capturer recording current commit and branch"
            ],
            "middleware": [
              "Serialization middleware handling complex object serialization",
              "Validation middleware checking checkpoint schema compliance",
              "Compression middleware reducing checkpoint file size",
              "Encryption middleware (optional) for sensitive data protection"
            ],
            "shared": [
              "Checkpoint model with pipeline_state, phase_results, timestamp, git_commit, context_state",
              "CheckpointMetadata model with checkpoint_id, phase, size, valid",
              "CheckpointConfig model with save_dir, retention_days, compression_enabled",
              "StateSerializer utility for complex object serialization",
              "CheckpointValidator utility for schema and integrity validation",
              "RetentionPolicy utility for cleanup logic",
              "AtomicWriter utility for safe file operations"
            ]
          },
          "testable_properties": [],
          "function_id": "CheckpointSystem.save_checkpoint",
          "related_concepts": [
            "State persistence",
            "Crash recovery",
            "Resume capability",
            "Incremental saves",
            "State validation",
            "Checkpoint versioning",
            "Atomic writes",
            "Storage optimization"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_005",
      "description": "The system must provide Go-based runtime with CLI tools for context engine and loop runner, concurrent execution management, and cross-platform support for darwin, linux, and windows",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_005.1",
          "description": "Implement context-engine CLI tool in go/cmd/context-engine/ that provides command-line interface for managing context arrays, batching operations, search indexing, and working context operations with support for flags, subcommands, and configuration loading",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [
            "CLI accepts commands for context operations: init, add, search, batch, list, clear, export, import",
            "CLI supports global flags: --config, --verbose, --debug, --output-format (json|text|table)",
            "CLI loads configuration from file specified via --config flag or default locations (~/.context-engine.yaml, .context-engine.yaml)",
            "CLI validates all input arguments and provides helpful error messages with usage examples",
            "CLI provides help text via --help flag for all commands and subcommands",
            "CLI supports batch operations via --batch flag or batch subcommand for processing multiple items",
            "CLI implements search command with filters: --query, --limit, --offset, --type",
            "CLI returns appropriate exit codes: 0 for success, 1 for user errors, 2 for system errors",
            "CLI outputs results in requested format (json, text, table) based on --output-format flag",
            "CLI logs all operations to stderr when --verbose or --debug flags are set",
            "CLI handles graceful shutdown on SIGINT and SIGTERM signals",
            "CLI persists context state to disk and loads on subsequent runs",
            "CLI command execution time is logged in debug mode",
            "CLI provides version information via --version flag"
          ],
          "implementation": {
            "frontend": [
              "CLI interface with command structure and flag parsing",
              "Interactive prompt mode for guided operations",
              "Progress indicators for long-running operations",
              "Colored output for success/error/warning messages",
              "Table formatting for list and search results",
              "Help documentation with examples for each command"
            ],
            "backend": [
              "Command dispatcher routing commands to appropriate handlers",
              "Context array management service interfacing with go/internal/planning/",
              "Batch processing service for bulk operations",
              "Search indexing service integration with context_window_array/search_index.py logic",
              "Configuration loader supporting YAML and JSON formats",
              "State persistence service for saving/loading context",
              "Logging service with configurable levels (info, debug, error)"
            ],
            "middleware": [
              "Input validation for all command arguments and flags",
              "Configuration validation ensuring required fields are present",
              "Error wrapping with contextual information for debugging",
              "Signal handling for graceful shutdown",
              "Command execution timeout handling with configurable limits"
            ],
            "shared": [
              "CLI models in go/internal/models/ for context data structures",
              "Path utilities in go/internal/path/ and go/internal/paths/",
              "JSON utilities in go/internal/json/ and go/internal/jsonutil/",
              "File system operations in go/internal/fs/",
              "Common error types and error handling patterns",
              "Configuration schema definition with validation rules",
              "CLI flag definitions shared across commands"
            ]
          },
          "testable_properties": [],
          "function_id": "ContextEngineCLI.main",
          "related_concepts": [
            "CLI argument parsing",
            "context window array management",
            "batch processing",
            "search indexing",
            "working context operations",
            "error handling and logging",
            "configuration file loading",
            "interactive mode"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_005.2",
          "description": "Implement loop-runner CLI tool in go/cmd/loop-runner/ that orchestrates autonomous development cycles, manages phase execution, coordinates with planning pipeline, handles checkpoints, and provides real-time monitoring with support for resume, pause, and status reporting",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [
            "CLI accepts commands: start, resume, pause, stop, status, list-checkpoints, restore",
            "CLI supports flags: --feature-spec, --checkpoint-dir, --max-iterations, --timeout, --parallel",
            "CLI starts autonomous loop with feature specification via --feature-spec flag or file path",
            "CLI creates checkpoints at phase boundaries and stores in --checkpoint-dir location",
            "CLI resumes from checkpoint when --resume flag provided with checkpoint ID",
            "CLI reports real-time status including current phase, progress percentage, elapsed time",
            "CLI handles pause requests gracefully, saving state before stopping",
            "CLI implements retry logic with exponential backoff for transient failures",
            "CLI limits loop iterations via --max-iterations flag to prevent infinite loops",
            "CLI executes phases in parallel when --parallel flag set and phases are independent",
            "CLI integrates with planning_pipeline/autonomous_loop.py for feature decomposition",
            "CLI integrates with silmari_rlm_act/pipeline.py for RLM-ACT phase execution",
            "CLI outputs structured logs in JSON format when --output-format=json specified",
            "CLI provides detailed error traces when phase execution fails",
            "CLI cleans up temporary resources on exit or error",
            "CLI validates feature specification format before starting loop",
            "CLI monitors timeout via --timeout flag and terminates loop if exceeded"
          ],
          "implementation": {
            "frontend": [
              "CLI interface with loop control commands",
              "Real-time progress display with phase status and percentage complete",
              "Interactive status dashboard showing current phase, steps completed, steps remaining",
              "Colored output for phase transitions and status changes",
              "Live log streaming with filtering by log level",
              "Checkpoint list view with creation time and description"
            ],
            "backend": [
              "Loop orchestrator service managing autonomous development cycle",
              "Phase executor service coordinating decomposition, research, learning, modeling, acting phases",
              "Checkpoint manager service creating, storing, and restoring checkpoints",
              "Status reporter service providing real-time progress updates",
              "Integration adapter for planning_pipeline Python modules",
              "Integration adapter for silmari_rlm_act Python modules",
              "State machine implementation tracking loop state transitions",
              "Retry service with configurable retry policies and backoff strategies",
              "Timeout manager enforcing time limits on loop and phase execution"
            ],
            "middleware": [
              "Feature specification validation ensuring required fields present",
              "Checkpoint ID validation and existence checking",
              "Command authorization ensuring only valid state transitions",
              "Error recovery middleware handling transient and permanent failures",
              "Timeout enforcement middleware canceling long-running operations",
              "Resource cleanup middleware releasing resources on exit"
            ],
            "shared": [
              "Loop state models in go/internal/models/ defining phase states and transitions",
              "Checkpoint data structures with serialization/deserialization logic",
              "Phase execution models representing decomposition, implementation, verification phases",
              "Status report models with progress metrics and timing information",
              "Error types for loop-specific failures (phase failure, timeout, invalid state)",
              "Configuration models for loop parameters and retry policies",
              "Path utilities for checkpoint storage and feature spec loading"
            ]
          },
          "testable_properties": [],
          "function_id": "LoopRunnerCLI.main",
          "related_concepts": [
            "autonomous loop orchestration",
            "phase execution management",
            "checkpoint creation and restoration",
            "real-time progress monitoring",
            "error recovery and retry logic",
            "integration with planning_pipeline/",
            "integration with silmari_rlm_act/",
            "state machine implementation",
            "concurrent phase execution"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_005.3",
          "description": "Implement planning logic and orchestration in go/internal/planning/ that handles feature decomposition, step generation, dependency resolution, task prioritization, and coordination with Python planning pipeline with support for parallel planning and incremental updates",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [
            "Planning logic decomposes high-level feature specifications into executable steps",
            "Planning logic constructs dependency graph ensuring steps execute in correct order",
            "Planning logic prioritizes tasks based on dependencies, complexity, and impact",
            "Planning logic detects circular dependencies and reports errors",
            "Planning logic validates plan completeness ensuring all requirements covered",
            "Planning logic supports parallel planning for independent feature branches",
            "Planning logic updates plans incrementally when new requirements added",
            "Planning logic integrates with planning_pipeline/decomposition.py for Python-based decomposition",
            "Planning logic integrates with planning_pipeline/step_decomposition.py for step-level breakdown",
            "Planning logic serializes plans to JSON format for storage and transmission",
            "Planning logic deserializes plans from JSON maintaining all relationships",
            "Planning logic estimates complexity metrics for each step (time, effort, risk)",
            "Planning logic identifies critical path through dependency graph",
            "Planning logic supports plan versioning and history tracking",
            "Planning logic validates step definitions ensuring all required fields present",
            "Planning logic handles planning errors gracefully with detailed diagnostics"
          ],
          "implementation": {
            "frontend": [
              "N/A - internal service, no direct frontend interaction"
            ],
            "backend": [
              "Feature decomposer service breaking features into user stories and tasks",
              "Step generator service creating executable steps from tasks",
              "Dependency resolver service constructing and validating dependency graphs",
              "Task prioritizer service ordering tasks by dependencies and priority scores",
              "Parallel planner service executing independent planning operations concurrently",
              "Plan validator service checking completeness and consistency",
              "Python integration adapter calling planning_pipeline Python modules",
              "Plan serializer service converting plans to/from JSON",
              "Complexity estimator service calculating step complexity metrics",
              "Critical path analyzer service identifying bottlenecks in plan",
              "Plan versioning service tracking plan changes over time"
            ],
            "middleware": [
              "Feature specification validation before decomposition",
              "Dependency cycle detection preventing invalid plans",
              "Plan consistency validation ensuring all references are valid",
              "Complexity threshold validation warning on overly complex plans",
              "Error aggregation collecting all validation errors for reporting"
            ],
            "shared": [
              "Plan models in go/internal/models/ defining feature, task, and step structures",
              "Dependency graph data structures with topological sort implementation",
              "Priority scoring algorithms and configuration",
              "Complexity metrics definitions (time complexity, cognitive complexity, risk score)",
              "Plan serialization schemas with versioning support",
              "Error types for planning failures (circular dependency, incomplete spec, invalid reference)",
              "Path utilities in go/internal/paths/ for plan storage locations",
              "JSON utilities in go/internal/jsonutil/ for plan serialization",
              "Concurrent execution utilities in go/internal/concurrent/ for parallel planning"
            ]
          },
          "testable_properties": [],
          "function_id": "PlanningLogic.orchestrate",
          "related_concepts": [
            "feature decomposition algorithms",
            "dependency graph construction",
            "task prioritization strategies",
            "parallel planning execution",
            "incremental plan updates",
            "plan validation and verification",
            "integration with planning_pipeline/decomposition.py",
            "integration with planning_pipeline/step_decomposition.py",
            "plan serialization and storage"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_005.4",
          "description": "Implement concurrent execution management in go/internal/concurrent/ providing worker pools, task queues, synchronization primitives, error aggregation, and graceful cancellation with support for dynamic scaling and resource limiting",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [
            "Concurrent execution manager creates configurable worker pools with min/max workers",
            "Concurrent execution manager accepts tasks via thread-safe queue",
            "Concurrent execution manager distributes tasks to available workers using round-robin or least-loaded strategy",
            "Concurrent execution manager aggregates errors from all workers into single error collection",
            "Concurrent execution manager supports graceful cancellation via context.Context propagation",
            "Concurrent execution manager limits concurrent operations via semaphore or rate limiter",
            "Concurrent execution manager scales workers dynamically based on queue depth and load",
            "Concurrent execution manager prevents deadlocks through timeout enforcement on all operations",
            "Concurrent execution manager provides wait mechanism blocking until all tasks complete",
            "Concurrent execution manager exposes metrics: active workers, queued tasks, completed tasks, failed tasks",
            "Concurrent execution manager handles panics in worker goroutines recovering and logging errors",
            "Concurrent execution manager supports priority queues for task scheduling",
            "Concurrent execution manager implements backpressure when queue reaches capacity",
            "Concurrent execution manager cleans up resources when pool is shut down",
            "Concurrent execution manager supports task retry with configurable retry policies",
            "Concurrent execution manager provides hooks for task lifecycle events (start, complete, fail)"
          ],
          "implementation": {
            "frontend": [
              "N/A - internal service, no direct frontend interaction"
            ],
            "backend": [
              "Worker pool manager creating and managing goroutine workers",
              "Task queue service implementing thread-safe FIFO or priority queue",
              "Task dispatcher service assigning tasks to workers using load balancing",
              "Error aggregator service collecting errors from concurrent operations",
              "Cancellation propagator service distributing context cancellation to all workers",
              "Resource limiter service enforcing concurrency limits via semaphores",
              "Dynamic scaler service adjusting worker count based on load metrics",
              "Deadlock detector service monitoring operation timeouts and detecting stuck tasks",
              "Metrics collector service tracking worker pool statistics",
              "Panic recovery service catching panics in workers and converting to errors",
              "Retry service executing failed tasks with backoff strategies",
              "Lifecycle hook manager invoking callbacks on task events"
            ],
            "middleware": [
              "Task validation ensuring tasks are properly configured before queuing",
              "Resource limit enforcement preventing queue overflow and excessive concurrency",
              "Timeout enforcement on all task executions with configurable limits",
              "Context validation ensuring contexts are not expired before task execution",
              "Backpressure handling blocking or rejecting tasks when system overloaded"
            ],
            "shared": [
              "Worker pool data structures with worker state management in go/internal/concurrent/",
              "Task queue implementations (FIFO, priority, bounded) with thread-safety guarantees",
              "Synchronization primitives (mutexes, wait groups, semaphores, channels)",
              "Error aggregation types collecting multiple errors with stack traces",
              "Metrics models tracking pool performance and resource utilization",
              "Retry policy definitions with exponential backoff and jitter",
              "Lifecycle hook definitions for task event callbacks",
              "Resource limit configuration models specifying concurrency constraints",
              "Load balancing algorithms (round-robin, least-loaded, random)",
              "Panic recovery utilities converting panics to structured errors"
            ]
          },
          "testable_properties": [],
          "function_id": "ConcurrentExecution.manager",
          "related_concepts": [
            "worker pool pattern",
            "task queue management",
            "goroutine synchronization",
            "context cancellation propagation",
            "error aggregation from concurrent operations",
            "resource limiting and throttling",
            "dynamic worker scaling",
            "deadlock prevention",
            "concurrent data structures"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_005.5",
          "description": "Build cross-platform binaries for darwin, linux, and windows in go/build/ with automated build pipeline, architecture support (amd64, arm64), build optimization, version embedding, and build artifact management",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [
            "Build pipeline produces binaries for darwin (macOS), linux, and windows platforms",
            "Build pipeline supports amd64 and arm64 architectures for each platform",
            "Build pipeline is automated via Makefile with targets: build, build-all, clean, install",
            "Build pipeline optimizes binaries with -ldflags for size reduction and dead code elimination",
            "Build pipeline embeds version information, git commit hash, and build timestamp in binaries",
            "Build pipeline organizes artifacts in go/build/ with directory structure: go/build/{platform}-{arch}/",
            "Build pipeline validates build outputs ensuring binaries are executable and contain version info",
            "Build pipeline supports local builds via 'make build' producing binary for current platform",
            "Build pipeline supports cross-compilation via 'make build-all' producing all platform binaries",
            "Build pipeline implements 'make install' target copying binary to system PATH location",
            "Build pipeline implements 'make clean' target removing all build artifacts",
            "Build pipeline vendors dependencies ensuring reproducible builds",
            "Build pipeline runs tests before building binaries via 'make test' dependency",
            "Build pipeline compresses binaries via UPX or gzip for distribution",
            "Build pipeline generates checksums (SHA256) for all binaries",
            "Build pipeline is integrated with CI/CD system for automated releases",
            "Build pipeline supports custom LDFLAGS for additional build-time configuration"
          ],
          "implementation": {
            "frontend": [
              "N/A - build process, no frontend interaction"
            ],
            "backend": [
              "Makefile defining build targets and platform-specific compilation rules",
              "Build script (build-and-install.sh) orchestrating multi-platform builds",
              "Version embedding logic injecting version info via -ldflags -X",
              "Optimization configuration applying build flags for size and performance",
              "Artifact organizer creating platform-specific directories in go/build/",
              "Validation script testing built binaries for correctness",
              "Compression service reducing binary size via UPX or gzip",
              "Checksum generator creating SHA256 hashes for verification",
              "Dependency vendoring ensuring go.mod and go.sum are up to date",
              "CI/CD integration script for GitHub Actions or similar"
            ],
            "middleware": [
              "Pre-build validation checking Go version and dependencies",
              "Post-build validation testing binary execution and version output",
              "Build failure handling with detailed error reporting",
              "Artifact cleanup on build failure preventing partial builds"
            ],
            "shared": [
              "Build configuration in Makefile with platform and architecture targets",
              "Version file or git tag reading for version information",
              "LDFLAGS template for embedding version, commit, and build time",
              "Build artifact paths in go/build/ with naming convention: {binary-name}-{platform}-{arch}",
              "Platform-specific build configurations (CGO_ENABLED, build tags)",
              "Compression configuration for binary size optimization",
              "Checksum file format (SHA256SUMS) for artifact verification",
              "CI/CD workflow definitions (.github/workflows/ or similar)",
              "Dependency management via go.mod specifying module requirements"
            ]
          },
          "testable_properties": [],
          "function_id": "CrossPlatformBuild.pipeline",
          "related_concepts": [
            "cross-platform compilation with GOOS and GOARCH",
            "build automation with Makefile",
            "binary optimization and size reduction",
            "version information embedding",
            "build artifact organization",
            "continuous integration pipeline",
            "dependency vendoring",
            "reproducible builds"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_006",
      "description": "The system must provide comprehensive documentation including architecture deep dives, native hooks documentation, research knowledge repository, and interactive command definitions",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_006.1",
          "description": "Maintain comprehensive ARCHITECTURE.md in docs/ directory with detailed system design documentation including four-layer memory architecture, multi-language stack (Python/Go/BAML), component interactions, orchestration patterns, and architectural decisions",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "ARCHITECTURE.md exists in docs/ directory and is accessible",
            "Document includes overview of four-layer memory architecture with clear explanation of each layer",
            "Document describes all 13 main directories and their architectural role",
            "Document explains Python/Go/BAML technology stack and rationale for each language choice",
            "Document includes component interaction diagrams showing data flow between layers",
            "Document details planning pipeline architecture and decomposition strategy",
            "Document explains context window array management and batching strategies",
            "Document describes autonomous loop orchestration and phase execution",
            "Document includes RLM-ACT pattern explanation with Research/Learn/Model/Act phases",
            "Document describes BAML integration for type-safe LLM interfaces",
            "Document includes Go runtime architecture for CLI tools and concurrent execution",
            "Document contains agent system architecture and subagent invocation patterns",
            "Document includes checkpoint and state management architecture",
            "Document describes integration points between Python planning and Go runtime",
            "Document includes troubleshooting section for common architectural issues",
            "Document is versioned with last updated date and commit hash",
            "Document includes references to related documentation files",
            "All diagrams are rendered correctly in markdown viewers",
            "Document is kept in sync with actual codebase architecture changes"
          ],
          "implementation": {
            "frontend": [
              "No frontend components required - documentation file only"
            ],
            "backend": [
              "Documentation generation service to extract architectural information from codebase",
              "Automated validation service to check documentation completeness",
              "Version tracking service to maintain documentation history with git commits",
              "Diagram generation service to create architecture diagrams from code structure"
            ],
            "middleware": [
              "Pre-commit hook to validate ARCHITECTURE.md exists and is up-to-date",
              "Documentation linting rules to ensure consistent formatting and structure",
              "Link validation to ensure all internal references are valid"
            ],
            "shared": [
              "ArchitectureDocumentModel with sections for overview, layers, components, patterns",
              "DiagramGenerationUtility for creating mermaid or ASCII diagrams",
              "DocumentationValidationUtility to check completeness criteria",
              "VersionMetadata model to track documentation version, date, commit hash"
            ]
          },
          "testable_properties": [],
          "function_id": "DocumentationService.maintainArchitectureMd",
          "related_concepts": [
            "system architecture documentation",
            "four-layer memory architecture",
            "planning pipeline design",
            "context window management",
            "autonomous loop orchestration",
            "RLM-ACT pattern",
            "BAML type-safe interfaces",
            "Go runtime execution",
            "component interaction diagrams",
            "architectural decision records"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_006.2",
          "description": "Maintain NATIVE-HOOKS.md in docs/ directory with comprehensive documentation on native hooks mode, including git hooks integration, Claude Code hooks, pre-commit workflows, checkpoint triggers, and hook configuration",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "NATIVE-HOOKS.md exists in docs/ directory and is accessible",
            "Document includes overview of native hooks mode and its purpose",
            "Document lists all available hooks (git hooks, Claude hooks, custom hooks)",
            "Document provides installation instructions for each hook type",
            "Document explains hook execution order and lifecycle",
            "Document includes configuration options for each hook with examples",
            "Document details pre-commit hook behavior and validation rules",
            "Document explains checkpoint trigger hooks and state management",
            "Document includes troubleshooting section for common hook issues",
            "Document provides examples of custom hook creation",
            "Document explains how hooks integrate with planning pipeline",
            "Document describes hook interaction with autonomous loop",
            "Document includes security considerations for hook execution",
            "Document explains how to disable/enable specific hooks",
            "Document provides performance considerations for hook execution",
            "Document includes hook testing procedures",
            "Document is versioned with last updated date",
            "All hook examples are tested and working",
            "Document includes references to hook implementation files"
          ],
          "implementation": {
            "frontend": [
              "No frontend components required - documentation file only"
            ],
            "backend": [
              "Hook documentation generator to extract hook metadata from implementation",
              "Hook validation service to test hook configurations",
              "Hook registry service to track all available hooks",
              "Example generation service to create working hook examples"
            ],
            "middleware": [
              "Hook validation middleware to ensure hooks follow documented patterns",
              "Configuration validation middleware for hook settings",
              "Pre-commit hook to validate NATIVE-HOOKS.md completeness"
            ],
            "shared": [
              "HookDocumentationModel with sections for each hook type",
              "HookMetadata model containing name, trigger, configuration, examples",
              "HookValidationUtility to test hook configurations",
              "HookExampleGenerator to create tested examples"
            ]
          },
          "testable_properties": [],
          "function_id": "DocumentationService.maintainNativeHooksMd",
          "related_concepts": [
            "git hooks integration",
            "pre-commit hooks",
            "post-commit hooks",
            "Claude Code hooks",
            "checkpoint triggers",
            "hook configuration",
            "workflow automation",
            "autonomous agent hooks",
            "validation hooks",
            "testing hooks",
            "deployment hooks"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_006.3",
          "description": "Organize and maintain research documents in thoughts/searchable/research/ directory with consistent naming conventions (YYYY-MM-DD-topic-name.md), metadata headers, status tracking, research findings, code references, and cross-references to related research",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "All research documents follow naming convention YYYY-MM-DD-topic-name.md",
            "Each research document includes metadata header with date, researcher, git commit, branch, repository, topic, tags, status",
            "Research documents include clear research question section",
            "Research documents include summary section with key findings",
            "Research documents include detailed findings with code references",
            "Research documents include cross-references to related research",
            "Research documents include status field (in-progress, complete, obsolete)",
            "Research documents use consistent markdown formatting and section structure",
            "Code references include file paths and line numbers or function names",
            "Research documents include open questions section for incomplete research",
            "Directory structure maintains thoughts/searchable/research/ organization",
            "Research index exists to catalog all research documents by topic and date",
            "Obsolete research is marked but retained for historical context",
            "Research documents are searchable by topic, tag, date, and status",
            "Each research document includes researcher attribution",
            "Research documents reference git commit for codebase state at time of research",
            "Related research links are validated and maintained",
            "Research templates are provided for new research documents"
          ],
          "implementation": {
            "frontend": [
              "No frontend components required - file organization system only"
            ],
            "backend": [
              "Research document creation service with template generation",
              "Metadata extraction service to parse research document headers",
              "Research indexing service to catalog all documents by topic/tag/date/status",
              "Cross-reference validation service to check related research links",
              "Research search service with full-text and metadata search",
              "Git integration service to capture commit hash at research creation",
              "Research status tracking service to monitor in-progress vs complete",
              "Obsolete research detection service to identify outdated findings"
            ],
            "middleware": [
              "Metadata validation middleware to ensure required fields present",
              "Naming convention enforcement middleware for file creation",
              "Cross-reference validation middleware to check link integrity",
              "Pre-commit hook to validate research document format"
            ],
            "shared": [
              "ResearchDocumentModel with metadata fields (date, researcher, commit, branch, topic, tags, status)",
              "ResearchMetadata model for header parsing",
              "ResearchIndex model to maintain searchable catalog",
              "ResearchTemplate utility to generate new research documents",
              "NamingConventionUtility to validate and generate file names",
              "CrossReferenceUtility to extract and validate related research links",
              "ResearchStatus enum (in-progress, complete, obsolete)"
            ]
          },
          "testable_properties": [],
          "function_id": "ResearchKnowledgeService.organizeResearchDocuments",
          "related_concepts": [
            "research documentation",
            "knowledge management",
            "searchable research",
            "metadata tagging",
            "research versioning",
            "cross-referencing",
            "research status tracking",
            "findings organization",
            "code reference tracking",
            "research templates"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_006.4",
          "description": "Organize and maintain shared knowledge in thoughts/searchable/shared/ directory including shared plans, architectural decisions, team documentation, symlinks to global knowledge base, and collaborative research with version control and conflict resolution",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "Shared knowledge directory exists at thoughts/searchable/shared/",
            "Shared plans are organized with clear naming and versioning",
            "Architectural decision records (ADRs) follow consistent template",
            "Symlinks to global knowledge base (global/, maceo/, shared/) are maintained",
            "Shared documentation includes author and last updated metadata",
            "Shared knowledge is searchable by topic, author, and date",
            "Version control tracks changes to shared documents",
            "Conflict resolution process exists for simultaneous edits",
            "Shared plans include status (draft, review, approved, implemented)",
            "Cross-references between shared knowledge and research documents work",
            "Shared knowledge index catalogs all documents",
            "Access controls ensure appropriate team visibility",
            "Symlink integrity is validated and broken links are detected",
            "Shared knowledge follows consistent markdown formatting",
            "Team members can contribute through documented process",
            "Obsolete shared knowledge is archived but accessible",
            "Shared knowledge includes references to implementing code"
          ],
          "implementation": {
            "frontend": [
              "No frontend components required - file organization system only"
            ],
            "backend": [
              "Shared knowledge creation service with template support",
              "Symlink management service to create and validate symlinks to global knowledge",
              "Shared knowledge indexing service to catalog all shared documents",
              "Version control integration service for tracking changes",
              "Conflict detection service for simultaneous edits",
              "Shared plan status tracking service (draft, review, approved, implemented)",
              "Access control service for team visibility management",
              "Architectural decision record (ADR) service with template generation",
              "Knowledge synchronization service to sync with global knowledge base",
              "Broken link detection service for symlink validation"
            ],
            "middleware": [
              "Metadata validation middleware for shared documents",
              "Symlink validation middleware to check link integrity",
              "Access control middleware for team visibility",
              "Conflict resolution middleware for edit conflicts",
              "Pre-commit hook to validate shared knowledge format"
            ],
            "shared": [
              "SharedKnowledgeModel with metadata (author, date, status, visibility)",
              "SharedPlanModel with status field (draft, review, approved, implemented)",
              "ArchitecturalDecisionRecord model following ADR template",
              "SymlinkMetadata model to track symlink targets and validity",
              "SharedKnowledgeIndex model for searchable catalog",
              "ConflictResolution utility for handling edit conflicts",
              "KnowledgeSyncUtility for synchronizing with global knowledge base",
              "AccessControl utility for managing team visibility"
            ]
          },
          "testable_properties": [],
          "function_id": "SharedKnowledgeService.organizeSharedKnowledge",
          "related_concepts": [
            "shared knowledge management",
            "collaborative documentation",
            "shared plans",
            "architectural decision records",
            "team knowledge base",
            "symlink management",
            "knowledge sharing",
            "version control",
            "conflict resolution",
            "knowledge synchronization"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_006.5",
          "description": "Define and maintain interactive commands in commands/ directory as markdown files, including status, debug, blockers, next, spec commands with clear descriptions, usage examples, parameter specifications, and integration with Claude Code interactive interface",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "Commands directory exists at commands/ with markdown files for each command",
            "Each command has dedicated markdown file (e.g., status.md, debug.md, blockers.md, next.md, spec.md)",
            "Command files include clear description of command purpose",
            "Command files include usage syntax and examples",
            "Command files specify all parameters (required and optional)",
            "Command files include expected output format",
            "Command files explain integration with autonomous loop",
            "Command files include error handling behavior",
            "Commands are accessible through Claude Code interface with slash prefix",
            "Command registry maintains list of all available commands",
            "Command validation ensures parameters meet requirements",
            "Command help text is displayed when user requests command info",
            "Commands integrate with planning pipeline and agent state",
            "Command execution is logged for debugging",
            "Commands provide meaningful feedback to user",
            "Command templates exist for creating new commands",
            "Command versioning tracks changes to command behavior",
            "Commands handle edge cases and invalid inputs gracefully"
          ],
          "implementation": {
            "frontend": [
              "Command input interface in Claude Code for slash command entry",
              "Command autocomplete UI showing available commands",
              "Command help display UI showing usage and examples",
              "Command output rendering UI for formatted results",
              "Command history UI showing previous command executions"
            ],
            "backend": [
              "Command parser service to parse markdown command definitions",
              "Command registry service to catalog all available commands",
              "Command execution service to invoke command handlers",
              "Command validation service to check parameters",
              "Command help service to generate usage information",
              "Command logging service to track executions",
              "Command template service for creating new commands",
              "Command integration service to connect with planning pipeline and agent state",
              "Command output formatting service for consistent response format"
            ],
            "middleware": [
              "Command authentication middleware to verify user permissions",
              "Command validation middleware to check parameter requirements",
              "Command preprocessing middleware to normalize input",
              "Command logging middleware to capture execution details",
              "Error handling middleware for command failures"
            ],
            "shared": [
              "CommandDefinitionModel with name, description, parameters, examples",
              "CommandParameter model for parameter specifications (type, required, default)",
              "CommandRegistry model to maintain list of available commands",
              "CommandExecutionLog model to track command history",
              "CommandParser utility to extract command metadata from markdown",
              "CommandValidator utility to check parameter requirements",
              "CommandFormatter utility to format command output",
              "CommandTemplate utility to generate new command files"
            ]
          },
          "testable_properties": [],
          "function_id": "InteractiveCommandService.defineCommands",
          "related_concepts": [
            "interactive commands",
            "slash commands",
            "command definitions",
            "command parameters",
            "command help text",
            "Claude Code integration",
            "command execution",
            "command validation",
            "command templates",
            "command registry"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    }
  ],
  "metadata": {
    "source": "agent_sdk_decomposition",
    "research_length": 15674,
    "decomposition_stats": {
      "requirements_found": 7,
      "subprocesses_expanded": 34,
      "total_nodes": 41,
      "extraction_time_ms": 20715,
      "expansion_time_ms": 450840
    },
    "source_research": "thoughts/searchable/research/2026-01-14-project-main-directories.md",
    "decomposed_at": "2026-01-14T12:58:40.914126"
  }
}