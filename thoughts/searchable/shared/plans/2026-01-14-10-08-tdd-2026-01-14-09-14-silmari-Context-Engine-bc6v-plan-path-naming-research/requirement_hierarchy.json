{
  "requirements": [
    {
      "id": "REQ_000",
      "description": "The CLI must support a --plan-path option that accepts a path to an existing TDD plan/hierarchy JSON document",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_000.1",
          "description": "Implement click.Path validation with exists=True, file_okay=True, dir_okay=False for the --plan-path CLI option",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "The --plan-path option uses click.Path type with exists=True parameter",
            "The --plan-path option uses click.Path type with file_okay=True parameter",
            "The --plan-path option uses click.Path type with dir_okay=False parameter",
            "The --plan-path option has default=None for optional usage",
            "CLI returns error exit code when --plan-path file does not exist",
            "CLI returns error exit code when --plan-path points to a directory instead of a file",
            "CLI accepts valid file paths that exist on the filesystem",
            "The option is accessible via '--plan-path' command line argument",
            "Running 'silmari-rlm-act run --help' shows --plan-path in the output"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Add @click.option decorator with '--plan-path' name in cli.py",
              "Configure click.Path(exists=True, file_okay=True, dir_okay=False) type validator",
              "Set default=None to make option optional",
              "Add plan_path parameter to the run() function signature"
            ],
            "middleware": [
              "Click framework handles path existence validation automatically",
              "Click framework handles file vs directory validation automatically"
            ],
            "shared": [
              "Path validation error messages are provided by click framework"
            ]
          },
          "testable_properties": [],
          "function_id": "CLI.planPathOption",
          "related_concepts": [
            "click.Path",
            "CLI argument validation",
            "file path validation",
            "click option decorator"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_000.2",
          "description": "Pass the plan_path CLI parameter to pipeline.run() as hierarchy_path parameter to maintain correct internal naming",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "The plan_path value from CLI is passed to pipeline.run() as hierarchy_path keyword argument",
            "When plan_path is None, hierarchy_path is not passed or is passed as None",
            "The pipeline.run() method signature accepts hierarchy_path as Optional[str]",
            "The mapping preserves the exact file path string without modification",
            "Integration test confirms plan_path='path/to/file.json' results in hierarchy_path='path/to/file.json' in pipeline",
            "The pipeline receives hierarchy_path in its kwargs dictionary"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "In cli.py run() function, pass hierarchy_path=plan_path to pipeline.run() call",
              "Ensure pipeline.run() method accepts **kwargs that includes hierarchy_path",
              "Access hierarchy_path via kwargs.get('hierarchy_path') in pipeline.run() method"
            ],
            "middleware": [],
            "shared": [
              "Document the naming distinction: CLI uses 'plan_path' externally, pipeline uses 'hierarchy_path' internally",
              "Add comment explaining the parameter rename for future maintainers"
            ]
          },
          "testable_properties": [],
          "function_id": "CLI.passPlanPathToHierarchyPath",
          "related_concepts": [
            "parameter mapping",
            "pipeline invocation",
            "CLI to pipeline interface",
            "variable naming convention"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_000.3",
          "description": "Skip ResearchPhase execution when --plan-path is provided since hierarchy JSON contains pre-decomposed requirements",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "When hierarchy_path is provided, ResearchPhase.execute() is NOT called",
            "When hierarchy_path is provided, no research document (.md) is generated",
            "Pipeline state shows ResearchPhase as skipped (not failed, not complete)",
            "Synthetic PhaseResult is created for skipped ResearchPhase with appropriate status",
            "Pipeline proceeds directly to TDDPlanningPhase after skipping Research and Decomposition",
            "No Claude API calls are made for research when hierarchy_path is provided",
            "Pipeline metadata includes 'research_skipped': True flag",
            "The skipped phase result includes hierarchy_path in its metadata for traceability"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Add conditional check in pipeline.run() for hierarchy_path existence",
              "Create synthetic PhaseResult for ResearchPhase with status indicating skip",
              "Store synthetic result in pipeline state via self.state.set_phase_result()",
              "Skip calling self._execute_phase(PhaseType.RESEARCH, ...) when hierarchy_path is set",
              "Continue pipeline execution from TDDPlanningPhase phase order"
            ],
            "middleware": [],
            "shared": [
              "PhaseResult model must support 'skipped' status or appropriate status indication",
              "PhaseStatus enum may need SKIPPED status value"
            ]
          },
          "testable_properties": [],
          "function_id": "Pipeline.skipResearchPhase",
          "related_concepts": [
            "phase skipping",
            "pipeline flow control",
            "ResearchPhase",
            "conditional execution"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_000.4",
          "description": "Skip DecompositionPhase execution when --plan-path is provided since the JSON already contains decomposed requirements",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "When hierarchy_path is provided, DecompositionPhase.execute() is NOT called",
            "When hierarchy_path is provided, no new requirement_hierarchy.json is generated",
            "The provided hierarchy JSON is loaded and validated via _validate_hierarchy_path()",
            "JSON is validated against RequirementHierarchy schema using from_dict()",
            "Each RequirementNode triggers __post_init__() validation for type and category",
            "Validation confirms type is in VALID_REQUIREMENT_TYPES frozenset",
            "Validation confirms category is in VALID_CATEGORIES frozenset",
            "Validation confirms description is non-empty string",
            "Synthetic PhaseResult is created for skipped DecompositionPhase",
            "Metadata includes 'hierarchy_path', 'requirements_count', 'total_nodes', 'validated': True",
            "On validation failure, PhaseResult has status=FAILED with specific error message",
            "Invalid JSON format returns error: 'Plan validation failed: Invalid JSON - {details}'",
            "Invalid requirement type returns error: 'Plan validation failed: {validation_error}'",
            "Pipeline state is updated with the synthetic decomposition result"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Call _validate_hierarchy_path(hierarchy_path) to load and validate JSON",
              "Handle json.JSONDecodeError for malformed JSON with appropriate error message",
              "Handle ValueError from RequirementNode.__post_init__() for schema violations",
              "Handle FileNotFoundError for missing files",
              "Create synthetic PhaseResult for DecompositionPhase with hierarchy metadata",
              "Set artifacts list to include the hierarchy_path",
              "Calculate total_nodes by iterating requirements and their children",
              "Store hierarchy_path in metadata for downstream phases to access"
            ],
            "middleware": [
              "RequirementHierarchy.from_dict() performs deserialization and validation",
              "RequirementNode.__post_init__() validates individual node constraints"
            ],
            "shared": [
              "VALID_REQUIREMENT_TYPES = frozenset(['parent', 'sub_process', 'implementation'])",
              "VALID_CATEGORIES = frozenset(['functional', 'non_functional', 'security', 'performance', 'usability', 'integration'])",
              "RequirementHierarchy data model with requirements list and metadata dict",
              "RequirementNode data model with id, description, type, parent_id, children, acceptance_criteria, category"
            ]
          },
          "testable_properties": [],
          "function_id": "Pipeline.skipDecompositionPhase",
          "related_concepts": [
            "phase skipping",
            "DecompositionPhase",
            "requirement hierarchy",
            "JSON import"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_000.5",
          "description": "Provide descriptive help text explaining the option accepts TDD plan/hierarchy JSON and skips research and decomposition phases",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "Help text is provided via help= parameter in @click.option decorator",
            "Help text mentions 'TDD plan/hierarchy JSON' to indicate expected file format",
            "Help text explains that research phase is skipped when option is used",
            "Help text explains that decomposition phase is skipped when option is used",
            "Running 'silmari-rlm-act run --help' displays the help text for --plan-path",
            "Help text is concise but informative (under 100 characters ideal)",
            "Help text matches the actual behavior of the option"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Set help='Path to existing TDD plan/hierarchy JSON (skips research and decomposition phases)' in @click.option",
              "Ensure help text accurately reflects phase skipping behavior"
            ],
            "middleware": [],
            "shared": []
          },
          "testable_properties": [],
          "function_id": "CLI.helpTextExplanation",
          "related_concepts": [
            "CLI help text",
            "user documentation",
            "click help parameter",
            "option documentation"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_001",
      "description": "The system must validate JSON files provided via --plan-path against the RequirementHierarchy schema",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_001.1",
          "description": "Load and parse JSON content using RequirementHierarchy.from_dict() to deserialize the JSON file into the RequirementHierarchy domain model",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "The function opens the file at hierarchy_path with UTF-8 encoding",
            "The function uses json.load() to parse the file content into a Python dictionary",
            "The function passes the parsed dictionary to RequirementHierarchy.from_dict() for deserialization",
            "The function returns a tuple containing (RequirementHierarchy, None, metadata) on success",
            "The function catches json.JSONDecodeError and returns (None, error_message, {}) with descriptive message 'Plan validation failed: Invalid JSON - {error details}'",
            "The function catches FileNotFoundError and returns (None, 'Plan validation failed: File not found - {path}', {})",
            "The function catches generic Exception and returns (None, 'Plan validation failed: {error}', {})",
            "The function calculates total_nodes by counting parent requirements plus all children recursively",
            "The function populates metadata dict with hierarchy_path, requirements_count, total_nodes, validated=True, and validation_timestamp"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Open file using 'with open(path, \"r\", encoding=\"utf-8\") as f'",
              "Parse JSON using 'data = json.load(f)'",
              "Deserialize using 'hierarchy = RequirementHierarchy.from_dict(data)'",
              "Count nodes by iterating hierarchy.requirements and their children",
              "Build metadata dict with hierarchy_path, requirements_count, total_nodes, validated, validation_timestamp",
              "Return tuple (hierarchy, None, metadata) on success",
              "Handle json.JSONDecodeError, ValueError, FileNotFoundError, and generic Exception separately"
            ],
            "middleware": [
              "Path resolution using pathlib.Path(hierarchy_path)",
              "Error handling chain: JSONDecodeError -> ValueError -> FileNotFoundError -> Exception"
            ],
            "shared": [
              "RequirementHierarchy dataclass with from_dict() class method",
              "RequirementNode dataclass with from_dict() class method that handles recursive children"
            ]
          },
          "testable_properties": [],
          "function_id": "Pipeline._validate_hierarchy_path",
          "related_concepts": [
            "JSON deserialization",
            "file I/O",
            "encoding handling",
            "path resolution",
            "schema validation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_001.2",
          "description": "Trigger RequirementNode.__post_init__() validation for each node in the hierarchy during deserialization",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "The __post_init__ method is automatically called by Python dataclass after __init__ completes",
            "Validation is triggered recursively for each RequirementNode when from_dict() reconstructs children",
            "Each RequirementNode in the hierarchy is validated including nested children at all levels",
            "Validation errors raised during from_dict() propagate up to the calling _validate_hierarchy_path method",
            "The validation order proceeds depth-first through the requirement tree structure",
            "Validation errors include the specific field and value that failed validation",
            "ValueError exceptions contain actionable error messages for debugging"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "RequirementNode.from_dict() calls cls() constructor which triggers __post_init__",
              "Children are reconstructed using '[cls.from_dict(c) for c in data.get(\"children\", [])]' which validates each child",
              "RequirementHierarchy.from_dict() iterates requirements list: '[RequirementNode.from_dict(req) for req in data.get(\"requirements\", [])]'",
              "Each RequirementNode construction validates type, description, and category fields"
            ],
            "middleware": [
              "Dataclass __post_init__ hook mechanism provides automatic validation trigger",
              "ValueError propagation from nested children to parent from_dict() call"
            ],
            "shared": [
              "@dataclass decorator on RequirementNode class",
              "__post_init__(self) method containing validation logic",
              "VALID_REQUIREMENT_TYPES constant: frozenset(['parent', 'sub_process', 'implementation'])",
              "VALID_CATEGORIES constant: frozenset(['functional', 'non_functional', 'security', 'performance', 'usability', 'integration'])"
            ]
          },
          "testable_properties": [],
          "function_id": "RequirementNode.__post_init__",
          "related_concepts": [
            "dataclass post-initialization",
            "automatic validation",
            "recursive validation",
            "validation cascade",
            "Python dataclasses"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_001.3",
          "description": "Validate requirement type is in VALID_REQUIREMENT_TYPES (parent, sub_process, implementation)",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "The validation checks 'self.type not in VALID_REQUIREMENT_TYPES'",
            "VALID_REQUIREMENT_TYPES is defined as frozenset(['parent', 'sub_process', 'implementation'])",
            "Valid type 'parent' passes validation without error",
            "Valid type 'sub_process' passes validation without error",
            "Valid type 'implementation' passes validation without error",
            "Invalid type raises ValueError with message \"Invalid type '{self.type}'. Must be one of: {', '.join(VALID_REQUIREMENT_TYPES)}\"",
            "Type validation is case-sensitive (e.g., 'Parent' is invalid, 'parent' is valid)",
            "Type validation occurs before description and category validation in __post_init__"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Define VALID_REQUIREMENT_TYPES = frozenset(['parent', 'sub_process', 'implementation']) at module level",
              "In __post_init__: if self.type not in VALID_REQUIREMENT_TYPES: raise ValueError(...)",
              "Format error message to show invalid value and list all valid options",
              "Use ', '.join(VALID_REQUIREMENT_TYPES) to create comma-separated list of valid values"
            ],
            "middleware": [
              "frozenset provides O(1) membership testing performance",
              "frozenset is immutable preventing runtime modification of valid types"
            ],
            "shared": [
              "VALID_REQUIREMENT_TYPES constant exported from planning_pipeline/models.py",
              "Constant used by both RequirementNode validation and external validators"
            ]
          },
          "testable_properties": [],
          "function_id": "RequirementNode.__post_init__.validate_type",
          "related_concepts": [
            "enum validation",
            "frozenset membership",
            "type hierarchy",
            "requirement classification",
            "domain constraints"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_001.4",
          "description": "Validate requirement category is in VALID_CATEGORIES (functional, non_functional, security, performance, usability, integration)",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "The validation checks 'self.category not in VALID_CATEGORIES'",
            "VALID_CATEGORIES is defined as frozenset(['functional', 'non_functional', 'security', 'performance', 'usability', 'integration'])",
            "Valid category 'functional' passes validation without error",
            "Valid category 'non_functional' passes validation without error",
            "Valid category 'security' passes validation without error",
            "Valid category 'performance' passes validation without error",
            "Valid category 'usability' passes validation without error",
            "Valid category 'integration' passes validation without error",
            "Invalid category raises ValueError with message \"Invalid category '{self.category}'. Must be one of: {', '.join(sorted(VALID_CATEGORIES))}\"",
            "Category has default value 'functional' if not specified in JSON",
            "Category validation is case-sensitive (e.g., 'Functional' is invalid, 'functional' is valid)",
            "Category validation occurs after type and description validation in __post_init__"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Define VALID_CATEGORIES = frozenset(['functional', 'non_functional', 'security', 'performance', 'usability', 'integration']) at module level",
              "In __post_init__: if self.category not in VALID_CATEGORIES: raise ValueError(...)",
              "Format error message with sorted() to ensure consistent ordering of valid options",
              "Use ', '.join(sorted(VALID_CATEGORIES)) to create alphabetically sorted comma-separated list"
            ],
            "middleware": [
              "frozenset provides O(1) membership testing performance",
              "sorted() ensures deterministic error message format for testing"
            ],
            "shared": [
              "VALID_CATEGORIES constant exported from planning_pipeline/models.py",
              "Default category='functional' specified in RequirementNode dataclass field definition",
              "from_dict() uses data.get('category', 'functional') to apply default"
            ]
          },
          "testable_properties": [],
          "function_id": "RequirementNode.__post_init__.validate_category",
          "related_concepts": [
            "category classification",
            "requirement taxonomy",
            "frozenset validation",
            "domain constraints",
            "ISO/IEC 25010 quality characteristics"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_001.5",
          "description": "Validate description field is non-empty for each requirement node",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "The validation checks 'not self.description or not self.description.strip()'",
            "Empty string '' raises ValueError with message 'Requirement description must not be empty'",
            "None value raises ValueError with message 'Requirement description must not be empty'",
            "Whitespace-only string '   ' raises ValueError with message 'Requirement description must not be empty'",
            "Tab-only string '\\t\\t' raises ValueError with message 'Requirement description must not be empty'",
            "Newline-only string '\\n\\n' raises ValueError with message 'Requirement description must not be empty'",
            "Mixed whitespace string ' \\t\\n ' raises ValueError with message 'Requirement description must not be empty'",
            "Description with leading/trailing whitespace but non-empty content passes validation",
            "Description validation occurs after type validation but before category validation in __post_init__",
            "Minimum valid description is a single non-whitespace character"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "In __post_init__: if not self.description or not self.description.strip(): raise ValueError('Requirement description must not be empty')",
              "Use short-circuit evaluation: 'not self.description' catches None/empty before calling strip()",
              "strip() removes all Unicode whitespace characters (space, tab, newline, etc.)",
              "Error message is static string, not formatted with the invalid value"
            ],
            "middleware": [
              "str.strip() handles all whitespace types including Unicode whitespace",
              "Short-circuit evaluation prevents AttributeError on None values"
            ],
            "shared": [
              "description: str field in RequirementNode dataclass is required (no default value)",
              "RequirementNode.from_dict() uses data['description'] which raises KeyError if missing"
            ]
          },
          "testable_properties": [],
          "function_id": "RequirementNode.__post_init__.validate_description",
          "related_concepts": [
            "string validation",
            "whitespace handling",
            "required field validation",
            "content verification",
            "data integrity"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_002",
      "description": "The system must produce two distinct artifact types at different pipeline phases: TDD Plan Markdown files and Requirement Hierarchy JSON files",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_002.1",
          "description": "TDDPlanningPhase must produce human-readable Markdown files with .md extension",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "TDDPlanningPhase.execute() produces one or more .md files as output artifacts",
            "All generated files have .md extension, not .markdown or other variations",
            "Generated Markdown files are human-readable without special tooling (viewable in any text editor)",
            "Markdown syntax is valid and renders correctly in GitHub/GitLab markdown previews",
            "Each generated file includes proper Markdown headers (# for H1, ## for H2, etc.)",
            "Files are saved with UTF-8 encoding to support special characters",
            "PhaseResult.artifacts list contains paths to all generated .md files",
            "File names follow naming convention: NN-phase-N-description.md (e.g., 01-phase-1-project-setup.md)",
            "00-overview.md file is always generated as the primary entry point",
            "Generated files are written atomically to prevent partial writes on failure"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Implement TDDPlanningPhase._write_markdown_file(path: Path, content: str) -> None method with UTF-8 encoding",
              "Implement TDDPlanningPhase._generate_overview_markdown(hierarchy: RequirementHierarchy) -> str to create 00-overview.md content",
              "Implement TDDPlanningPhase._generate_phase_markdown(phase_num: int, requirements: list[RequirementNode]) -> str for numbered phase files",
              "Add artifact_paths: list[Path] tracking within execute() to collect all generated file paths",
              "Return PhaseResult with artifacts=[str(p) for p in artifact_paths] after successful generation",
              "Implement atomic write using tempfile + rename pattern to prevent partial file corruption",
              "Add validation that all file paths end with .md before writing"
            ],
            "middleware": [
              "Validate output directory exists and is writable before attempting file generation",
              "Log each file generation with path and size for audit trail"
            ],
            "shared": [
              "Define MARKDOWN_FILE_EXTENSION = '.md' constant for consistent extension usage",
              "Define TDD_PLAN_FILENAME_PATTERN = '{:02d}-{}.md' for consistent file naming",
              "Create MarkdownWriter utility class with write_atomic(path, content) method"
            ]
          },
          "testable_properties": [],
          "function_id": "TDDPlanningPhase.generateMarkdownArtifacts",
          "related_concepts": [
            "TDDPlanningPhase",
            "Markdown generation",
            "human-readable documentation",
            "file I/O",
            "artifact production",
            "phase output"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.2",
          "description": "DecompositionPhase must produce machine-readable JSON files with .json extension",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "DecompositionPhase.execute() produces exactly one .json file named requirement_hierarchy.json",
            "Generated file has .json extension, not .JSON or other variations",
            "JSON output is valid and parseable by standard JSON parsers (json.load() succeeds)",
            "JSON is formatted with 2-space indentation for human readability while maintaining machine parseability",
            "JSON includes UTF-8 encoding declaration for special character support",
            "Output JSON passes RequirementHierarchy.from_dict() deserialization without errors",
            "PhaseResult.artifacts list contains the path to requirement_hierarchy.json",
            "JSON file is written atomically to prevent partial writes on failure",
            "Generated JSON is deterministic (same input produces identical output for version control)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Implement DecompositionPhase._write_json_file(path: Path, data: dict) -> None with json.dump(data, f, indent=2, ensure_ascii=False)",
              "Implement RequirementHierarchy.to_dict() -> dict method for serialization to JSON-compatible dict",
              "Implement RequirementNode.to_dict() -> dict method for recursive serialization including children",
              "Call hierarchy.to_dict() and write result to requirement_hierarchy.json in output directory",
              "Return PhaseResult with artifacts=[str(json_path)] after successful generation",
              "Implement atomic write using tempfile.NamedTemporaryFile + os.rename pattern",
              "Add JSON schema validation of output before final write to catch serialization bugs"
            ],
            "middleware": [
              "Validate output directory exists and is writable before attempting JSON generation",
              "Log JSON generation with file size and node count for audit trail"
            ],
            "shared": [
              "Define JSON_FILE_EXTENSION = '.json' constant for consistent extension usage",
              "Define HIERARCHY_FILENAME = 'requirement_hierarchy.json' constant",
              "Ensure json module is imported with proper error handling for JSONEncodeError"
            ]
          },
          "testable_properties": [],
          "function_id": "DecompositionPhase.generateJsonArtifacts",
          "related_concepts": [
            "DecompositionPhase",
            "JSON serialization",
            "machine-readable format",
            "RequirementHierarchy",
            "schema validation",
            "artifact production"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.3",
          "description": "TDD Plan Markdown must contain phases, testable behaviors, and requirements tables",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "00-overview.md contains Phase Summary table with columns: Phase, Description, Requirements, Status",
            "00-overview.md contains Requirements section with hierarchical structure (###, ####, #####)",
            "Each requirement (REQ_XXX) has its own section with description and child requirements",
            "Testable Behaviors are rendered as numbered lists under each leaf requirement",
            "Phase files (01-phase-N-*.md) contain: Overview, Dependencies, Behaviors Covered, Changes Required sections",
            "Changes Required section includes New Files table with columns: File, Purpose",
            "TDD Cycle section includes Red (failing tests), Green (implement), Refactor subsections",
            "Success Criteria section includes Automated and Manual checklists with [ ] checkbox syntax",
            "Testable Function section includes executable Python code block demonstrating phase completion",
            "All markdown tables are properly formatted with header separators (|---|---|)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Implement TDDPlanMarkdownGenerator class with methods for each markdown section type",
              "Implement _generate_phase_summary_table(phases: list[PhaseInfo]) -> str returning markdown table",
              "Implement _generate_requirements_hierarchy(requirements: list[RequirementNode]) -> str with recursive heading levels",
              "Implement _generate_testable_behaviors_list(behaviors: list[TestableProperty]) -> str with numbered list",
              "Implement _generate_tdd_cycle_section(phase: PhaseInfo) -> str with Red/Green/Refactor subsections",
              "Implement _generate_success_criteria_section(criteria: list[str]) -> str with checkbox markdown",
              "Implement _format_markdown_table(headers: list[str], rows: list[list[str]]) -> str utility",
              "Add validation that all required sections are present before returning generated content"
            ],
            "middleware": [
              "Validate markdown syntax using basic regex checks (table alignment, header format)",
              "Escape special markdown characters in requirement descriptions to prevent rendering issues"
            ],
            "shared": [
              "Define MARKDOWN_SECTION_HEADERS dict mapping section names to their heading levels",
              "Define PhaseInfo dataclass with phase_number, description, requirements, status fields",
              "Define TestableProperty dataclass with behavior description and expected outcome"
            ]
          },
          "testable_properties": [],
          "function_id": "TDDPlanningPhase.generatePlanContent",
          "related_concepts": [
            "TDD Plan structure",
            "Markdown tables",
            "testable behaviors",
            "phase organization",
            "requirements traceability",
            "implementation plan"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.4",
          "description": "Requirement Hierarchy JSON must contain nested requirement tree with id, description, type, parent_id, children, acceptance_criteria, and category fields",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "Each RequirementNode serializes with required fields: id (string), description (string), type (enum: parent|sub_process|implementation)",
            "Each RequirementNode serializes parent_id as string or null for root nodes",
            "Each RequirementNode serializes children as array of nested RequirementNode objects (recursive)",
            "Each RequirementNode serializes acceptance_criteria as array of strings",
            "Each RequirementNode serializes category as enum: functional|non_functional|security|performance|usability|integration",
            "Top-level JSON object contains 'requirements' array and 'metadata' object",
            "Metadata object contains source, decomposition_stats with requirements_found, subprocesses_expanded, total_nodes",
            "All id fields follow REQ_XXX or REQ_XXX.N format (e.g., REQ_001, REQ_001.2)",
            "parent_id correctly references the parent node's id, or is null for top-level requirements",
            "JSON passes round-trip test: RequirementHierarchy.from_dict(hierarchy.to_dict()) produces equivalent object"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Implement RequirementNode.to_dict() -> dict with all required fields",
              "Implement recursive serialization: 'children': [child.to_dict() for child in self.children]",
              "Implement RequirementHierarchy.to_dict() -> dict with requirements array and metadata",
              "Add type validation in to_dict(): assert self.type in VALID_REQUIREMENT_TYPES",
              "Add category validation in to_dict(): assert self.category in VALID_CATEGORIES",
              "Implement ImplementationComponents.to_dict() -> dict for implementation field serialization",
              "Implement TestableProperty.to_dict() -> dict for testable_properties field serialization",
              "Add metadata serialization with decomposition_stats including timestamp"
            ],
            "middleware": [
              "Add JSON schema validation against defined schema before returning dict",
              "Validate all parent_id references point to valid nodes in the hierarchy"
            ],
            "shared": [
              "Define VALID_REQUIREMENT_TYPES = frozenset(['parent', 'sub_process', 'implementation'])",
              "Define VALID_CATEGORIES = frozenset(['functional', 'non_functional', 'security', 'performance', 'usability', 'integration'])",
              "Define RequirementHierarchySchema TypedDict for IDE type checking of dict output",
              "Define RequirementNodeSchema TypedDict for IDE type checking of nested node dicts"
            ]
          },
          "testable_properties": [],
          "function_id": "RequirementHierarchy.serializeToJson",
          "related_concepts": [
            "RequirementHierarchy",
            "RequirementNode",
            "JSON schema",
            "nested data structures",
            "tree serialization",
            "parent-child relationships"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.5",
          "description": "Store artifacts in appropriate directory structure under thoughts/searchable/shared/plans/",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "All artifacts are stored under thoughts/searchable/shared/plans/ base directory",
            "Each pipeline run creates a new timestamped directory: YYYY-MM-DD-HH-MM-tdd-{plan_name}/",
            "Requirement hierarchy JSON is stored at: {plan_dir}/requirement_hierarchy.json",
            "TDD plan overview is stored at: {plan_dir}/00-overview.md",
            "Phase-specific markdown files are stored at: {plan_dir}/NN-phase-N-{slugified-name}.md",
            "Directory names are slugified (lowercase, hyphens instead of spaces, no special characters)",
            "Directory is created with appropriate permissions (755 for directories, 644 for files)",
            "If directory already exists, append incrementing suffix (-01, -02) to avoid overwriting",
            "All paths are resolved to absolute paths before storage operations",
            "Storage operation is atomic: either all files are written or none (use temp directory + rename)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Implement ArtifactStorage class with base_path: Path = Path('thoughts/searchable/shared/plans')",
              "Implement _generate_plan_directory_name(plan_name: str, timestamp: datetime) -> str with slugification",
              "Implement _ensure_unique_directory(path: Path) -> Path that appends suffix if path exists",
              "Implement create_plan_directory(plan_name: str) -> Path that creates and returns the unique directory",
              "Implement store_hierarchy_json(plan_dir: Path, hierarchy: RequirementHierarchy) -> Path",
              "Implement store_tdd_markdown(plan_dir: Path, filename: str, content: str) -> Path",
              "Implement atomic_store_all(plan_dir: Path, files: dict[str, str]) -> list[Path] with temp dir pattern",
              "Add cleanup logic to remove temp directory on failure during atomic storage"
            ],
            "middleware": [
              "Validate base_path exists and is writable before any storage operations",
              "Create base_path directory structure if it doesn't exist with mkdir(parents=True)"
            ],
            "shared": [
              "Define PLANS_BASE_PATH = Path('thoughts/searchable/shared/plans') constant",
              "Define DIRECTORY_NAME_PATTERN = '{date}-tdd-{name}' for consistent naming",
              "Import slugify utility or implement _slugify(text: str) -> str for name normalization",
              "Define FILE_PERMISSIONS = 0o644 and DIR_PERMISSIONS = 0o755 constants"
            ]
          },
          "testable_properties": [],
          "function_id": "ArtifactStorage.storeArtifacts",
          "related_concepts": [
            "artifact storage",
            "directory structure",
            "file organization",
            "plan directories",
            "timestamp naming",
            "path resolution"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_003",
      "description": "The system must maintain consistent CLI option behavior across all orchestrator implementations",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_003.1",
          "description": "silmari_rlm_act/cli.py must accept JSON hierarchy files via --plan-path, validating the JSON structure against RequirementHierarchy schema and skipping Research and Decomposition phases when a valid hierarchy is provided",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "The --plan-path option must accept only files with valid JSON format",
            "Invalid JSON must produce a clear error message with the parse error details",
            "The JSON must validate against RequirementHierarchy.from_dict() schema",
            "Each requirement node must have 'type' in ['parent', 'sub_process', 'implementation']",
            "Each requirement node must have 'category' in ['functional', 'non_functional', 'security', 'performance', 'usability', 'integration']",
            "Each requirement node must have a non-empty 'description' field",
            "When a valid JSON hierarchy is provided, ResearchPhase must be skipped",
            "When a valid JSON hierarchy is provided, DecompositionPhase must be skipped",
            "The pipeline must proceed directly to TDDPlanningPhase with the loaded hierarchy",
            "The result metadata must include 'requirements_count' and 'total_nodes' fields",
            "The help text must clearly state: 'Path to existing requirement hierarchy JSON file (skips research and decomposition phases)'",
            "The internal variable must be passed as 'hierarchy_path' to pipeline.run()",
            "click.Path(exists=True, file_okay=True, dir_okay=False) must validate file existence",
            "PhaseResult with status=FAILED and descriptive errors array must be returned for invalid JSON"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Implement JSON file loading with proper error handling in cli.py",
              "Call RequirementHierarchy.from_dict() for schema validation",
              "Trigger RequirementNode.__post_init__() validation for each node",
              "Pass validated hierarchy to pipeline.run() as hierarchy_path parameter",
              "Implement phase skipping logic in pipeline.py lines 140-280",
              "Return PhaseResult with appropriate status and error messages"
            ],
            "middleware": [
              "click.Path validator for file existence check",
              "JSON parse validation before schema validation",
              "RequirementNode field validation via __post_init__"
            ],
            "shared": [
              "RequirementHierarchy dataclass with from_dict() class method",
              "RequirementNode dataclass with __post_init__ validation",
              "VALID_REQUIREMENT_TYPES frozenset constant",
              "VALID_CATEGORIES frozenset constant",
              "PhaseResult dataclass for error reporting"
            ]
          },
          "testable_properties": [],
          "function_id": "SilmariRlmActCli.planPathJsonHandler",
          "related_concepts": [
            "RequirementHierarchy schema validation",
            "RequirementNode dataclass validation",
            "VALID_REQUIREMENT_TYPES validation",
            "VALID_CATEGORIES validation",
            "Phase skipping logic",
            "JSON schema enforcement",
            "Pipeline data flow"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_003.2",
          "description": "resume_pipeline.py must accept Markdown plan files via --plan-path for the decomposition step, supporting both --plan-path and --plan_path argument forms with proper path validation",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "The --plan-path option must accept .md (Markdown) files",
            "The --plan_path alias must work identically to --plan-path",
            "The dest parameter must be set to 'plan_path' for internal variable naming",
            "The help text must state: 'Path to plan document (required for decomposition step)'",
            "Presence check validation must be performed (file must exist)",
            "The option must be required when running the decomposition step",
            "Example usage in help must show: 'resume_pipeline.py decomposition --plan-path thoughts/searchable/plans/2025-12-31-plan.md'",
            "The system must accept full paths, relative paths to .md files",
            "Error message must clearly indicate when plan-path is missing for decomposition step",
            "The Markdown file path must be passed to the decomposition orchestrator"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Add argparse argument with both --plan-path and --plan_path forms",
              "Set dest='plan_path' for consistent internal naming",
              "Implement file existence validation",
              "Pass plan_path to decomposition step handler",
              "Generate clear error messages for missing required argument"
            ],
            "middleware": [
              "Path existence validation before decomposition step",
              "File extension validation for .md files (optional enhancement)"
            ],
            "shared": [
              "Common path resolution utilities",
              "Error message templates for missing arguments"
            ]
          },
          "testable_properties": [],
          "function_id": "ResumePipeline.planPathMarkdownHandler",
          "related_concepts": [
            "Markdown file validation",
            "Decomposition step requirements",
            "Legacy CLI compatibility",
            "argparse argument handling",
            "Plan document path resolution"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_003.3",
          "description": "planning_orchestrator.py must accept Markdown .md files via --plan-path with auto-resolution capabilities, supporting full paths, relative paths, and filename-only inputs with interactive fallback",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "The --plan-path option must accept Markdown .md files explicitly",
            "The --plan_path alias must work identically to --plan-path",
            "The dest parameter must be set to 'plan_path' for internal variable naming",
            "The metavar must be set to 'FILE' for help text clarity",
            "The help text must state: 'Plan .md file: full path, relative path, or just filename (auto-resolved)'",
            "Full absolute paths must be accepted and validated directly",
            "Relative paths must be resolved from current working directory",
            "Filename-only inputs must trigger auto-resolution via resolve_file_path()",
            "Auto-resolution must search in standard plan directories (e.g., thoughts/searchable/plans/)",
            "When auto-resolution fails, interactive fallback must prompt user for path",
            "Multiple matching files during auto-resolution must present selection to user",
            "The resolved path must be validated for existence before proceeding",
            "Clear error messages must be provided when file cannot be found or resolved"
          ],
          "implementation": {
            "frontend": [
              "Interactive prompt for file selection when multiple matches found",
              "Interactive fallback prompt when auto-resolution fails",
              "Display list of candidate files for user selection"
            ],
            "backend": [
              "Add argparse argument with --plan-path and --plan_path forms",
              "Set dest='plan_path' and metavar='FILE'",
              "Implement resolve_file_path() utility function",
              "Search standard directories for filename matches",
              "Handle multiple match scenarios with selection logic",
              "Pass resolved plan_path to planning orchestrator execution"
            ],
            "middleware": [
              "Path format detection (absolute vs relative vs filename-only)",
              "File existence validation after resolution",
              "Markdown file extension verification"
            ],
            "shared": [
              "resolve_file_path() utility function",
              "Standard plan directory constants",
              "Path resolution configuration"
            ]
          },
          "testable_properties": [],
          "function_id": "PlanningOrchestrator.planPathAutoResolution",
          "related_concepts": [
            "Auto-resolution file lookup",
            "Interactive fallback prompts",
            "resolve_file_path utility",
            "Multiple path format support",
            "Markdown plan file detection"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_003.4",
          "description": "Document the expected file type for each CLI implementation clearly in help text, ensuring users understand the distinction between JSON hierarchy files and Markdown plan files across different orchestrators",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "silmari_rlm_act/cli.py help text must explicitly state: 'Path to existing requirement hierarchy JSON file (skips research and decomposition phases)'",
            "silmari_rlm_act/cli.py help text must NOT use 'TDD plan/hierarchy JSON' conflated terminology",
            "resume_pipeline.py help text must explicitly state: 'Path to TDD plan Markdown document (.md) required for decomposition step'",
            "planning_orchestrator.py help text must explicitly state: 'Path to TDD plan Markdown file (.md): full path, relative path, or just filename (auto-resolved)'",
            "Each CLI must include example usage showing correct file extension in help output",
            "Help text must explain what phases are skipped when the option is used (for silmari_rlm_act)",
            "Help text must explain the relationship between JSON hierarchy and Markdown plan files",
            "Consider adding a --help-files option that explains the artifact file types in detail",
            "Documentation must be consistent across all three CLI implementations",
            "Error messages must reference the correct expected file type when validation fails"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Update cli.py option help string to explicitly mention JSON and hierarchy",
              "Update resume_pipeline.py option help string to explicitly mention Markdown",
              "Update planning_orchestrator.py option help string to explicitly mention Markdown with auto-resolution",
              "Add example usage to each help string showing correct file extension",
              "Ensure error messages reference correct file type expectations"
            ],
            "middleware": [],
            "shared": [
              "Shared help text templates for file type descriptions",
              "Constants for standard file extensions (.json, .md)",
              "Error message templates with file type context"
            ]
          },
          "testable_properties": [],
          "function_id": "CLIDocumentation.helpTextClarification",
          "related_concepts": [
            "CLI help text standards",
            "User experience documentation",
            "File type disambiguation",
            "Option naming conventions",
            "Cross-CLI consistency"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_004",
      "description": "The system must return appropriate error messages and phase results when JSON validation fails",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_004.1",
          "description": "Return PhaseResult with status=FAILED when JSON parsing fails",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "When json.load() raises JSONDecodeError, the method returns (None, error_message, {}) tuple",
            "The error message contains 'Invalid JSON' prefix followed by the specific parse error details",
            "The returned tuple format is (hierarchy: None, error: str, metadata: empty dict)",
            "The calling code in run() method creates PhaseResult with status=PhaseStatus.FAILED",
            "The PhaseResult.phase_type is set to PhaseType.DECOMPOSITION",
            "The PhaseResult.errors array contains exactly one error string from the validation",
            "The PhaseResult.metadata includes 'validation_failed': True",
            "The PhaseResult.metadata includes 'validated': False per REQ_004.4.9",
            "The PhaseResult.metadata includes 'error_count': 1 per REQ_004.4.10",
            "The PhaseResult includes timing information (started_at, completed_at, duration_seconds)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Implement try-except block in _validate_hierarchy_path catching json.JSONDecodeError",
              "Format error message as f'Plan validation failed: Invalid JSON - {e}'",
              "Return tuple (None, error_message, {}) to signal validation failure",
              "In run() method, check if error is returned and create failed PhaseResult",
              "Set PhaseResult fields: phase_type=DECOMPOSITION, status=FAILED, errors=[error]",
              "Populate metadata with validation_failed=True, validated=False, error_count=1"
            ],
            "middleware": [],
            "shared": [
              "PhaseResult dataclass from models.py",
              "PhaseStatus.FAILED enum value",
              "PhaseType.DECOMPOSITION enum value"
            ]
          },
          "testable_properties": [],
          "function_id": "RLMActPipeline._validate_hierarchy_path",
          "related_concepts": [
            "PhaseResult",
            "PhaseStatus.FAILED",
            "json.JSONDecodeError",
            "hierarchy validation",
            "error handling"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_004.2",
          "description": "Include specific validation errors in the errors array of PhaseResult",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "PhaseResult.errors is a list[str] containing all validation error messages",
            "Each error message is descriptive and actionable for the user",
            "Error messages include the file path that failed validation",
            "Error messages include the specific validation rule that was violated",
            "For JSON parse errors, message includes line/column information if available",
            "For schema validation errors, message includes the field name that failed",
            "For type/category errors, message includes the invalid value and valid options",
            "Multiple errors can be accumulated if validation produces multiple failures",
            "Errors are accessible via result.errors after pipeline.run() returns"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Create PhaseResult with errors=[error] where error is the validation error string",
              "Ensure error messages follow format: 'Plan validation failed: <specific reason>'",
              "Include hierarchy_path in metadata so user knows which file failed",
              "For ValueError from RequirementHierarchy validation, capture the full exception message",
              "Return the PhaseResult immediately upon validation failure without continuing pipeline"
            ],
            "middleware": [],
            "shared": [
              "PhaseResult.errors: list[str] field definition",
              "Error message formatting conventions",
              "RequirementHierarchy validation exceptions"
            ]
          },
          "testable_properties": [],
          "function_id": "RLMActPipeline.run",
          "related_concepts": [
            "PhaseResult.errors",
            "error aggregation",
            "validation error messages",
            "user-facing errors"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_004.3",
          "description": "Validate invalid JSON produces clear error message (test_plan_path_validates_json_format)",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "Test creates a temporary file containing invalid JSON syntax (e.g., 'not valid json {{{')",
            "Test invokes pipeline.run() with hierarchy_path pointing to invalid JSON file",
            "Test asserts result.status == PhaseStatus.FAILED",
            "Test asserts result.errors contains at least one error with 'json' or 'valid' keyword (case-insensitive)",
            "Test verifies the pipeline does not proceed to TDD_PLANNING phase",
            "Test fixture uses tmp_path to create temporary files",
            "Test uses mock_cwa and mock_beads_controller fixtures",
            "Test uses AutonomyMode.FULLY_AUTONOMOUS to avoid user interaction"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Create test file: invalid_doc = tmp_path / 'invalid.json'",
              "Write invalid content: invalid_doc.write_text('not valid json {{{')",
              "Create pipeline instance with mocked dependencies",
              "Call result = pipeline.run(research_question='', hierarchy_path=str(invalid_doc))",
              "Assert result.status == PhaseStatus.FAILED",
              "Assert any('json' in err.lower() or 'valid' in err.lower() for err in result.errors)"
            ],
            "middleware": [],
            "shared": [
              "pytest fixtures: temp_project, mock_cwa, mock_beads_controller, tmp_path",
              "PhaseStatus enum for assertion",
              "RLMActPipeline class import"
            ]
          },
          "testable_properties": [],
          "function_id": "TestPlanDocumentValidation.test_plan_path_validates_json_format",
          "related_concepts": [
            "pytest",
            "json.JSONDecodeError",
            "PhaseStatus.FAILED",
            "error message validation",
            "test fixtures"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_004.4",
          "description": "Validate incorrect requirement type produces error (test_plan_path_validates_requirement_type)",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "Test creates JSON with requirement having type='invalid_type' (not in VALID_REQUIREMENT_TYPES)",
            "VALID_REQUIREMENT_TYPES = frozenset(['parent', 'sub_process', 'implementation'])",
            "Test verifies result.status == PhaseStatus.FAILED when invalid type is used",
            "Test verifies error message contains 'type' or 'invalid' keyword",
            "Test JSON structure includes all required fields: id, description, type, parent_id, children, acceptance_criteria, category",
            "Test uses valid category='functional' to isolate type validation failure",
            "RequirementNode.__post_init__() raises ValueError for invalid type"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Create hierarchy dict with requirements containing invalid type",
              "hierarchy = {'requirements': [{'id': 'REQ_001', 'description': 'Test', 'type': 'invalid_type', 'parent_id': None, 'children': [], 'acceptance_criteria': [], 'category': 'functional'}], 'metadata': {}}",
              "Write JSON to temp file: invalid_doc.write_text(json.dumps(hierarchy))",
              "Call pipeline.run() with hierarchy_path",
              "Assert result.status == PhaseStatus.FAILED",
              "Assert any('type' in err.lower() or 'invalid' in err.lower() for err in result.errors)"
            ],
            "middleware": [],
            "shared": [
              "RequirementNode dataclass with type validation in __post_init__",
              "VALID_REQUIREMENT_TYPES frozenset constant",
              "ValueError exception for invalid types"
            ]
          },
          "testable_properties": [],
          "function_id": "TestPlanDocumentValidation.test_plan_path_validates_requirement_type",
          "related_concepts": [
            "VALID_REQUIREMENT_TYPES",
            "RequirementNode validation",
            "dataclass __post_init__",
            "type validation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_004.5",
          "description": "Validate incorrect category produces error (test_plan_path_validates_requirement_category)",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "Test creates JSON with requirement having category='invalid_category' (not in VALID_CATEGORIES)",
            "VALID_CATEGORIES = frozenset(['functional', 'non_functional', 'security', 'performance', 'usability', 'integration'])",
            "Test verifies result.status == PhaseStatus.FAILED when invalid category is used",
            "Test verifies error message contains 'category' or 'invalid' keyword",
            "Test JSON structure includes all required fields with valid type='parent'",
            "Test uses valid type to isolate category validation failure",
            "RequirementNode.__post_init__() raises ValueError for invalid category"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Create hierarchy dict with requirements containing invalid category",
              "hierarchy = {'requirements': [{'id': 'REQ_001', 'description': 'Test', 'type': 'parent', 'parent_id': None, 'children': [], 'acceptance_criteria': [], 'category': 'invalid_category'}], 'metadata': {}}",
              "Write JSON to temp file: invalid_doc.write_text(json.dumps(hierarchy))",
              "Call pipeline.run() with hierarchy_path",
              "Assert result.status == PhaseStatus.FAILED",
              "Assert any('category' in err.lower() or 'invalid' in err.lower() for err in result.errors)"
            ],
            "middleware": [],
            "shared": [
              "RequirementNode dataclass with category validation in __post_init__",
              "VALID_CATEGORIES frozenset constant",
              "ValueError exception for invalid categories"
            ]
          },
          "testable_properties": [],
          "function_id": "TestPlanDocumentValidation.test_plan_path_validates_requirement_category",
          "related_concepts": [
            "VALID_CATEGORIES",
            "RequirementNode validation",
            "dataclass __post_init__",
            "category validation"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_005",
      "description": "The system must support auto-detection and resolution of plan and hierarchy file relationships",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_005.1",
          "description": "Detect file type based on .md vs .json extension to determine processing path",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [
            "MUST return 'markdown' for files with .md extension regardless of case (.MD, .Md)",
            "MUST return 'json' for files with .json extension regardless of case (.JSON, .Json)",
            "MUST return 'unknown' for files with other extensions or no extension",
            "MUST handle Path objects and string paths as input",
            "MUST validate file existence before type detection",
            "MUST raise FileNotFoundError if file does not exist",
            "MUST support both absolute and relative paths",
            "SHOULD log a warning when file extension doesn't match content (e.g., .md file containing JSON)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "FileTypeDetector class with detect_file_type(path: Union[str, Path]) -> Literal['markdown', 'json', 'unknown'] method",
              "FileType enum with MARKDOWN, JSON, UNKNOWN values",
              "Case-insensitive extension matching using Path.suffix.lower()",
              "Unit tests: test_detect_md_extension, test_detect_json_extension, test_detect_unknown_extension, test_case_insensitivity"
            ],
            "middleware": [
              "Input validation for path parameter (non-empty, valid characters)",
              "Logging integration for debug-level extension detection events"
            ],
            "shared": [
              "FileType enum in planning_pipeline/models.py or silmari_rlm_act/models.py",
              "FILE_TYPE_EXTENSIONS constant mapping: {'.md': FileType.MARKDOWN, '.json': FileType.JSON}"
            ]
          },
          "testable_properties": [],
          "function_id": "FileTypeDetector.detect_file_type",
          "related_concepts": [
            "file extension parsing",
            "path handling",
            "type discrimination",
            "pathlib"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_005.2",
          "description": "If Markdown plan provided, search for sibling requirement_hierarchy.json in same directory",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [
            "MUST search for 'requirement_hierarchy.json' file in same directory as provided .md file",
            "MUST search for alternative naming patterns: 'requirements_hierarchy.json', 'hierarchy.json'",
            "MUST return absolute Path to found JSON file or None if not found",
            "MUST validate JSON file is parseable as RequirementHierarchy before returning",
            "MUST log info message when sibling JSON is found with path details",
            "MUST log debug message when no sibling JSON found",
            "SHOULD support searching parent directory if same-level search fails (configurable)",
            "SHOULD cache directory listing to avoid repeated filesystem calls",
            "MUST handle permission errors gracefully by returning None with warning log"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "SiblingFileResolver class in planning_pipeline/helpers.py",
              "find_sibling_hierarchy_json(plan_path: Path, search_parent: bool = False) -> Optional[Path] method",
              "HIERARCHY_JSON_PATTERNS constant: ['requirement_hierarchy.json', 'requirements_hierarchy.json', 'hierarchy.json']",
              "_validate_hierarchy_json(json_path: Path) -> bool helper method using RequirementHierarchy.from_dict()",
              "Unit tests: test_finds_exact_name, test_finds_alternative_name, test_validates_json_content, test_returns_none_when_missing"
            ],
            "middleware": [
              "JSON schema validation using RequirementHierarchy.from_dict() with error handling",
              "Permission error handling with PermissionError catch block"
            ],
            "shared": [
              "HIERARCHY_JSON_PATTERNS list constant in planning_pipeline/helpers.py",
              "Optional integration with existing resolve_file_path() function"
            ]
          },
          "testable_properties": [],
          "function_id": "SiblingFileResolver.find_sibling_hierarchy_json",
          "related_concepts": [
            "sibling file discovery",
            "directory traversal",
            "convention-based resolution",
            "RequirementHierarchy"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_005.3",
          "description": "If JSON hierarchy provided and no TDD plan exists, create TDD plan from hierarchy",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [
            "MUST check for existing TDD plan files (00-overview.md pattern) in same directory as hierarchy JSON",
            "MUST check for existing TDD plan in thoughts/searchable/shared/plans/ directory with matching date prefix",
            "MUST call TDDPlanningPhase.execute() to generate plan if no existing plan found",
            "MUST return path to existing plan if found, avoiding duplicate generation",
            "MUST create plan directory following naming convention: YYYY-MM-DD-tdd-{plan_name}/",
            "MUST generate overview (00-overview.md) and phase files (01-xxx.md, 02-xxx.md)",
            "MUST preserve hierarchy_path in generated plan metadata",
            "MUST log info message indicating whether existing plan was found or new plan generated",
            "SHOULD validate generated plan contains all requirements from hierarchy",
            "MUST handle TDDPlanningPhase failures gracefully with PhaseResult.FAILED status"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "TDDPlanGenerator class or extend existing TDDPlanningPhase",
              "create_tdd_plan_from_hierarchy(hierarchy_path: Path, plan_name: str = 'feature') -> PhaseResult method",
              "_find_existing_tdd_plan(hierarchy_path: Path) -> Optional[Path] helper method",
              "_validate_plan_completeness(plan_dir: Path, hierarchy: RequirementHierarchy) -> bool helper",
              "Integration with existing TDDPlanningPhase.execute() for actual generation",
              "Unit tests: test_skips_when_plan_exists, test_generates_new_plan, test_validates_plan_completeness"
            ],
            "middleware": [
              "Phase result handling with PhaseStatus.COMPLETE/FAILED",
              "Metadata tracking: skipped=True/False, reason='existing_plan_found'/'generated'"
            ],
            "shared": [
              "TDD_PLAN_PATTERNS constant: ['00-overview.md', '*-phase-*.md']",
              "DATE_PREFIX_PATTERN regex: r'\\d{4}-\\d{2}-\\d{2}-'"
            ]
          },
          "testable_properties": [],
          "function_id": "TDDPlanGenerator.create_tdd_plan_from_hierarchy",
          "related_concepts": [
            "TDD plan generation",
            "TDDPlanningPhase",
            "RequirementHierarchy",
            "Markdown generation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_005.4",
          "description": "Support path resolution via resolve_file_path() for relative paths and filenames",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [
            "MUST accept absolute paths and verify existence before returning",
            "MUST accept relative paths from project root and resolve to absolute",
            "MUST accept just filename and search in thoughts/searchable/shared/{file_type}/ directory",
            "MUST accept partial filename and perform fuzzy match on date-prefixed files",
            "MUST support file_type parameter: 'research', 'plans', 'hierarchy' (new)",
            "MUST search multiple directory variants: thoughts/shared/, thoughts/searchable/shared/",
            "MUST return first match for fuzzy searches, prioritizing exact matches",
            "MUST handle .json files when file_type='hierarchy' by searching plans directories",
            "MUST return None with debug log when no match found",
            "SHOULD support tilde expansion for home directory paths (~)",
            "MUST maintain backward compatibility with existing resolve_file_path() signature"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Extend existing resolve_file_path() in planning_pipeline/helpers.py",
              "Add file_type='hierarchy' support with JSON-specific search patterns",
              "resolve_file_path(project_path: Path, path_input: str, file_type: str) -> Optional[Path]",
              "_search_hierarchy_json(project_path: Path, filename: str) -> Optional[Path] helper",
              "Unit tests: test_resolves_absolute_path, test_resolves_relative_path, test_resolves_filename_only, test_fuzzy_match, test_hierarchy_json_search"
            ],
            "middleware": [
              "Path validation using Path.exists() and Path.is_file()",
              "Home directory expansion using Path.expanduser()"
            ],
            "shared": [
              "SEARCH_DIRECTORIES mapping: {'research': [...], 'plans': [...], 'hierarchy': [...]}",
              "Update existing file_type parameter to accept 'hierarchy' value"
            ]
          },
          "testable_properties": [],
          "function_id": "PathResolver.resolve_file_path_enhanced",
          "related_concepts": [
            "path resolution",
            "fuzzy matching",
            "thoughts directory structure",
            "cross-directory support"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_005.5",
          "description": "Maintain sibling file relationship pattern: plans/date-feature/requirement_hierarchy.json + plans/date-feature/00-overview.md",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [
            "MUST enforce directory structure: thoughts/searchable/shared/plans/YYYY-MM-DD-{plan_name}/",
            "MUST ensure requirement_hierarchy.json and 00-overview.md exist in same directory",
            "MUST create missing sibling file when one is provided (hierarchy -> plan or plan -> hierarchy lookup)",
            "MUST validate both files reference each other in metadata",
            "MUST update hierarchy JSON metadata.plan_path when TDD plan is generated",
            "MUST update plan overview front matter with hierarchy_path when plan is generated",
            "MUST support atomic operations: either both files valid or operation fails",
            "MUST provide method to check relationship integrity: is_sibling_pair_valid(dir_path)",
            "SHOULD support listing all valid sibling pairs in project for discovery",
            "MUST log warning when relationship is broken (one file exists without other)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "SiblingFileRelationshipManager class in planning_pipeline/helpers.py or new module",
              "ensure_sibling_pair(plan_path: Path = None, hierarchy_path: Path = None) -> tuple[Path, Path] method",
              "is_sibling_pair_valid(directory: Path) -> bool validation method",
              "list_sibling_pairs(project_path: Path) -> list[tuple[Path, Path]] discovery method",
              "update_hierarchy_metadata(hierarchy_path: Path, plan_path: Path) -> None method",
              "update_plan_frontmatter(plan_path: Path, hierarchy_path: Path) -> None method",
              "Unit tests: test_creates_missing_plan, test_creates_missing_hierarchy, test_validates_pair, test_updates_metadata"
            ],
            "middleware": [
              "Atomic file operation wrapper using tempfile and rename",
              "Rollback mechanism if one file operation fails",
              "File locking for concurrent access safety (optional)"
            ],
            "shared": [
              "SIBLING_PATTERN constant: {'hierarchy': 'requirement_hierarchy.json', 'overview': '00-overview.md'}",
              "PlanDirectoryStructure dataclass with hierarchy_path, overview_path, phase_files attributes",
              "Integration with existing pipeline artifacts and checkpoint metadata"
            ]
          },
          "testable_properties": [],
          "function_id": "SiblingFileRelationshipManager.maintain_sibling_relationship",
          "related_concepts": [
            "file organization",
            "artifact relationship",
            "directory structure conventions",
            "metadata linking"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_006",
      "description": "The system must include proper metadata in pipeline results when using --plan-path option",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_006.1",
          "description": "Include requirements_count (top-level requirement count) in pipeline result metadata when validating hierarchy from --plan-path option",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "When --plan-path is provided, the decomposition PhaseResult.metadata must contain a 'requirements_count' key",
            "requirements_count must be an integer representing only top-level requirements (parent_id == null), not total nodes",
            "requirements_count must equal len(hierarchy.requirements) from the parsed RequirementHierarchy",
            "requirements_count must be included in PhaseResult.metadata before returning from _validate_hierarchy_path",
            "Test: Hierarchy with 2 top-level requirements each having 3 children must report requirements_count=2",
            "Test: Empty hierarchy (no requirements) must report requirements_count=0",
            "Test: requirements_count must be accessible via pipeline.state.get_phase_result(PhaseType.DECOMPOSITION).metadata['requirements_count']"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Modify RLMActPipeline._validate_hierarchy_path() to calculate len(hierarchy.requirements) after successful validation",
              "Add 'requirements_count' key to the metadata dictionary returned from _validate_hierarchy_path",
              "Ensure requirements_count is propagated to the synthetic PhaseResult for DECOMPOSITION phase when --plan-path is used",
              "Add test case test_requirements_count_is_top_level_only to verify counting excludes children",
              "Add test case test_requirements_count_is_integer to verify type correctness"
            ],
            "middleware": [],
            "shared": [
              "RequirementHierarchy.requirements property exposes top-level requirements list",
              "PhaseResult.metadata dictionary stores arbitrary key-value pairs for phase outcomes"
            ]
          },
          "testable_properties": [],
          "function_id": "RLMActPipeline._validate_hierarchy_path",
          "related_concepts": [
            "RequirementHierarchy",
            "PhaseResult.metadata",
            "JSON hierarchy validation",
            "top-level requirements counting"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_006.2",
          "description": "Include total_nodes count (all nodes at all levels) in pipeline result metadata when validating hierarchy from --plan-path option",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "When --plan-path is provided, the decomposition PhaseResult.metadata must contain a 'total_nodes' key",
            "total_nodes must be an integer representing ALL nodes in the hierarchy tree (parents + children + grandchildren)",
            "total_nodes must be calculated by recursively counting: sum of 1 + len(children) + sum(len(grandchild.children)) for each requirement",
            "total_nodes must equal the count computed by _count_nodes() or equivalent tree traversal",
            "Test: Hierarchy with 2 parents, each having 2 children, each having 2 grandchildren = 2 + 4 + 8 = 14 total_nodes",
            "Test: Single parent with no children must report total_nodes=1",
            "Test: total_nodes must always be >= requirements_count"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Modify RLMActPipeline._validate_hierarchy_path() to traverse all hierarchy levels and count nodes",
              "Implement node counting logic: for each req in hierarchy.requirements, count += 1 + len(children) + sum(len(child.children))",
              "Add 'total_nodes' key to the metadata dictionary returned from _validate_hierarchy_path",
              "Ensure counting handles arbitrary nesting depth (currently supports 3 levels: parent/sub_process/implementation)",
              "Add test case test_total_nodes_counts_all_levels to verify recursive counting",
              "Add test case test_total_nodes_is_integer to verify type correctness"
            ],
            "middleware": [],
            "shared": [
              "RequirementNode.children property provides access to child nodes",
              "Reuse _count_nodes() helper method from DecompositionPhase for consistent counting logic"
            ]
          },
          "testable_properties": [],
          "function_id": "RLMActPipeline._validate_hierarchy_path",
          "related_concepts": [
            "RequirementHierarchy tree traversal",
            "nested requirement counting",
            "PhaseResult.metadata",
            "recursive node counting"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_006.3",
          "description": "Track and propagate the 'source' field from hierarchy metadata to identify the decomposition origin (e.g., agent_sdk_decomposition, test, baml_decomposition)",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "When --plan-path is provided, the source field from hierarchy.metadata must be included in PhaseResult.metadata",
            "If hierarchy.metadata contains 'source' key, it must be copied to PhaseResult.metadata['source']",
            "If hierarchy.metadata does not contain 'source' key, PhaseResult.metadata['source'] should be set to 'external_hierarchy' or omitted",
            "Valid source values include: 'agent_sdk_decomposition', 'baml_decomposition', 'test', 'manual'",
            "Test: Hierarchy with source='agent_sdk_decomposition' must propagate source to result metadata",
            "Test: Hierarchy with source='test' must propagate source to result metadata",
            "Test: Hierarchy without source field must handle gracefully (not raise exception)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Access hierarchy.metadata['source'] after successful RequirementHierarchy.from_dict() deserialization",
              "Add conditional logic: if 'source' in hierarchy.metadata, add it to validation_metadata dict",
              "Handle missing source field gracefully with dict.get('source', None) pattern",
              "Update tests to verify source field propagation from hierarchy JSON to PhaseResult.metadata",
              "Add test case test_source_field_propagates_from_hierarchy_metadata"
            ],
            "middleware": [],
            "shared": [
              "RequirementHierarchy.metadata dict contains arbitrary metadata including 'source' key",
              "DecompositionPhase sets source='agent_sdk_decomposition' when creating new hierarchies"
            ]
          },
          "testable_properties": [],
          "function_id": "RLMActPipeline._validate_hierarchy_path",
          "related_concepts": [
            "RequirementHierarchy.metadata",
            "decomposition source tracking",
            "provenance metadata",
            "pipeline traceability"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_006.4",
          "description": "Include decomposition_stats in pipeline result metadata when available in the hierarchy JSON file",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "When --plan-path hierarchy contains decomposition_stats in metadata, it must be included in PhaseResult.metadata",
            "decomposition_stats must be a dict containing: requirements_found, subprocesses_expanded, total_nodes, extraction_time_ms, expansion_time_ms",
            "If decomposition_stats is present, copy it to PhaseResult.metadata['decomposition_stats']",
            "If decomposition_stats is not present (e.g., manually created hierarchy), omit the field rather than setting to null",
            "decomposition_stats.total_nodes should be consistent with the calculated total_nodes (validation check)",
            "Test: Hierarchy with decomposition_stats must propagate all stat fields to result metadata",
            "Test: Hierarchy without decomposition_stats must not include the key in result metadata"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "Access hierarchy.metadata.get('decomposition_stats') after successful validation",
              "Add conditional logic: if decomposition_stats exists and is a dict, include it in validation_metadata",
              "Optionally validate that decomposition_stats.total_nodes matches calculated total_nodes (warning if mismatch)",
              "Preserve all decomposition_stats fields: requirements_found, subprocesses_expanded, total_nodes, extraction_time_ms, expansion_time_ms",
              "Add test case test_decomposition_stats_propagates_when_present",
              "Add test case test_decomposition_stats_omitted_when_absent"
            ],
            "middleware": [],
            "shared": [
              "DecompositionConfig controls decomposition behavior and stats collection",
              "planning_pipeline.decomposition.decompose_requirements() generates decomposition_stats",
              "Shared schema for decomposition_stats: {requirements_found: int, subprocesses_expanded: int, total_nodes: int, extraction_time_ms: int, expansion_time_ms: int}"
            ]
          },
          "testable_properties": [],
          "function_id": "RLMActPipeline._validate_hierarchy_path",
          "related_concepts": [
            "decomposition statistics",
            "RequirementHierarchy.metadata.decomposition_stats",
            "pipeline metrics",
            "timing statistics"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    }
  ],
  "metadata": {
    "source": "agent_sdk_decomposition",
    "research_length": 14094,
    "decomposition_stats": {
      "requirements_found": 7,
      "subprocesses_expanded": 33,
      "total_nodes": 40,
      "extraction_time_ms": 21224,
      "expansion_time_ms": 431919
    }
  }
}